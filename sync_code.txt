// src/domains/sync/types.rs
use crate::errors::{DomainError, ValidationError};
use chrono::{DateTime, Utc};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::str::FromStr;
use uuid::Uuid;
use sqlx::FromRow;

/// The direction of a sync operation
#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
pub enum SyncDirection {
    Upload,
    Download,
}

impl FromStr for SyncDirection {
    type Err = DomainError;
    fn from_str(s: &str) -> Result<Self, Self::Err> {
        match s {
            "upload" => Ok(SyncDirection::Upload),
            "download" => Ok(SyncDirection::Download),
            _ => Err(DomainError::Validation(ValidationError::custom(
                &format!("Invalid SyncDirection string: {}", s)
            )))
        }
    }
}

impl From<SyncDirection> for String {
    fn from(direction: SyncDirection) -> Self {
        match direction {
            SyncDirection::Upload => "upload".to_string(),
            SyncDirection::Download => "download".to_string(),
        }
    }
}

impl SyncDirection {
    pub fn as_str(&self) -> &'static str {
        match self {
            SyncDirection::Upload => "upload",
            SyncDirection::Download => "download",
        }
    }
}

/// The status of a sync batch
#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
pub enum SyncBatchStatus {
    Pending,
    Processing,
    Completed,
    Failed,
    PartiallyFailed,
}

impl FromStr for SyncBatchStatus {
    type Err = DomainError;
    fn from_str(s: &str) -> Result<Self, Self::Err> {
        match s {
            "pending" => Ok(SyncBatchStatus::Pending),
            "processing" => Ok(SyncBatchStatus::Processing),
            "completed" => Ok(SyncBatchStatus::Completed),
            "failed" => Ok(SyncBatchStatus::Failed),
            "partially_failed" => Ok(SyncBatchStatus::PartiallyFailed),
            _ => Err(DomainError::Validation(ValidationError::custom(
                &format!("Invalid SyncBatchStatus string: {}", s)
            )))
        }
    }
}

impl From<SyncBatchStatus> for String {
    fn from(status: SyncBatchStatus) -> Self {
        match status {
            SyncBatchStatus::Pending => "pending".to_string(),
            SyncBatchStatus::Processing => "processing".to_string(),
            SyncBatchStatus::Completed => "completed".to_string(),
            SyncBatchStatus::Failed => "failed".to_string(),
            SyncBatchStatus::PartiallyFailed => "partially_failed".to_string(),
        }
    }
}

impl SyncBatchStatus {
    pub fn as_str(&self) -> &'static str {
        match self {
            SyncBatchStatus::Pending => "pending",
            SyncBatchStatus::Processing => "processing",
            SyncBatchStatus::Completed => "completed",
            SyncBatchStatus::Failed => "failed",
            SyncBatchStatus::PartiallyFailed => "partially_failed",
        }
    }
}

/// The status of a device sync connection
#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
pub enum DeviceSyncStatus {
    Success,
    PartialSuccess,
    Failed,
    InProgress,
}

impl FromStr for DeviceSyncStatus {
    type Err = DomainError;
    fn from_str(s: &str) -> Result<Self, Self::Err> {
        match s {
            "success" => Ok(DeviceSyncStatus::Success),
            "partial_success" => Ok(DeviceSyncStatus::PartialSuccess),
            "failed" => Ok(DeviceSyncStatus::Failed),
            "in_progress" => Ok(DeviceSyncStatus::InProgress),
            _ => Err(DomainError::Validation(ValidationError::custom(
                &format!("Invalid DeviceSyncStatus string: {}", s)
            )))
        }
    }
}

impl From<DeviceSyncStatus> for String {
    fn from(status: DeviceSyncStatus) -> Self {
        match status {
            DeviceSyncStatus::Success => "success".to_string(),
            DeviceSyncStatus::PartialSuccess => "partial_success".to_string(),
            DeviceSyncStatus::Failed => "failed".to_string(),
            DeviceSyncStatus::InProgress => "in_progress".to_string(),
        }
    }
}

impl DeviceSyncStatus {
    pub fn as_str(&self) -> &'static str {
        match self {
            DeviceSyncStatus::Success => "success",
            DeviceSyncStatus::PartialSuccess => "partial_success",
            DeviceSyncStatus::Failed => "failed",
            DeviceSyncStatus::InProgress => "in_progress",
        }
    }
}

/// The type of change operation
#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
pub enum ChangeOperationType {
    Create,
    Update,
    Delete,
    HardDelete,
}

impl ChangeOperationType {
    pub fn as_str(&self) -> &'static str {
        match self {
            ChangeOperationType::Create => "create",
            ChangeOperationType::Update => "update",
            ChangeOperationType::Delete => "delete",
            ChangeOperationType::HardDelete => "hard_delete",
        }
    }

    pub fn from_str(s: &str) -> Option<Self> {
        match s {
            "create" => Some(ChangeOperationType::Create),
            "update" => Some(ChangeOperationType::Update),
            "delete" => Some(ChangeOperationType::Delete),
            "hard_delete" => Some(ChangeOperationType::HardDelete),
            _ => None,
        }
    }
}

impl From<ChangeOperationType> for String {
    fn from(op_type: ChangeOperationType) -> Self {
        op_type.as_str().to_string()
    }
}

/// The status of conflict resolution
#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
pub enum ConflictResolutionStatus {
    Resolved,
    Unresolved,
    Manual,
    Ignored,
}

impl ConflictResolutionStatus {
    pub fn as_str(&self) -> &'static str {
        match self {
            ConflictResolutionStatus::Resolved => "resolved",
            ConflictResolutionStatus::Unresolved => "unresolved",
            ConflictResolutionStatus::Manual => "manual",
            ConflictResolutionStatus::Ignored => "ignored",
        }
    }
}

/// The strategy for conflict resolution
#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
pub enum ConflictResolutionStrategy {
    ServerWins,
    ClientWins,
    LastWriteWins,
    MergePrioritizeServer,
    MergePrioritizeClient,
    Manual,
}

impl ConflictResolutionStrategy {
    pub fn as_str(&self) -> &'static str {
        match self {
            ConflictResolutionStrategy::ServerWins => "server_wins",
            ConflictResolutionStrategy::ClientWins => "client_wins",
            ConflictResolutionStrategy::LastWriteWins => "last_write_wins",
            ConflictResolutionStrategy::MergePrioritizeServer => "merge_prioritize_server",
            ConflictResolutionStrategy::MergePrioritizeClient => "merge_prioritize_client",
            ConflictResolutionStrategy::Manual => "manual",
        }
    }
}

/// Sync priority for entities
#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
pub enum SyncPriority {
    High,    // 'high'
    Normal,  // 'normal'
    Low,     // 'low'
    Never,   // 'never'
}

impl SyncPriority {
    pub fn as_str(&self) -> &'static str {
        match self {
            SyncPriority::High => "high",
            SyncPriority::Normal => "normal",
            SyncPriority::Low => "low",
            SyncPriority::Never => "never",
        }
    }
}

impl Default for SyncPriority {
    fn default() -> Self {
        SyncPriority::Normal
    }
}

impl std::str::FromStr for SyncPriority {
    type Err = crate::errors::DomainError;
    fn from_str(s: &str) -> Result<Self, Self::Err> {
        match s.to_lowercase().as_str() {
            "high" => Ok(SyncPriority::High),
            "normal" => Ok(SyncPriority::Normal),
            "low" => Ok(SyncPriority::Low),
            "never" => Ok(SyncPriority::Never),
            _ => Err(crate::errors::DomainError::Validation(crate::errors::ValidationError::custom(&format!("Invalid SyncPriority string: {}", s))))
        }
    }
}

impl From<SyncPriority> for String {
    fn from(priority: SyncPriority) -> Self {
        priority.as_str().to_string()
    }
}

/// Detailed sync priority levels for more granular control, used internally by sync scheduler
#[derive(Debug, Clone, Copy, PartialEq, Eq, PartialOrd, Ord, Serialize, Deserialize)]
pub enum DetailedSyncPriority {
    High,
    Normal,
    Low,
    Never,
}

/// Sync mode configuration
#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
pub enum SyncMode {
    /// Sync all available data (initial sync)
    Full,
    /// Sync only changes since last sync
    Incremental,
    /// Sync only essential data (metadata)
    Minimal,
    /// Sync only specific entities
    Selective,
}

impl SyncMode {
    pub fn as_str(&self) -> &'static str {
        match self {
            SyncMode::Full => "full",
            SyncMode::Incremental => "incremental",
            SyncMode::Minimal => "minimal",
            SyncMode::Selective => "selective",
        }
    }
}

/// Data purge strategy
#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
pub enum DataPurgeStrategy {
    /// Never purge this data
    Never,
    /// Purge when storage is low
    WhenStorageLow,
    /// Purge after specific time (e.g., 30 days)
    AfterTime,
    /// Purge after sync confirmed
    AfterSync,
    /// Purge immediately after use
    Immediate,
}

/// Represents a batch of changes for sync
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SyncBatch {
    pub batch_id: String,
    pub device_id: Uuid,
    pub direction: SyncDirection,
    pub status: SyncBatchStatus,
    pub item_count: Option<i64>,
    pub total_size: Option<i64>,
    pub priority: Option<i64>,
    pub attempts: Option<i64>,
    pub last_attempt_at: Option<DateTime<Utc>>,
    pub error_message: Option<String>,
    pub created_at: DateTime<Utc>,
    pub completed_at: Option<DateTime<Utc>>,
}

/// Represents a device's sync state
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DeviceSyncState {
    pub device_id: Uuid,
    pub user_id: Uuid,
    pub last_upload_timestamp: Option<DateTime<Utc>>,
    pub last_download_timestamp: Option<DateTime<Utc>>,
    pub last_sync_status: Option<DeviceSyncStatus>,
    pub last_sync_attempt_at: Option<DateTime<Utc>>,
    pub server_version: Option<i64>,
    pub sync_enabled: Option<bool>,
    pub created_at: DateTime<Utc>,
    pub updated_at: DateTime<Utc>,
}

/// Metadata about a device
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DeviceMetadata {
    pub device_id: Uuid,
    pub name: String,
    pub platform: String,
    pub model: Option<String>,
    pub os_version: Option<String>,
    pub app_version: String,
    pub last_active_at: Option<DateTime<Utc>>,
    pub created_at: DateTime<Utc>,
    pub updated_at: DateTime<Utc>,
}

/// Represents a change in the change log
#[derive(Debug, Clone, Serialize, Deserialize, FromRow)]
pub struct ChangeLogEntry {
    pub operation_id: Uuid,
    pub entity_table: String,
    pub entity_id: Uuid,
    pub operation_type: ChangeOperationType,
    pub field_name: Option<String>,
    pub old_value: Option<String>,
    pub new_value: Option<String>,
    pub document_metadata: Option<String>,
    pub timestamp: DateTime<Utc>,
    pub user_id: Uuid,
    pub device_id: Option<Uuid>,
    pub sync_batch_id: Option<String>,
    pub processed_at: Option<DateTime<Utc>>,
    pub sync_error: Option<String>,
}

/// Represents a tombstone record for a hard-deleted entity
#[derive(Debug, Clone, Serialize, Deserialize, FromRow)]
pub struct Tombstone {
    /// Unique ID for the tombstone
    pub id: Uuid,
    
    /// ID of the deleted entity
    pub entity_id: Uuid,
    
    /// Type of the deleted entity (table name)
    pub entity_type: String,
    
    /// User ID who performed the deletion
    pub deleted_by: Uuid,
    
    /// When the deletion occurred
    pub deleted_at: DateTime<Utc>,
    
    /// Operation ID for batch operations
    pub operation_id: Uuid,
    
    /// Additional metadata for the tombstone
    pub additional_metadata: Option<String>,
}

impl Tombstone {
    /// Create a new tombstone with a generated operation ID
    pub fn new(
        entity_id: Uuid,
        entity_type: &str,
        deleted_by: Uuid,
    ) -> Self {
        Self {
            id: Uuid::new_v4(),
            entity_id,
            entity_type: entity_type.to_string(),
            deleted_by,
            deleted_at: Utc::now(),
            operation_id: Uuid::new_v4(),
            additional_metadata: None,
        }
    }
    
    /// Create a new tombstone with a specific operation ID
    pub fn with_operation_id(
        entity_id: Uuid,
        entity_type: &str,
        deleted_by: Uuid,
        operation_id: Uuid,
    ) -> Self {
        Self {
            id: Uuid::new_v4(),
            entity_id,
            entity_type: entity_type.to_string(),
            deleted_by,
            deleted_at: Utc::now(),
            operation_id,
            additional_metadata: None,
        }
    }
}

/// Application connection settings
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AppConnectionSettings {
    pub id: String, // Always "cloud"
    pub api_endpoint: String,
    pub api_version: Option<String>,
    pub connection_timeout: Option<i64>,
    pub offline_mode_enabled: Option<bool>,
    pub retry_count: Option<i64>,
    pub retry_delay: Option<i64>,
    pub updated_at: DateTime<Utc>,
}

/// Represents a conflict between local and remote changes
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SyncConflict {
    pub conflict_id: Uuid,
    pub entity_table: String,
    pub entity_id: Uuid,
    pub field_name: Option<String>,
    pub local_change: ChangeLogEntry,
    pub remote_change: ChangeLogEntry,
    pub resolution_status: ConflictResolutionStatus,
    pub resolution_strategy: Option<ConflictResolutionStrategy>,
    pub resolved_by_user_id: Option<Uuid>,
    pub resolved_at: Option<DateTime<Utc>>,
    pub created_at: DateTime<Utc>,
}

/// Data Transfer Object for initializing a sync session
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SyncSessionInitDto {
    pub device_id: Uuid,
    pub user_id: Uuid,
    pub sync_mode: SyncMode,
    pub last_sync_timestamp: Option<DateTime<Utc>>,
    pub network_type: Option<String>,
    pub battery_level: Option<f64>,
    pub available_storage: Option<i64>,
    pub app_version: String,
}

/// Response from initializing a sync session
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SyncSessionInitResponse {
    pub session_id: String,
    pub server_time: DateTime<Utc>,
    pub last_known_client_sync: Option<DateTime<Utc>>,
    pub sync_mode_approved: SyncMode,
    pub estimated_download_size: Option<i64>,
    pub estimated_download_count: Option<i64>,
}

/// DTO for creating a new sync batch
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CreateSyncBatchDto {
    pub device_id: Uuid,
    pub direction: SyncDirection,
    pub priority: Option<i64>,
}

/// DTO for uploading changes to server
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct UploadChangesDto {
    pub batch_id: String,
    pub device_id: Uuid,
    pub user_id: Uuid,
    pub changes: Vec<ChangeLogEntry>,
    pub tombstones: Option<Vec<Tombstone>>,
}

/// Response from uploading changes
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct UploadChangesResponse {
    pub batch_id: String,
    pub changes_accepted: i64,
    pub changes_rejected: i64,
    pub conflicts_detected: i64,
    pub conflicts: Option<Vec<SyncConflict>>,
    pub server_timestamp: DateTime<Utc>,
}

/// DTO for downloading changes from server
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DownloadChangesDto {
    pub batch_id: String,
    pub device_id: Uuid,
    pub user_id: Uuid,
    pub last_sync_timestamp: Option<DateTime<Utc>>,
    pub tables_requested: Option<Vec<String>>,
    pub entity_ids_requested: Option<HashMap<String, Vec<Uuid>>>,
    pub max_changes: Option<i64>,
    pub include_blobs: Option<bool>,
}

/// Response from downloading changes
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DownloadChangesResponse {
    pub batch_id: String,
    pub changes: Vec<ChangeLogEntry>,
    pub tombstones: Option<Vec<Tombstone>>,
    pub has_more: bool,
    pub server_timestamp: DateTime<Utc>,
    pub next_batch_hint: Option<String>,
}

/// DTO for selective sync configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SelectiveSyncConfigDto {
    pub user_id: Uuid,
    pub device_id: Uuid,
    pub table_config: HashMap<String, TableSyncConfig>,
    pub storage_quota_mb: Option<i64>,
}

/// Configuration for syncing a specific table
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TableSyncConfig {
    pub table_name: String,
    pub enabled: bool,
    pub priority: SyncPriority,
    pub purge_strategy: DataPurgeStrategy,
    pub retention_days: Option<i64>,
    pub include_blobs: bool,
    pub sync_field_level: bool,  // Whether to sync field-level changes or just record-level
    pub filter_config: Option<TableFilterConfig>,
}

/// Filters for selective sync of a table
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TableFilterConfig {
    pub time_window_days: Option<i64>,
    pub related_to_user_only: Option<bool>,
    pub specific_ids: Option<Vec<Uuid>>,
    pub custom_filter: Option<String>, // JSON string with custom filter criteria
}

/// DTO for confirming changes have been processed
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ConfirmChangesDto {
    pub batch_id: String,
    pub device_id: Uuid,
    pub user_id: Uuid,
    pub processed_change_ids: Vec<Uuid>,
    pub failed_change_ids: Option<Vec<Uuid>>,
    pub failure_details: Option<HashMap<Uuid, String>>,
}

/// Response from confirming changes
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ConfirmChangesResponse {
    pub batch_id: String,
    pub confirmation_status: String,
    pub server_timestamp: DateTime<Utc>,
}

/// Data about sync progress to report to user
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SyncProgress {
    pub sync_in_progress: bool,
    pub current_operation: Option<String>,
    pub total_changes: i64,
    pub processed_changes: i64,
    pub pending_uploads: i64,
    pub pending_downloads: i64,
    pub last_sync_timestamp: Option<DateTime<Utc>>,
    pub last_sync_status: Option<DeviceSyncStatus>,
    pub sync_errors: Vec<String>,
    pub table_progress: Option<HashMap<String, TableSyncProgress>>,
}

/// Progress for a specific table
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TableSyncProgress {
    pub table_name: String,
    pub records_synced: i64,
    pub total_records: Option<i64>,
    pub bytes_transferred: Option<i64>,
    pub completed: bool,
}

/// Sync statistics for the app
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SyncStats {
    pub total_uploads: i64,
    pub total_downloads: i64,
    pub failed_uploads: i64,
    pub failed_downloads: i64,
    pub conflicts_encountered: i64,
    pub conflicts_resolved_auto: i64,
    pub conflicts_resolved_manual: i64,
    pub conflicts_pending: i64,
    pub total_bytes_uploaded: i64,
    pub total_bytes_downloaded: i64,
    pub last_full_sync: Option<DateTime<Utc>>,
    pub avg_sync_duration_seconds: Option<f64>,
}

/// Record of a sync session
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SyncSession {
    pub session_id: String,
    pub user_id: Uuid,
    pub device_id: Uuid,
    pub start_time: DateTime<Utc>,
    pub end_time: Option<DateTime<Utc>>,
    pub sync_mode: SyncMode,
    pub status: DeviceSyncStatus,
    pub error_message: Option<String>,
    pub changes_uploaded: Option<i64>,
    pub changes_downloaded: Option<i64>,
    pub conflicts_encountered: Option<i64>,
    pub bytes_transferred: Option<i64>,
    pub network_type: Option<String>,
    pub duration_seconds: Option<f64>,
}

/// Configuration for sync scheduling
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SyncScheduleConfig {
    pub auto_sync_enabled: bool,
    pub wifi_only: bool,
    pub charging_only: Option<bool>,
    pub min_battery_percentage: Option<i64>,
    pub background_sync_interval_minutes: Option<i64>,
    pub quiet_hours_start: Option<i64>, // Hour of day (0-23)
    pub quiet_hours_end: Option<i64>,   // Hour of day (0-23)
    pub max_sync_frequency_minutes: Option<i64>,
    pub allow_metered_connection: Option<bool>,
}

fn parse_uuid(uuid_str: &str, field_name: &str) -> Result<Uuid, DomainError> {
    Uuid::parse_str(uuid_str).map_err(|_| DomainError::Validation(ValidationError::format(
        field_name, &format!("Invalid UUID format: {}", uuid_str)
    )))
}

fn parse_optional_uuid(uuid_str: Option<String>, field_name: &str) -> Result<Option<Uuid>, DomainError> {
    uuid_str.map(|s| parse_uuid(&s, field_name)).transpose()
}

fn parse_datetime(dt_str: &str, field_name: &str) -> Result<DateTime<Utc>, DomainError> {
    DateTime::parse_from_rfc3339(dt_str)
        .map(|dt| dt.with_timezone(&Utc))
        .map_err(|_| DomainError::Validation(ValidationError::format(
            field_name, &format!("Invalid RFC3339 format: {}", dt_str)
        )))
}

fn parse_optional_datetime(dt_str: Option<String>, field_name: &str) -> Result<Option<DateTime<Utc>>, DomainError> {
    dt_str.map(|s| parse_datetime(&s, field_name)).transpose()
}

#[derive(Debug, Clone, FromRow)]
pub struct SyncBatchRow {
    pub batch_id: String,
    pub device_id: String,
    pub direction: String,
    pub status: String,
    pub item_count: Option<i64>,
    pub total_size: Option<i64>,
    pub priority: Option<i64>,
    pub attempts: Option<i64>,
    pub last_attempt_at: Option<String>,
    pub error_message: Option<String>,
    pub created_at: String,
    pub completed_at: Option<String>,
}

#[derive(Debug, Clone, FromRow)]
pub struct DeviceSyncStateRow {
    pub device_id: String,
    pub user_id: String,
    pub last_upload_timestamp: Option<String>,
    pub last_download_timestamp: Option<String>,
    pub last_sync_status: Option<String>,
    pub last_sync_attempt_at: Option<String>,
    pub server_version: Option<i64>,
    pub sync_enabled: Option<i64>,
    pub created_at: String,
    pub updated_at: String,
}

#[derive(Debug, Clone, FromRow)]
pub struct ChangeLogEntryRow {
    pub operation_id: String,
    pub entity_table: String,
    pub entity_id: String,
    pub operation_type: String,
    pub field_name: Option<String>,
    pub old_value: Option<String>,
    pub new_value: Option<String>,
    pub document_metadata: Option<String>,
    pub timestamp: String,
    pub user_id: String,
    pub device_id: Option<String>,
    pub sync_batch_id: Option<String>,
    pub processed_at: Option<String>,
    pub sync_error: Option<String>,
}

#[derive(Debug, Clone, FromRow)]
pub struct TombstoneRow {
    pub id: String,
    pub entity_id: String,
    pub entity_type: String,
    pub deleted_by: String,
    pub deleted_at: String,
    pub operation_id: String,
    pub additional_metadata: Option<String>,
}

#[derive(Debug, Clone, FromRow)]
pub struct AppConnectionSettingsRow {
    pub id: String,
    pub api_endpoint: String,
    pub api_version: Option<String>,
    pub connection_timeout: Option<i64>,
    pub offline_mode_enabled: Option<i64>,
    pub retry_count: Option<i64>,
    pub retry_delay: Option<i64>,
    pub updated_at: String,
}

#[derive(Debug, Clone, FromRow)]
pub struct SyncConflictRow {
    pub conflict_id: String,
    pub entity_table: String,
    pub entity_id: String,
    pub field_name: Option<String>,
    pub local_change_op_id: String,
    pub remote_change_op_id: String,
    pub resolution_status: String,
    pub resolution_strategy: Option<String>,
    pub resolved_by_user_id: Option<String>,
    pub resolved_at: Option<String>,
    pub created_at: String,
}

#[derive(Debug, Clone, FromRow)]
pub struct SyncSessionRow {
    pub session_id: String,
    pub user_id: String,
    pub device_id: String,
    pub start_time: String,
    pub end_time: Option<String>,
    pub sync_mode: String,
    pub status: String,
    pub error_message: Option<String>,
    pub changes_uploaded: Option<i64>,
    pub changes_downloaded: Option<i64>,
    pub conflicts_encountered: Option<i64>,
    pub bytes_transferred: Option<i64>,
    pub network_type: Option<String>,
    pub duration_seconds: Option<f64>,
}

impl TryFrom<SyncBatchRow> for SyncBatch {
    type Error = DomainError;
    fn try_from(row: SyncBatchRow) -> Result<Self, Self::Error> {
        Ok(Self {
            batch_id: row.batch_id,
            device_id: parse_uuid(&row.device_id, "sync_batch.device_id")?,
            direction: SyncDirection::from_str(&row.direction)?,
            status: SyncBatchStatus::from_str(&row.status)?,
            item_count: row.item_count,
            total_size: row.total_size,
            priority: row.priority,
            attempts: row.attempts,
            last_attempt_at: parse_optional_datetime(row.last_attempt_at, "sync_batch.last_attempt_at")?,
            error_message: row.error_message,
            created_at: parse_datetime(&row.created_at, "sync_batch.created_at")?,
            completed_at: parse_optional_datetime(row.completed_at, "sync_batch.completed_at")?,
        })
    }
}

impl TryFrom<DeviceSyncStateRow> for DeviceSyncState {
    type Error = DomainError;
    fn try_from(row: DeviceSyncStateRow) -> Result<Self, Self::Error> {
        Ok(Self {
            device_id: parse_uuid(&row.device_id, "device_sync_state.device_id")?,
            user_id: parse_uuid(&row.user_id, "device_sync_state.user_id")?,
            last_upload_timestamp: parse_optional_datetime(row.last_upload_timestamp, "device_sync_state.last_upload_timestamp")?,
            last_download_timestamp: parse_optional_datetime(row.last_download_timestamp, "device_sync_state.last_download_timestamp")?,
            last_sync_status: row.last_sync_status.map(|s| DeviceSyncStatus::from_str(&s)).transpose()?,
            last_sync_attempt_at: parse_optional_datetime(row.last_sync_attempt_at, "device_sync_state.last_sync_attempt_at")?,
            server_version: row.server_version,
            sync_enabled: row.sync_enabled.map(|v| v == 1),
            created_at: parse_datetime(&row.created_at, "device_sync_state.created_at")?,
            updated_at: parse_datetime(&row.updated_at, "device_sync_state.updated_at")?,
        })
    }
}

impl TryFrom<ChangeLogEntryRow> for ChangeLogEntry {
    type Error = DomainError;
    fn try_from(row: ChangeLogEntryRow) -> Result<Self, Self::Error> {
        Ok(Self {
            operation_id: parse_uuid(&row.operation_id, "change_log.operation_id")?,
            entity_table: row.entity_table,
            entity_id: parse_uuid(&row.entity_id, "change_log.entity_id")?,
            operation_type: ChangeOperationType::from_str(&row.operation_type).ok_or_else(|| {
                DomainError::Validation(ValidationError::custom("Invalid ChangeOperationType"))
            })?,
            field_name: row.field_name,
            old_value: row.old_value,
            new_value: row.new_value,
            document_metadata: row.document_metadata,
            timestamp: parse_datetime(&row.timestamp, "change_log.timestamp")?,
            user_id: parse_uuid(&row.user_id, "change_log.user_id")?,
            device_id: parse_optional_uuid(row.device_id, "change_log.device_id")?,
            sync_batch_id: row.sync_batch_id,
            processed_at: parse_optional_datetime(row.processed_at, "change_log.processed_at")?,
            sync_error: row.sync_error,
        })
    }
}

impl TryFrom<TombstoneRow> for Tombstone {
    type Error = DomainError;
    fn try_from(row: TombstoneRow) -> Result<Self, Self::Error> {
        Ok(Self {
            id: parse_uuid(&row.id, "tombstone.id")?,
            entity_id: parse_uuid(&row.entity_id, "tombstone.entity_id")?,
            entity_type: row.entity_type,
            deleted_by: parse_uuid(&row.deleted_by, "tombstone.deleted_by")?,
            deleted_at: parse_datetime(&row.deleted_at, "tombstone.deleted_at")?,
            operation_id: parse_uuid(&row.operation_id, "tombstone.operation_id")?,
            additional_metadata: row.additional_metadata,
        })
    }
}

impl TryFrom<AppConnectionSettingsRow> for AppConnectionSettings {
    type Error = DomainError;
    fn try_from(row: AppConnectionSettingsRow) -> Result<Self, Self::Error> {
        Ok(Self {
            id: row.id,
            api_endpoint: row.api_endpoint,
            api_version: row.api_version,
            connection_timeout: row.connection_timeout,
            offline_mode_enabled: row.offline_mode_enabled.map(|v| v == 1),
            retry_count: row.retry_count,
            retry_delay: row.retry_delay,
            updated_at: parse_datetime(&row.updated_at, "app_connection_settings.updated_at")?,
        })
    }
}

impl TryFrom<SyncSessionRow> for SyncSession {
    type Error = DomainError;
    fn try_from(row: SyncSessionRow) -> Result<Self, Self::Error> {
        Ok(Self {
            session_id: row.session_id,
            user_id: parse_uuid(&row.user_id, "sync_session.user_id")?,
            device_id: parse_uuid(&row.device_id, "sync_session.device_id")?,
            start_time: parse_datetime(&row.start_time, "sync_session.start_time")?,
            end_time: parse_optional_datetime(row.end_time, "sync_session.end_time")?,
            sync_mode: SyncMode::from_str(&row.sync_mode)?,
            status: DeviceSyncStatus::from_str(&row.status)?,
            error_message: row.error_message,
            changes_uploaded: row.changes_uploaded,
            changes_downloaded: row.changes_downloaded,
            conflicts_encountered: row.conflicts_encountered,
            bytes_transferred: row.bytes_transferred,
            network_type: row.network_type,
            duration_seconds: row.duration_seconds,
        })
    }
}

impl FromStr for ConflictResolutionStatus {
    type Err = DomainError;
    fn from_str(s: &str) -> Result<Self, Self::Err> {
        match s {
            "resolved" => Ok(Self::Resolved),
            "unresolved" => Ok(Self::Unresolved),
            "manual" => Ok(Self::Manual),
            "ignored" => Ok(Self::Ignored),
            _ => Err(DomainError::Validation(ValidationError::custom(
                &format!("Invalid ConflictResolutionStatus string: {}", s)
            )))
        }
    }
}

impl FromStr for ConflictResolutionStrategy {
    type Err = DomainError;
    fn from_str(s: &str) -> Result<Self, Self::Err> {
        match s {
            "server_wins" => Ok(Self::ServerWins),
            "client_wins" => Ok(Self::ClientWins),
            "last_write_wins" => Ok(Self::LastWriteWins),
            "merge_prioritize_server" => Ok(Self::MergePrioritizeServer),
            "merge_prioritize_client" => Ok(Self::MergePrioritizeClient),
            "manual" => Ok(Self::Manual),
            _ => Err(DomainError::Validation(ValidationError::custom(
                &format!("Invalid ConflictResolutionStrategy string: {}", s)
            )))
        }
    }
}

impl FromStr for SyncMode {
    type Err = DomainError;
    fn from_str(s: &str) -> Result<Self, Self::Err> {
        match s {
            "full" => Ok(Self::Full),
            "incremental" => Ok(Self::Incremental),
            "minimal" => Ok(Self::Minimal),
            "selective" => Ok(Self::Selective),
            _ => Err(DomainError::Validation(ValidationError::custom(
                &format!("Invalid SyncMode string: {}", s)
            )))
        }
    }
}

// Aliases for HTTP sync transport
pub type RemoteChange = ChangeLogEntry;
pub type PushPayload = UploadChangesDto;
pub type PushChangesResponse = UploadChangesResponse;
pub type FetchChangesResponse = DownloadChangesResponse;

// ----- Sync Configuration -----
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SyncConfig {
    pub user_id: Uuid,
    pub sync_interval_minutes: i64,
    pub background_sync_enabled: bool,
    pub wifi_only: bool,
    pub charging_only: bool,
    pub sync_priority_threshold: i64,
    pub document_sync_enabled: bool,
    pub metadata_sync_enabled: bool,
    pub server_token: Option<String>,
    pub last_sync_timestamp: Option<DateTime<Utc>>,
    pub created_at: DateTime<Utc>,
    pub updated_at: DateTime<Utc>,
}

// ----- Sync Status Overview -----
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SyncStatus {
    pub user_id: Uuid,
    pub last_sync_timestamp: Option<DateTime<Utc>>,
    pub last_device_sync: Option<DateTime<Utc>>,
    pub sync_enabled: bool,
    pub offline_mode: bool,
    pub pending_changes: i64,
    pub pending_documents: i64,
    pub sync_in_progress: bool,
}

// ----- Sync Operation Log -----
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SyncOperationLog {
    pub id: Uuid,
    pub batch_id: String,
    pub operation: String,
    pub entity_type: Option<String>,
    pub entity_id: Option<Uuid>,
    pub status: String,
    pub error_message: Option<String>,
    pub blob_key: Option<String>,
    pub created_at: DateTime<Utc>,
}

// ----- Sync Queue Item -----
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SyncQueueItem {
    pub id: Uuid,
    pub sync_batch_id: Option<String>,
    pub entity_id: Uuid,
    pub entity_type: String,
    pub operation_type: String,
    pub status: String,
    pub blob_key: Option<String>,
    pub created_at: DateTime<Utc>,
    pub completed_at: Option<DateTime<Utc>>,
    pub error_message: Option<String>,
    pub retry_count: i64,
}

// src/domains/sync/repository.rs
use std::collections::HashMap;
use sqlx::{SqlitePool, Sqlite, Transaction, query, query_as, QueryBuilder, Row};
use uuid::Uuid;
use chrono::{DateTime, Utc};
use async_trait::async_trait;
use serde_json::json;

use crate::errors::{DbError, DomainError, DomainResult, ValidationError};
use crate::auth::AuthContext;
use crate::domains::sync::types::{
    SyncPriority, SyncBatchStatus, SyncDirection, SyncBatch, SyncConfig, SyncStatus,
    DeviceSyncState, ChangeLogEntry, ChangeOperationType, Tombstone, SyncConflict,
};

/// Repository for sync-related operations and tracking
#[async_trait]
pub trait SyncRepository: Send + Sync {
    /// Create a new sync batch for tracking a sync operation
    async fn create_sync_batch(&self, batch: &SyncBatch) -> DomainResult<()>;

    /// Update a sync batch's status
    async fn update_sync_batch_status(
        &self,
        batch_id: &str,
        status: SyncBatchStatus,
        error_message: Option<&str>
    ) -> DomainResult<()>;

    /// Update batch stats within a transaction
    async fn update_batch_stats<'t>(
        &self,
        batch_id: &str,
        processed: u32,
        conflicts: u32,
        errors: u32,
        tx: &mut Transaction<'t, Sqlite>
    ) -> DomainResult<()>;

    /// Finalize a sync batch with overall results
    async fn finalize_sync_batch(
        &self,
        batch_id: &str,
        status: SyncBatchStatus,
        error_message: Option<&str>,
        total_processed: u32
    ) -> DomainResult<()>;

    /// Get a user's sync configuration
    async fn get_sync_config(&self, user_id: Uuid) -> DomainResult<SyncConfig>;

    /// Update a user's sync configuration
    async fn update_sync_config(&self, config: &SyncConfig) -> DomainResult<()>;

    /// Get a user's sync status overview
    async fn get_sync_status(&self, user_id: Uuid) -> DomainResult<SyncStatus>;

    /// Update the sync state token (server_token in sync_configs)
    async fn update_sync_state_token(&self, user_id: Uuid, token: Option<String>) -> DomainResult<()>;

    /// Get device sync state
    async fn get_device_sync_state(&self, device_id: Uuid) -> DomainResult<DeviceSyncState>;

    /// Update device sync state
    async fn update_device_sync_state(&self, state: &DeviceSyncState) -> DomainResult<()>;

    /// Log a sync conflict
    async fn log_sync_conflict<'t>(
        &self,
        conflict: &SyncConflict,
        tx: &mut Transaction<'t, Sqlite>
    ) -> DomainResult<()>;

    /// Find conflicts for a batch
    async fn find_conflicts_for_batch(&self, batch_id: &str) -> DomainResult<Vec<ChangeLogEntry>>;
}

/// Repository for change log operations
#[async_trait]
pub trait ChangeLogRepository: Send + Sync {
    /// Create a new change log entry
    async fn create_change_log(&self, entry: &ChangeLogEntry) -> DomainResult<()>;

    /// Create a new change log entry within a transaction
    async fn create_change_log_with_tx<'t>(
        &self,
        entry: &ChangeLogEntry,
        tx: &mut Transaction<'t, Sqlite>
    ) -> DomainResult<()>;

    /// Find unprocessed changes by priority
    async fn find_unprocessed_changes_by_priority(
        &self,
        priority: SyncPriority,
        limit: u32
    ) -> DomainResult<Vec<ChangeLogEntry>>;

    /// Mark a change log entry as processed
    async fn mark_as_processed<'t>(
        &self,
        operation_id: Uuid,
        batch_id: &str,
        timestamp: DateTime<Utc>,
        tx: &mut Transaction<'t, Sqlite>
    ) -> DomainResult<()>;

    /// Get changes for a specific entity
    async fn get_changes_for_entity(
        &self,
        entity_table: &str,
        entity_id: Uuid,
        limit: u32
    ) -> DomainResult<Vec<ChangeLogEntry>>;

    /// Get last change timestamp for an entity field
    async fn get_last_field_change_timestamp(
        &self,
        entity_table: &str,
        entity_id: Uuid,
        field_name: &str
    ) -> DomainResult<Option<DateTime<Utc>>>;
}

/// Repository for tombstone operations
#[async_trait]
pub trait TombstoneRepository: Send + Sync {
    /// Create a new tombstone
    async fn create_tombstone(&self, tombstone: &Tombstone) -> DomainResult<()>;

    /// Create a new tombstone within a transaction
    async fn create_tombstone_with_tx<'t>(
        &self,
        tombstone: &Tombstone,
        tx: &mut Transaction<'t, Sqlite>
    ) -> DomainResult<()>;

    /// Find unpushed tombstones for sync
    async fn find_unpushed_tombstones(&self, limit: u32) -> DomainResult<Vec<Tombstone>>;

    /// Mark a tombstone as pushed (by marking its change_log entry)
    async fn mark_as_pushed<'t>(
        &self,
        tombstone_id: Uuid,
        batch_id: &str,
        timestamp: DateTime<Utc>,
        tx: &mut Transaction<'t, Sqlite>
    ) -> DomainResult<()>;

    /// Check if entity was already tombstoned
    async fn check_entity_tombstoned(
        &self,
        entity_type: &str,
        entity_id: Uuid
    ) -> DomainResult<bool>;

    /// Find tombstones since a specific date
    async fn find_tombstones_since(
        &self,
        user_id: Uuid,
        since: DateTime<Utc>,
        table_filter: Option<&str>
    ) -> DomainResult<Vec<Tombstone>>;
}

/// SQLite implementation of the SyncRepository
pub struct SqliteSyncRepository {
    pool: SqlitePool,
}

impl SqliteSyncRepository {
    pub fn new(pool: SqlitePool) -> Self {
        Self { pool }
    }
}

#[async_trait]
impl SyncRepository for SqliteSyncRepository {
    async fn create_sync_batch(&self, batch: &SyncBatch) -> DomainResult<()> {
        let mut builder = QueryBuilder::new(
            "INSERT INTO sync_batches (batch_id, device_id, direction, status, "
        );
        builder.push("item_count, total_size, priority, attempts, last_attempt_at, error_message, created_at, completed_at) VALUES (");
        
        let batch_id = &batch.batch_id;
        let device_id = batch.device_id.to_string();
        let direction = batch.direction.as_str();
        let status = batch.status.as_str();
        let created_at = batch.created_at.to_rfc3339();
        
        builder.push_bind(batch_id);
        builder.push(", ");
        builder.push_bind(device_id);
        builder.push(", ");
        builder.push_bind(direction);
        builder.push(", ");
        builder.push_bind(status);
        builder.push(", ");
        
        // Handle optional fields
        if let Some(item_count) = batch.item_count {
            builder.push_bind(item_count);
        } else {
            builder.push("NULL");
        }
        builder.push(", ");
        
        if let Some(total_size) = batch.total_size {
            builder.push_bind(total_size);
        } else {
            builder.push("NULL");
        }
        builder.push(", ");
        
        if let Some(priority) = batch.priority {
            builder.push_bind(priority);
        } else {
            builder.push("NULL");
        }
        builder.push(", ");
        
        if let Some(attempts) = batch.attempts {
            builder.push_bind(attempts);
        } else {
            builder.push("NULL");
        }
        builder.push(", ");
        
        if let Some(last_attempt) = batch.last_attempt_at {
            let last_attempt_str = last_attempt.to_rfc3339();
            builder.push_bind(last_attempt_str);
        } else {
            builder.push("NULL");
        }
        builder.push(", ");
        
        if let Some(error_msg) = &batch.error_message {
            builder.push_bind(error_msg);
        } else {
            builder.push("NULL");
        }
        builder.push(", ");
        
        builder.push_bind(created_at);
        builder.push(", ");
        
        if let Some(completed_at) = batch.completed_at {
            let completed_at_str = completed_at.to_rfc3339();
            builder.push_bind(completed_at_str);
        } else {
            builder.push("NULL");
        }
        
        builder.push(")");
        
        let query = builder.build();
        query.execute(&self.pool)
            .await
            .map_err(DbError::from)?;

        Ok(())
    }

    async fn update_sync_batch_status(
        &self, 
        batch_id: &str, 
        status: SyncBatchStatus,
        error_message: Option<&str>
    ) -> DomainResult<()> {
        let mut query_builder = QueryBuilder::new(
            "UPDATE sync_batches SET status = "
        );
        
        let status_str = status.as_str();
        query_builder.push_bind(status_str);
        query_builder.push(", attempts = attempts + 1, last_attempt_at = ");
        query_builder.push("strftime('%Y-%m-%dT%H:%M:%fZ', 'now')");
        
        query_builder.push(", error_message = ");
        if let Some(msg) = error_message {
            query_builder.push_bind(msg);
        } else {
            query_builder.push("NULL");
        }
        
        query_builder.push(" WHERE batch_id = ");
        query_builder.push_bind(batch_id);
        
        let query = query_builder.build();
        query.execute(&self.pool)
            .await
            .map_err(DbError::from)?;

        Ok(())
    }

    async fn update_batch_stats<'t>(
        &self,
        batch_id: &str,
        processed: u32,
        _conflicts: u32,
        errors: u32,
        tx: &mut Transaction<'t, Sqlite>
    ) -> DomainResult<()> {
        query(
            r#"
            UPDATE sync_batches
            SET
                item_count = item_count + ?,
                status = CASE WHEN ? > 0 THEN 'partially_failed' ELSE status END
            WHERE batch_id = ?
            "#,
        )
        .bind(processed as i64)
        .bind(errors as i64)
        .bind(batch_id)
        .execute(&mut **tx)
        .await
        .map_err(DbError::from)?;

        Ok(())
    }

    async fn finalize_sync_batch(
        &self,
        batch_id: &str,
        status: SyncBatchStatus,
        error_message: Option<&str>,
        total_processed: u32
    ) -> DomainResult<()> {
        let mut builder = QueryBuilder::new(
            "UPDATE sync_batches SET status = "
        );
        
        let status_str = status.as_str();
        builder.push_bind(status_str);
        builder.push(", error_message = ");
        
        if let Some(msg) = error_message {
            builder.push_bind(msg);
        } else {
            builder.push("NULL");
        }
        
        builder.push(", item_count = ");
        builder.push_bind(total_processed as i64);
        
        builder.push(", completed_at = strftime('%Y-%m-%dT%H:%M:%fZ', 'now')");
        builder.push(" WHERE batch_id = ");
        builder.push_bind(batch_id);
        
        let query = builder.build();
        query.execute(&self.pool)
            .await
            .map_err(DbError::from)?;

        Ok(())
    }

    async fn get_sync_config(&self, user_id: Uuid) -> DomainResult<SyncConfig> {
        let user_id_str = user_id.to_string();
        
        let row = query!(
            r#"
            SELECT 
                user_id,
                sync_interval_minutes,
                background_sync_enabled,
                wifi_only,
                charging_only,
                sync_priority_threshold,
                document_sync_enabled,
                metadata_sync_enabled,
                server_token,
                last_sync_timestamp,
                created_at,
                updated_at
            FROM sync_configs 
            WHERE user_id = ?
            "#,
            user_id_str
        )
        .fetch_optional(&self.pool)
        .await
        .map_err(DbError::from)?;

        match row {
            Some(row) => {
                let last_sync = match &row.last_sync_timestamp {
                    Some(ts) => Some(
                        DateTime::parse_from_rfc3339(ts)
                            .map_err(|_| DomainError::Validation(ValidationError::format(
                                "last_sync_timestamp", &format!("Invalid RFC3339 format: {}", ts)
                            )))?
                            .with_timezone(&Utc)
                    ),
                    None => None,
                };

                let created_at = DateTime::parse_from_rfc3339(&row.created_at)
                    .map_err(|_| DomainError::Validation(ValidationError::format(
                        "created_at", &format!("Invalid RFC3339 format: {}", row.created_at)
                    )))?
                    .with_timezone(&Utc);

                let updated_at = DateTime::parse_from_rfc3339(&row.updated_at)
                    .map_err(|_| DomainError::Validation(ValidationError::format(
                        "updated_at", &format!("Invalid RFC3339 format: {}", row.updated_at)
                    )))?
                    .with_timezone(&Utc);
                
                Ok(SyncConfig {
                    user_id,
                    sync_interval_minutes: row.sync_interval_minutes,
                    background_sync_enabled: row.background_sync_enabled == 1,
                    wifi_only: row.wifi_only == 1,
                    charging_only: row.charging_only == 1,
                    sync_priority_threshold: row.sync_priority_threshold,
                    document_sync_enabled: row.document_sync_enabled == 1,
                    metadata_sync_enabled: row.metadata_sync_enabled == 1,
                    server_token: row.server_token,
                    last_sync_timestamp: last_sync,
                    created_at,
                    updated_at,
                })
            },
            None => {
                // Create default config
                let now = Utc::now();
                let default_config = SyncConfig {
                    user_id,
                    sync_interval_minutes: 60,
                    background_sync_enabled: true,
                    wifi_only: true,
                    charging_only: false,
                    sync_priority_threshold: 1,
                    document_sync_enabled: true,
                    metadata_sync_enabled: true,
                    server_token: None,
                    last_sync_timestamp: None,
                    created_at: now,
                    updated_at: now,
                };

                let config_id = Uuid::new_v4().to_string();
                let now_str = now.to_rfc3339();
                let user_id_str = user_id.to_string();
                let bg_sync_enabled = if default_config.background_sync_enabled { 1 } else { 0 };
                let wifi_only = if default_config.wifi_only { 1 } else { 0 };
                let charging_only = if default_config.charging_only { 1 } else { 0 };
                let doc_sync_enabled = if default_config.document_sync_enabled { 1 } else { 0 };
                let meta_sync_enabled = if default_config.metadata_sync_enabled { 1 } else { 0 };

                query!(
                    r#"
                    INSERT INTO sync_configs (
                        id, user_id, sync_interval_minutes, background_sync_enabled, wifi_only,
                        charging_only, sync_priority_threshold, document_sync_enabled,
                        metadata_sync_enabled, server_token, last_sync_timestamp, created_at, updated_at
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    "#,
                    config_id,
                    user_id_str,
                    default_config.sync_interval_minutes,
                    bg_sync_enabled,
                    wifi_only,
                    charging_only,
                    default_config.sync_priority_threshold,
                    doc_sync_enabled,
                    meta_sync_enabled,
                    default_config.server_token,
                    None::<String>, // last_sync_timestamp is None initially
                    now_str,
                    now_str
                )
                .execute(&self.pool)
                .await
                .map_err(DbError::from)?;

                Ok(default_config)
            }
        }
    }

    async fn update_sync_config(&self, config: &SyncConfig) -> DomainResult<()> {
        let now_str = Utc::now().to_rfc3339();
        let last_sync_str = config.last_sync_timestamp.map(|dt| dt.to_rfc3339());
        let user_id_str = config.user_id.to_string();
        let bg_sync_enabled = if config.background_sync_enabled { 1 } else { 0 };
        let wifi_only = if config.wifi_only { 1 } else { 0 };
        let charging_only = if config.charging_only { 1 } else { 0 };
        let doc_sync_enabled = if config.document_sync_enabled { 1 } else { 0 };
        let meta_sync_enabled = if config.metadata_sync_enabled { 1 } else { 0 };

        query!(
            r#"
            UPDATE sync_configs SET
                sync_interval_minutes = ?,
                background_sync_enabled = ?,
                wifi_only = ?,
                charging_only = ?,
                sync_priority_threshold = ?,
                document_sync_enabled = ?,
                metadata_sync_enabled = ?,
                server_token = ?,
                last_sync_timestamp = ?,
                updated_at = ?
            WHERE user_id = ?
            "#,
            config.sync_interval_minutes,
            bg_sync_enabled,
            wifi_only,
            charging_only,
            config.sync_priority_threshold,
            doc_sync_enabled,
            meta_sync_enabled,
            config.server_token,
            last_sync_str,
            now_str,
            user_id_str
        )
        .execute(&self.pool)
        .await
        .map_err(DbError::from)?;

        Ok(())
    }

    async fn get_sync_status(&self, user_id: Uuid) -> DomainResult<SyncStatus> {
        // Get basic config
        let config = self.get_sync_config(user_id).await?;
        
        // Get device sync state with last attempt timestamp
        let user_id_str = user_id.to_string();
        let device_state_row = query!(
            r#"
            SELECT 
                device_id,
                last_sync_attempt_at
            FROM device_sync_state
            WHERE user_id = ?
            ORDER BY last_sync_attempt_at DESC
            LIMIT 1
            "#,
            user_id_str
        )
        .fetch_optional(&self.pool)
        .await
        .map_err(DbError::from)?;
        
        let last_device_sync = match device_state_row {
            Some(row) => match &row.last_sync_attempt_at {
                Some(ts) => DateTime::parse_from_rfc3339(ts).ok().map(|dt| dt.with_timezone(&Utc)),
                None => None
            },
            None => None
        };
        
        // Get pending changes count
        let pending_changes = query!(
            r#"
            SELECT COUNT(*) as count 
            FROM change_log 
            WHERE user_id = ? AND sync_batch_id IS NULL AND processed_at IS NULL
            "#,
            user_id_str
        )
        .fetch_one(&self.pool)
        .await
        .map_err(DbError::from)?
        .count;
        
        // Get pending document uploads count
        let pending_docs = query!(
            r#"
            SELECT COUNT(*) as count 
            FROM media_documents 
            WHERE blob_status = 'pending' AND deleted_at IS NULL 
            AND created_by_user_id = ?
            "#,
            user_id_str
        )
        .fetch_one(&self.pool)
        .await
        .map_err(DbError::from)?
        .count;
        
        // Check for active sync
        let active_syncs = query!(
            r#"
            SELECT COUNT(*) as count 
            FROM sync_batches sb
            JOIN device_sync_state dss ON sb.device_id = dss.device_id
            WHERE dss.user_id = ? AND sb.status IN ('pending', 'processing')
            "#,
            user_id_str
        )
        .fetch_one(&self.pool)
        .await
        .map_err(DbError::from)?
        .count;

        Ok(SyncStatus {
            user_id,
            last_sync_timestamp: config.last_sync_timestamp,
            last_device_sync,
            sync_enabled: config.background_sync_enabled,
            offline_mode: false,
            pending_changes,
            pending_documents: pending_docs,
            sync_in_progress: active_syncs > 0,
        })
    }

    async fn update_sync_state_token(&self, user_id: Uuid, token: Option<String>) -> DomainResult<()> {
        let user_id_str = user_id.to_string();
        
        query!(
            r#"
            UPDATE sync_configs
            SET server_token = ?,
                updated_at = strftime('%Y-%m-%dT%H:%M:%fZ', 'now')
            WHERE user_id = ?
            "#,
            token,
            user_id_str
        )
        .execute(&self.pool)
        .await
        .map_err(DbError::from)?;
        
        Ok(())
    }

    async fn get_device_sync_state(&self, device_id: Uuid) -> DomainResult<DeviceSyncState> {
        let device_id_str = device_id.to_string();
        
        let row = query!(
            r#"
            SELECT 
                device_id,
                user_id,
                last_upload_timestamp,
                last_download_timestamp,
                last_sync_status,
                last_sync_attempt_at,
                server_version,
                sync_enabled,
                created_at,
                updated_at
            FROM device_sync_state
            WHERE device_id = ?
            "#,
            device_id_str
        )
        .fetch_optional(&self.pool)
        .await
        .map_err(DbError::from)?;
        
        match row {
            Some(row) => {
                // Parse user_id
                let user_id = Uuid::parse_str(&row.user_id)
                    .map_err(|_| DomainError::Validation(ValidationError::format(
                        "user_id", &format!("Invalid UUID format: {}", row.user_id)
                    )))?;
                
                // Parse timestamps
                let last_upload = match &row.last_upload_timestamp {
                    Some(ts) => Some(
                        DateTime::parse_from_rfc3339(ts)
                            .map_err(|_| DomainError::Validation(ValidationError::format(
                                "last_upload_timestamp", &format!("Invalid RFC3339 format: {}", ts)
                            )))?
                            .with_timezone(&Utc)
                    ),
                    None => None,
                };
                
                let last_download = match &row.last_download_timestamp {
                    Some(ts) => Some(
                        DateTime::parse_from_rfc3339(ts)
                            .map_err(|_| DomainError::Validation(ValidationError::format(
                                "last_download_timestamp", &format!("Invalid RFC3339 format: {}", ts)
                            )))?
                            .with_timezone(&Utc)
                    ),
                    None => None,
                };
                
                let last_attempt = match &row.last_sync_attempt_at {
                    Some(ts) => Some(
                        DateTime::parse_from_rfc3339(ts)
                            .map_err(|_| DomainError::Validation(ValidationError::format(
                                "last_sync_attempt_at", &format!("Invalid RFC3339 format: {}", ts)
                            )))?
                            .with_timezone(&Utc)
                    ),
                    None => None,
                };
                
                let created_at = DateTime::parse_from_rfc3339(&row.created_at)
                    .map_err(|_| DomainError::Validation(ValidationError::format(
                        "created_at", &format!("Invalid RFC3339 format: {}", row.created_at)
                    )))?
                    .with_timezone(&Utc);
                    
                let updated_at = DateTime::parse_from_rfc3339(&row.updated_at)
                    .map_err(|_| DomainError::Validation(ValidationError::format(
                        "updated_at", &format!("Invalid RFC3339 format: {}", row.updated_at)
                    )))?
                    .with_timezone(&Utc);
                
                // Convert last_sync_status string to DeviceSyncStatus enum if present
                let last_sync_status = match &row.last_sync_status {
                    Some(status_str) => match status_str.as_str() {
                        "success" => Some(crate::domains::sync::types::DeviceSyncStatus::Success),
                        "partial_success" => Some(crate::domains::sync::types::DeviceSyncStatus::PartialSuccess),
                        "failed" => Some(crate::domains::sync::types::DeviceSyncStatus::Failed),
                        "in_progress" => Some(crate::domains::sync::types::DeviceSyncStatus::InProgress),
                        _ => None
                    },
                    None => None
                };
                
                Ok(DeviceSyncState {
                    device_id,
                    user_id,
                    last_upload_timestamp: last_upload,
                    last_download_timestamp: last_download,
                    last_sync_status,
                    last_sync_attempt_at: last_attempt,
                    server_version: Some(row.server_version.unwrap_or(0)),
                    sync_enabled: Some(row.sync_enabled.unwrap_or(0) == 1),
                    created_at,
                    updated_at,
                })
            },
            None => Err(DomainError::EntityNotFound("DeviceSyncState".to_string(), device_id))
        }
    }

    async fn update_device_sync_state(&self, state: &DeviceSyncState) -> DomainResult<()> {
        let now_str = Utc::now().to_rfc3339();
        let device_id_str = state.device_id.to_string();
        let user_id_str = state.user_id.to_string();
        let last_upload_str = state.last_upload_timestamp.map(|dt| dt.to_rfc3339());
        let last_download_str = state.last_download_timestamp.map(|dt| dt.to_rfc3339());
        let last_attempt_str = state.last_sync_attempt_at.map(|dt| dt.to_rfc3339());
        let created_at_str = state.created_at.to_rfc3339();
        
        // Convert DeviceSyncStatus enum to string if present
        let last_sync_status_str = state.last_sync_status.as_ref().map(|s| s.as_str());
        
        // Prepare values that would create temporary values in the query
        let server_version = state.server_version.unwrap_or(0);
        let sync_enabled = if state.sync_enabled.unwrap_or(false) { 1 } else { 0 };
        
        query!(
            r#"
            INSERT INTO device_sync_state (
                device_id, user_id, last_upload_timestamp, last_download_timestamp,
                last_sync_status, last_sync_attempt_at, server_version, sync_enabled,
                created_at, updated_at
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ON CONFLICT(device_id) DO UPDATE SET
                user_id = excluded.user_id,
                last_upload_timestamp = excluded.last_upload_timestamp,
                last_download_timestamp = excluded.last_download_timestamp,
                last_sync_status = excluded.last_sync_status,
                last_sync_attempt_at = excluded.last_sync_attempt_at,
                server_version = excluded.server_version,
                sync_enabled = excluded.sync_enabled,
                updated_at = excluded.updated_at
            "#,
            device_id_str,
            user_id_str,
            last_upload_str,
            last_download_str,
            last_sync_status_str,
            last_attempt_str,
            server_version,
            sync_enabled,
            created_at_str,
            now_str
        )
        .execute(&self.pool)
        .await
        .map_err(DbError::from)?;
        
        Ok(())
    }

    async fn log_sync_conflict<'t>(
        &self,
        conflict: &SyncConflict,
        tx: &mut Transaction<'t, Sqlite>
    ) -> DomainResult<()> {
        let mut details_map = HashMap::new();
        
        // Extract values for the JSON details
        if let Some(val) = &conflict.local_change.new_value {
            details_map.insert("local_value", val.clone());
        }
        if let Some(val) = &conflict.remote_change.new_value {
            details_map.insert("remote_value", val.clone());
        }
        
        let details_json = json!(details_map).to_string();
        
        // Prepare string values
        let conflict_id_str = conflict.conflict_id.to_string();
        let entity_id_str = conflict.entity_id.to_string();
        let local_op_id_str = conflict.local_change.operation_id.to_string();
        let remote_op_id_str = conflict.remote_change.operation_id.to_string();
        let resolution_status_str = conflict.resolution_status.as_str();
        let resolution_strategy_str = conflict.resolution_strategy.as_ref().map(|s| s.as_str());
        let resolved_by_str = conflict.resolved_by_user_id.map(|id| id.to_string());
        let resolved_at_str = conflict.resolved_at.map(|dt| dt.to_rfc3339());
        let created_at_str = conflict.created_at.to_rfc3339();
        
        query!(
            r#"
            INSERT INTO sync_conflicts (
                conflict_id, entity_table, entity_id, field_name,
                local_change_op_id, remote_change_op_id, resolution_status,
                resolution_strategy, resolved_by_user_id, resolved_at, created_at, details
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            "#,
            conflict_id_str,
            conflict.entity_table,
            entity_id_str,
            conflict.field_name,
            local_op_id_str,
            remote_op_id_str,
            resolution_status_str,
            resolution_strategy_str,
            resolved_by_str,
            resolved_at_str,
            created_at_str,
            details_json
        )
        .execute(&mut **tx)
        .await
        .map_err(DbError::from)?;
        
        Ok(())
    }

    async fn find_conflicts_for_batch(&self, batch_id: &str) -> DomainResult<Vec<ChangeLogEntry>> {
        // Helper function to safely get a &str from an Option<String>
        fn field_unwrap(opt: &Option<String>) -> &str {
            opt.as_deref().unwrap_or("")
        }
        
        let rows = query!(
            r#"
            SELECT 
                operation_id as "operation_id!", entity_table as "entity_table!",
                entity_id as "entity_id!", operation_type as "operation_type!",
                field_name, old_value, new_value, document_metadata,
                timestamp as "timestamp!", user_id as "user_id!", device_id,
                sync_batch_id, processed_at, sync_error
            FROM change_log
            WHERE sync_batch_id = ? AND sync_error IS NOT NULL
            "#,
            batch_id
        )
        .fetch_all(&self.pool)
        .await
        .map_err(DbError::from)?;
        
        let mut entries = Vec::with_capacity(rows.len());
        for row in rows {
            let operation_id = Uuid::parse_str(&row.operation_id)
                .map_err(|_| DomainError::Validation(ValidationError::format(
                    "operation_id", &format!("Invalid UUID: {}", &row.operation_id)
                )))?;
                
            let entity_id = Uuid::parse_str(&row.entity_id)
                .map_err(|_| DomainError::Validation(ValidationError::format(
                    "entity_id", &format!("Invalid UUID: {}", &row.entity_id)
                )))?;
                
            let user_id = Uuid::parse_str(&row.user_id)
                .map_err(|_| DomainError::Validation(ValidationError::format(
                    "user_id", &format!("Invalid UUID: {}", &row.user_id)
                )))?;
                
            let device_id = match &row.device_id {
                Some(id) => Some(Uuid::parse_str(id)
                    .map_err(|_| DomainError::Validation(ValidationError::format(
                        "device_id", &format!("Invalid UUID: {}", id)
                    )))?),
                None => None
            };
            
            let timestamp = DateTime::parse_from_rfc3339(&row.timestamp)
                .map_err(|_| DomainError::Validation(ValidationError::format(
                    "timestamp", &format!("Invalid RFC3339: {}", &row.timestamp)
                )))?
                .with_timezone(&Utc);
                
            let processed_at = match &row.processed_at {
                Some(ts) => Some(DateTime::parse_from_rfc3339(ts)
                    .map_err(|_| DomainError::Validation(ValidationError::format(
                        "processed_at", &format!("Invalid RFC3339: {}", ts)
                    )))?
                    .with_timezone(&Utc)),
                None => None
            };
            
            let operation_type = match row.operation_type.as_str() {
                "create" => ChangeOperationType::Create,
                "update" => ChangeOperationType::Update,
                "delete" => ChangeOperationType::Delete,
                "hard_delete" => ChangeOperationType::HardDelete,
                _ => ChangeOperationType::Update // Default
            };
            
            entries.push(ChangeLogEntry {
                operation_id,
                entity_table: row.entity_table.to_string(),
                entity_id,
                operation_type,
                field_name: row.field_name.clone(),
                old_value: row.old_value.clone(),
                new_value: row.new_value.clone(),
                timestamp,
                user_id,
                device_id,
                document_metadata: row.document_metadata.clone(),
                sync_batch_id: row.sync_batch_id.clone(),
                processed_at,
                sync_error: row.sync_error.clone(),
            });
        }
        
        Ok(entries)
    }
}

/// SQLite implementation of ChangeLogRepository
pub struct SqliteChangeLogRepository {
    pool: SqlitePool,
}

impl SqliteChangeLogRepository {
    pub fn new(pool: SqlitePool) -> Self {
        Self { pool }
    }
}

#[async_trait]
impl ChangeLogRepository for SqliteChangeLogRepository {
    async fn create_change_log(&self, entry: &ChangeLogEntry) -> DomainResult<()> {
        let mut tx = self.pool.begin().await.map_err(DbError::from)?;
        self.create_change_log_with_tx(entry, &mut tx).await?;
        tx.commit().await.map_err(DbError::from)?;
        Ok(())
    }

    async fn create_change_log_with_tx<'t>(
        &self,
        entry: &ChangeLogEntry,
        tx: &mut Transaction<'t, Sqlite>
    ) -> DomainResult<()> {
        // Generate priority value based on operation type
        let priority_value = match entry.operation_type {
            ChangeOperationType::Create => 7,
            ChangeOperationType::Update => 5,
            ChangeOperationType::Delete => 8,
            ChangeOperationType::HardDelete => 9,
        };
        
        // Convert values to strings
        let operation_id_str = entry.operation_id.to_string();
        let entity_id_str = entry.entity_id.to_string();
        let operation_type_str = entry.operation_type.as_str();
        let timestamp_str = entry.timestamp.to_rfc3339();
        let user_id_str = entry.user_id.to_string();
        let device_id_str = entry.device_id.map(|id| id.to_string());
        let processed_at_str = entry.processed_at.map(|dt| dt.to_rfc3339());
        
        query!(
            r#"
            INSERT INTO change_log (
                operation_id, entity_table, entity_id, operation_type, field_name,
                old_value, new_value, document_metadata, timestamp, user_id, device_id,
                sync_batch_id, processed_at, sync_error, priority
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            "#,
            operation_id_str,
            entry.entity_table,
            entity_id_str,
            operation_type_str,
            entry.field_name,
            entry.old_value,
            entry.new_value,
            entry.document_metadata,
            timestamp_str,
            user_id_str,
            device_id_str,
            entry.sync_batch_id,
            processed_at_str,
            entry.sync_error,
            priority_value
        )
        .execute(&mut **tx)
        .await
        .map_err(DbError::from)?;
        
        Ok(())
    }

    async fn find_unprocessed_changes_by_priority(
        &self,
        priority: SyncPriority,
        limit: u32
    ) -> DomainResult<Vec<ChangeLogEntry>> {
        // Helper function to safely get a &str from an Option<String>
        fn field_unwrap(opt: &Option<String>) -> &str {
            opt.as_deref().unwrap_or("")
        }
        
        let priority_val = match priority {
            SyncPriority::High => 8,
            SyncPriority::Normal => 5,
            SyncPriority::Low => 3,
            SyncPriority::Never => 0,
        };
        
        // Convert limit to i64 to avoid temporary value in query! macro
        let limit_val = limit as i64;
        
        let rows = query!(
            r#"
            SELECT 
                operation_id as "operation_id!", entity_table as "entity_table!", 
                entity_id as "entity_id!", operation_type as "operation_type!", 
                field_name, old_value, new_value, document_metadata, 
                timestamp as "timestamp!", user_id as "user_id!", device_id,
                sync_batch_id, processed_at, sync_error
            FROM change_log
            WHERE processed_at IS NULL AND sync_batch_id IS NULL
            AND priority >= ?
            ORDER BY priority DESC, timestamp ASC
            LIMIT ?
            "#,
            priority_val,
            limit_val
        )
        .fetch_all(&self.pool)
        .await
        .map_err(DbError::from)?;
        
        let mut entries = Vec::with_capacity(rows.len());
        for row in rows {
            let operation_id = Uuid::parse_str(&row.operation_id)
                .map_err(|_| DomainError::Validation(ValidationError::format(
                    "operation_id", &format!("Invalid UUID: {}", &row.operation_id)
                )))?;
                
            let entity_id = Uuid::parse_str(&row.entity_id)
                .map_err(|_| DomainError::Validation(ValidationError::format(
                    "entity_id", &format!("Invalid UUID: {}", &row.entity_id)
                )))?;
                
            let user_id = Uuid::parse_str(&row.user_id)
                .map_err(|_| DomainError::Validation(ValidationError::format(
                    "user_id", &format!("Invalid UUID: {}", &row.user_id)
                )))?;
                
            let device_id = match &row.device_id {
                Some(id) => Some(Uuid::parse_str(id)
                    .map_err(|_| DomainError::Validation(ValidationError::format(
                        "device_id", &format!("Invalid UUID: {}", id)
                    )))?),
                None => None
            };
            
            let timestamp = DateTime::parse_from_rfc3339(&row.timestamp)
                .map_err(|_| DomainError::Validation(ValidationError::format(
                    "timestamp", &format!("Invalid RFC3339: {}", &row.timestamp)
                )))?
                .with_timezone(&Utc);
                
            let processed_at = match &row.processed_at {
                Some(ts) => Some(DateTime::parse_from_rfc3339(ts)
                    .map_err(|_| DomainError::Validation(ValidationError::format(
                        "processed_at", &format!("Invalid RFC3339: {}", ts)
                    )))?
                    .with_timezone(&Utc)),
                None => None
            };
            
            let operation_type = match row.operation_type.as_str() {
                "create" => ChangeOperationType::Create,
                "update" => ChangeOperationType::Update,
                "delete" => ChangeOperationType::Delete,
                "hard_delete" => ChangeOperationType::HardDelete,
                _ => ChangeOperationType::Update // Default
            };
            
            entries.push(ChangeLogEntry {
                operation_id,
                entity_table: row.entity_table.to_string(),
                entity_id,
                operation_type,
                field_name: row.field_name.clone(),
                old_value: row.old_value.clone(),
                new_value: row.new_value.clone(),
                timestamp,
                user_id,
                device_id,
                document_metadata: row.document_metadata.clone(),
                sync_batch_id: row.sync_batch_id.clone(),
                processed_at,
                sync_error: row.sync_error.clone(),
            });
        }
        
        Ok(entries)
    }

    async fn mark_as_processed<'t>(
        &self,
        operation_id: Uuid,
        batch_id: &str,
        timestamp: DateTime<Utc>,
        tx: &mut Transaction<'t, Sqlite>
    ) -> DomainResult<()> {
        let timestamp_str = timestamp.to_rfc3339();
        let operation_id_str = operation_id.to_string();
        
        query!(
            r#"
            UPDATE change_log
            SET processed_at = ?,
                sync_batch_id = ?
            WHERE operation_id = ?
            "#,
            timestamp_str,
            batch_id,
            operation_id_str
        )
        .execute(&mut **tx)
        .await
        .map_err(DbError::from)?;
        
        Ok(())
    }

    async fn get_changes_for_entity(
        &self,
        entity_table: &str,
        entity_id: Uuid,
        limit: u32
    ) -> DomainResult<Vec<ChangeLogEntry>> {
        let entity_id_str = entity_id.to_string();
        let limit_i64 = limit as i64;
        
        let rows = query!(
            r#"
            SELECT 
                operation_id as "operation_id!", entity_table as "entity_table!", 
                entity_id as "entity_id!", operation_type as "operation_type!", 
                field_name, old_value, new_value, document_metadata, 
                timestamp as "timestamp!", user_id as "user_id!", device_id,
                sync_batch_id, processed_at, sync_error
            FROM change_log
            WHERE entity_table = ? AND entity_id = ?
            ORDER BY timestamp DESC
            LIMIT ?
            "#,
            entity_table,
            entity_id_str,
            limit_i64
        )
        .fetch_all(&self.pool)
        .await
        .map_err(DbError::from)?;
        
        let mut entries = Vec::with_capacity(rows.len());
        for row in rows {
            let operation_id = Uuid::parse_str(&row.operation_id)
                .map_err(|_| DomainError::Validation(ValidationError::format(
                    "operation_id", &format!("Invalid UUID: {}", &row.operation_id)
                )))?;
                
            let row_entity_id = Uuid::parse_str(&row.entity_id)
                .map_err(|_| DomainError::Validation(ValidationError::format(
                    "entity_id", &format!("Invalid UUID: {}", &row.entity_id)
                )))?;
                
            let user_id = Uuid::parse_str(&row.user_id)
                .map_err(|_| DomainError::Validation(ValidationError::format(
                    "user_id", &format!("Invalid UUID: {}", &row.user_id)
                )))?;
                
            let device_id = match &row.device_id {
                Some(id) => Some(Uuid::parse_str(id)
                    .map_err(|_| DomainError::Validation(ValidationError::format(
                        "device_id", &format!("Invalid UUID: {}", id)
                    )))?),
                None => None
            };
            
            let timestamp = DateTime::parse_from_rfc3339(&row.timestamp)
                .map_err(|_| DomainError::Validation(ValidationError::format(
                    "timestamp", &format!("Invalid RFC3339: {}", &row.timestamp)
                )))?
                .with_timezone(&Utc);
                
            let processed_at = match &row.processed_at {
                Some(ts) => Some(DateTime::parse_from_rfc3339(ts)
                    .map_err(|_| DomainError::Validation(ValidationError::format(
                        "processed_at", &format!("Invalid RFC3339: {}", ts)
                    )))?
                    .with_timezone(&Utc)),
                None => None
            };
            
            let operation_type = match row.operation_type.as_str() {
                "create" => ChangeOperationType::Create,
                "update" => ChangeOperationType::Update,
                "delete" => ChangeOperationType::Delete,
                "hard_delete" => ChangeOperationType::HardDelete,
                _ => ChangeOperationType::Update // Default
            };
            
            entries.push(ChangeLogEntry {
                operation_id,
                entity_table: row.entity_table.to_string(),
                entity_id: row_entity_id,
                operation_type,
                field_name: row.field_name.clone(),
                old_value: row.old_value.clone(),
                new_value: row.new_value.clone(),
                timestamp,
                user_id,
                device_id,
                document_metadata: row.document_metadata.clone(),
                sync_batch_id: row.sync_batch_id.clone(),
                processed_at,
                sync_error: row.sync_error.clone(),
            });
        }
        
        Ok(entries)
    }

    async fn get_last_field_change_timestamp(
        &self,
        entity_table: &str,
        entity_id: Uuid,
        field_name: &str
    ) -> DomainResult<Option<DateTime<Utc>>> {
        let entity_id_str = entity_id.to_string();
        
        let row = query!(
            r#"
            SELECT timestamp FROM change_log
            WHERE entity_table = ? AND entity_id = ? AND field_name = ?
            ORDER BY timestamp DESC
            LIMIT 1
            "#,
            entity_table,
            entity_id_str,
            field_name
        )
        .fetch_optional(&self.pool)
        .await
        .map_err(DbError::from)?;
        
        match row {
            Some(row) => {
                let timestamp = DateTime::parse_from_rfc3339(&row.timestamp)
                    .map_err(|_| DomainError::Validation(ValidationError::format(
                        "timestamp", &format!("Invalid RFC3339: {}", row.timestamp)
                    )))?
                    .with_timezone(&Utc);
                
                Ok(Some(timestamp))
            },
            None => Ok(None)
        }
    }
}

/// SQLite implementation of TombstoneRepository
pub struct SqliteTombstoneRepository {
    pool: SqlitePool,
}

impl SqliteTombstoneRepository {
    pub fn new(pool: SqlitePool) -> Self {
        Self { pool }
    }
}

#[async_trait]
impl TombstoneRepository for SqliteTombstoneRepository {
    async fn create_tombstone(&self, tombstone: &Tombstone) -> DomainResult<()> {
        let mut tx = self.pool.begin().await.map_err(DbError::from)?;
        self.create_tombstone_with_tx(tombstone, &mut tx).await?;
        tx.commit().await.map_err(DbError::from)?;
        Ok(())
    }

    async fn create_tombstone_with_tx<'t>(&self, tombstone: &Tombstone, tx: &mut Transaction<'t, Sqlite>) -> DomainResult<()> {
        // Convert values to strings
        let id_str = tombstone.id.to_string();
        let entity_id_str = tombstone.entity_id.to_string();
        let deleted_by_str = tombstone.deleted_by.to_string();
        let deleted_at_str = tombstone.deleted_at.to_rfc3339();
        let operation_id_str = tombstone.operation_id.to_string();
        
        query!(
            r#"
            INSERT INTO tombstones (
                id, entity_id, entity_type, deleted_by, deleted_at, operation_id, additional_metadata
            ) VALUES (?, ?, ?, ?, ?, ?, ?)
            "#,
            id_str,
            entity_id_str,
            tombstone.entity_type,
            deleted_by_str,
            deleted_at_str,
            operation_id_str,
            tombstone.additional_metadata
        )
        .execute(&mut **tx)
        .await
        .map_err(DbError::from)?;
        
        Ok(())
    }

    async fn find_unpushed_tombstones(&self, limit: u32) -> DomainResult<Vec<Tombstone>> {
        // Helper function to safely get a &str from an Option<String>
        fn field_unwrap(opt: &Option<String>) -> &str {
            opt.as_deref().unwrap_or("")
        }
        
        let limit_i64 = limit as i64;
        
        let rows = query!(
            r#"
            SELECT 
                id as "id!", entity_id as "entity_id!", entity_type as "entity_type!", 
                deleted_by as "deleted_by!", deleted_at as "deleted_at!", 
                operation_id as "operation_id!", additional_metadata
            FROM tombstones
            WHERE pushed_at IS NULL AND sync_batch_id IS NULL
            LIMIT ?
            "#,
            limit_i64
        )
        .fetch_all(&self.pool)
        .await
        .map_err(DbError::from)?;
        
        let mut tombstones = Vec::with_capacity(rows.len());
        for row in rows {
            let id = Uuid::parse_str(&row.id)
                .map_err(|_| DomainError::Validation(ValidationError::format(
                    "id", &format!("Invalid UUID: {}", &row.id)
                )))?;
                
            let entity_id = Uuid::parse_str(&row.entity_id)
                .map_err(|_| DomainError::Validation(ValidationError::format(
                    "entity_id", &format!("Invalid UUID: {}", &row.entity_id)
                )))?;
                
            let deleted_by = Uuid::parse_str(&row.deleted_by)
                .map_err(|_| DomainError::Validation(ValidationError::format(
                    "deleted_by", &format!("Invalid UUID: {}", &row.deleted_by)
                )))?;
                
            let operation_id = Uuid::parse_str(&row.operation_id)
                .map_err(|_| DomainError::Validation(ValidationError::format(
                    "operation_id", &format!("Invalid UUID: {}", &row.operation_id)
                )))?;
            
            let deleted_at = DateTime::parse_from_rfc3339(&row.deleted_at)
                .map_err(|_| DomainError::Validation(ValidationError::format(
                    "deleted_at", &format!("Invalid RFC3339: {}", &row.deleted_at)
                )))?
                .with_timezone(&Utc);
            
            tombstones.push(Tombstone {
                id,
                entity_id,
                entity_type: row.entity_type.to_string(),
                deleted_by,
                deleted_at,
                operation_id,
                additional_metadata: row.additional_metadata.clone()
            });
        }
        
        Ok(tombstones)
    }

    async fn mark_as_pushed<'t>(
        &self,
        tombstone_id: Uuid,
        batch_id: &str,
        timestamp: DateTime<Utc>,
        tx: &mut Transaction<'t, Sqlite>
    ) -> DomainResult<()> {
        let timestamp_str = timestamp.to_rfc3339();
        let tombstone_id_str = tombstone_id.to_string();
        
        query!(
            r#"
            UPDATE tombstones
            SET pushed_at = ?,
                sync_batch_id = ?
            WHERE id = ?
            "#,
            timestamp_str,
            batch_id,
            tombstone_id_str
        )
        .execute(&mut **tx)
        .await
        .map_err(DbError::from)?;
        
        Ok(())
    }

    async fn check_entity_tombstoned(&self, entity_type: &str, entity_id: Uuid) -> DomainResult<bool> {
        let entity_id_str = entity_id.to_string();
        
        let count = query!(
            r#"
            SELECT COUNT(*) as count FROM tombstones
            WHERE entity_type = ? AND entity_id = ?
            "#,
            entity_type,
            entity_id_str
        )
        .fetch_one(&self.pool)
        .await
        .map_err(DbError::from)?;
        
        Ok(count.count > 0)
    }
    
    async fn find_tombstones_since(
        &self,
        user_id: Uuid,
        since: DateTime<Utc>,
        table_filter: Option<&str>
    ) -> DomainResult<Vec<Tombstone>> {
        let user_id_str = user_id.to_string();
        let since_str = since.to_rfc3339();
        
        // Build query with conditional filter
        let mut sql = String::from(
            "SELECT id, entity_id, entity_type, deleted_by, deleted_at, operation_id, additional_metadata
             FROM tombstones
             WHERE deleted_by = ? AND deleted_at >= ?"
        );
        
        if let Some(table) = table_filter {
            sql.push_str(&format!(" AND entity_type = '{}'", table));
        }
        
        sql.push_str(" ORDER BY deleted_at DESC");
        
        // We need to use query_as_with for dynamic SQL
        let query = sqlx::query(&sql)
            .bind(user_id_str)
            .bind(since_str);
            
        let rows = query.fetch_all(&self.pool)
            .await
            .map_err(DbError::from)?;
            
        let mut tombstones = Vec::with_capacity(rows.len());
        for row in rows {
            // You need to extract values from each column by name since we're not using query! macro
            let id_str: String = row.get("id");
            let entity_id_str: String = row.get("entity_id");
            let entity_type: String = row.get("entity_type");
            let deleted_by_str: String = row.get("deleted_by");
            let deleted_at_str: String = row.get("deleted_at");
            let operation_id_str: String = row.get("operation_id");
            let additional_metadata: Option<String> = row.get("additional_metadata");
            
            let id = Uuid::parse_str(&id_str)
                .map_err(|_| DomainError::Validation(ValidationError::format(
                    "id", &format!("Invalid UUID: {}", &id_str)
                )))?;
                
            let entity_id = Uuid::parse_str(&entity_id_str)
                .map_err(|_| DomainError::Validation(ValidationError::format(
                    "entity_id", &format!("Invalid UUID: {}", &entity_id_str)
                )))?;
                
            let deleted_by = Uuid::parse_str(&deleted_by_str)
                .map_err(|_| DomainError::Validation(ValidationError::format(
                    "deleted_by", &format!("Invalid UUID: {}", &deleted_by_str)
                )))?;
                
            let operation_id = Uuid::parse_str(&operation_id_str)
                .map_err(|_| DomainError::Validation(ValidationError::format(
                    "operation_id", &format!("Invalid UUID: {}", &operation_id_str)
                )))?;
            
            let deleted_at = DateTime::parse_from_rfc3339(&deleted_at_str)
                .map_err(|_| DomainError::Validation(ValidationError::format(
                    "deleted_at", &format!("Invalid RFC3339: {}", &deleted_at_str)
                )))?
                .with_timezone(&Utc);
            
            tombstones.push(Tombstone {
                id,
                entity_id,
                entity_type,
                deleted_by,
                deleted_at,
                operation_id,
                additional_metadata
            });
        }
        
        Ok(tombstones)
    }
}


// /Users/sagarshrestha/ipad_rust_core copy/src/auth/repository.rs
use crate::errors::{DbError, DbResult};
use crate::domains::user::types::{User, UserRow};
use sqlx::{SqlitePool, query_as};
use uuid::Uuid;
use chrono::Utc;
use async_trait::async_trait;
use crate::types::UserRole;
use chrono::DateTime;

#[async_trait]
pub(crate) trait AuthRepository: Send + Sync {
    async fn find_user_by_email(&self, email: &str) -> DbResult<User>;
    async fn update_last_login(&self, user_id: Uuid) -> DbResult<()>;
    async fn log_login_attempt(&self, email: &str, success: bool, user_id: Option<Uuid>, device_id: &str) -> DbResult<()>;
    async fn log_logout(&self, user_id: Uuid, device_id: &str) -> DbResult<()>;
    async fn add_revoked_token(&self, jti: &str, expiry: i64) -> DbResult<()>;
    async fn is_token_revoked(&self, jti: &str) -> DbResult<bool>;
    async fn delete_expired_revoked_tokens(&self) -> DbResult<u64>;
}

pub(crate) struct SqliteAuthRepository {
    pool: SqlitePool,
}

impl SqliteAuthRepository {
    pub fn new(pool: SqlitePool) -> Self {
        Self { pool }
    }
}

#[async_trait]
impl AuthRepository for SqliteAuthRepository {
    async fn find_user_by_email(&self, email: &str) -> DbResult<User> {
        let row = query_as::<_, UserRow>(
            "SELECT * FROM users WHERE email = ? AND deleted_at IS NULL")
            .bind(email)
            .fetch_optional(&self.pool)
            .await
            .map_err(DbError::from)?
            .ok_or_else(|| DbError::NotFound("User".to_string(), email.to_string()))?;
        
        row.into_entity().map_err(|e| match e {
            crate::errors::DomainError::Database(db_err) => db_err,
            _ => DbError::Other(e.to_string()),
        })
    }
    
    async fn update_last_login(&self, user_id: Uuid) -> DbResult<()> {
        let now = Utc::now().to_rfc3339();
        
        // Use sqlx::query for UPDATE
        sqlx::query(
            "UPDATE users SET last_login = ? WHERE id = ?")
            .bind(now)
            .bind(user_id.to_string())
            .execute(&self.pool)
            .await
            .map_err(DbError::from)?;
            
        Ok(())
    }
    
    async fn log_login_attempt(&self, email: &str, success: bool, user_id: Option<Uuid>, device_id: &str) -> DbResult<()> {
        let log_id = Uuid::new_v4();
        let now = Utc::now().to_rfc3339();
        let action = if success { "login_success" } else { "login_fail" };
        let user_id_str = user_id.map(|id| id.to_string());
        
        // Use sqlx::query for INSERT
        sqlx::query(
            "INSERT INTO audit_logs (id, user_id, action, entity_table, entity_id, details, timestamp) 
             VALUES (?, ?, ?, 'users', ?, ?, ?)")
            .bind(log_id.to_string())
            .bind(user_id_str.as_deref())
            .bind(action)
            .bind(user_id_str.as_deref())
            .bind(format!("{{\"email\":\"{}\",\"device_id\":\"{}\"}}", email, device_id))
            .bind(now)
            .execute(&self.pool)
            .await
            .map_err(DbError::from)?;
            
        Ok(())
    }
    
    async fn log_logout(&self, user_id: Uuid, device_id: &str) -> DbResult<()> {
        let log_id = Uuid::new_v4();
        let now = Utc::now().to_rfc3339();
        
        // Use sqlx::query for INSERT
        sqlx::query(
            "INSERT INTO audit_logs (id, user_id, action, entity_table, entity_id, details, timestamp) 
             VALUES (?, ?, 'logout', 'users', ?, ?, ?)")
            .bind(log_id.to_string())
            .bind(user_id.to_string())
            .bind(user_id.to_string())
            .bind(format!("{{\"device_id\":\"{}\"}}", device_id))
            .bind(now)
            .execute(&self.pool)
            .await
            .map_err(DbError::from)?;
            
        Ok(())
    }

    async fn add_revoked_token(&self, jti: &str, expiry: i64) -> DbResult<()> {
        sqlx::query("INSERT OR IGNORE INTO revoked_tokens (jti, expiry) VALUES (?, ?)")
            .bind(jti)
            .bind(expiry)
            .execute(&self.pool)
            .await
            .map_err(DbError::from)?; // Ignore potential unique constraint violation if already revoked
        Ok(())
    }

    async fn is_token_revoked(&self, jti: &str) -> DbResult<bool> {
        let count: i64 = sqlx::query_scalar("SELECT COUNT(*) FROM revoked_tokens WHERE jti = ?")
            .bind(jti)
            .fetch_one(&self.pool)
            .await
            .map_err(DbError::from)?;
        Ok(count > 0)
    }

    async fn delete_expired_revoked_tokens(&self) -> DbResult<u64> {
        let now = Utc::now().timestamp();
        let result = sqlx::query("DELETE FROM revoked_tokens WHERE expiry < ?")
            .bind(now)
            .execute(&self.pool)
            .await
            .map_err(DbError::from)?;
        Ok(result.rows_affected())
    }
}

// /Users/sagarshrestha/ipad_rust_core copy/src/auth/context.rs
use uuid::Uuid;
use crate::types::{UserRole, Permission};
use crate::errors::ServiceError;

/// Represents the authentication context for the current operation
#[derive(Debug, Clone)]
pub struct AuthContext {
    /// The ID of the authenticated user
    pub user_id: Uuid,
    
    /// The role of the authenticated user
    pub role: UserRole,
    
    /// The ID of the current device
    pub device_id: String,
    
    /// Whether or not the app is currently in offline mode
    pub offline_mode: bool,
}

impl AuthContext {
    /// Create a new authentication context
    pub fn new(user_id: Uuid, role: UserRole, device_id: String, offline_mode: bool) -> Self {
        Self {
            user_id,
            role,
            device_id,
            offline_mode,
        }
    }
    
    /// Check if user has a specific permission
    pub fn has_permission(&self, permission: Permission) -> bool {
        self.role.has_permission(permission)
    }
    
    /// Authorize a specific permission, returning an error if not allowed
    pub fn authorize(&self, permission: Permission) -> Result<(), ServiceError> {
        if self.has_permission(permission) {
            Ok(())
        } else {
            Err(ServiceError::PermissionDenied(format!(
                "User does not have permission: {:?}",
                permission
            )))
        }
    }
    
    /// Authorize multiple permissions, requiring all of them
    pub fn authorize_all(&self, permissions: &[Permission]) -> Result<(), ServiceError> {
        if self.role.has_permissions(permissions) {
            Ok(())
        } else {
            Err(ServiceError::PermissionDenied(
                format!("User does not have all required permissions")
            ))
        }
    }
    
    /// Verify user is an admin
    pub fn authorize_admin(&self) -> Result<(), ServiceError> {
        if matches!(self.role, UserRole::Admin) {
            Ok(())
        } else {
            Err(ServiceError::PermissionDenied(
                "This action requires administrator privileges".to_string()
            ))
        }
    }
    
    /// Check if feature is available offline when in offline mode
    pub fn check_offline_feature(&self, feature_name: &str, available_offline: bool) -> Result<(), ServiceError> {
        if self.offline_mode && !available_offline {
            Err(ServiceError::OfflineFeatureUnavailable(feature_name.to_string()))
        } else {
            Ok(())
        }
    }
    
    /// For certain operations restricted to the user's own records
    pub fn authorize_self_or_admin(&self, resource_owner_id: &Uuid) -> Result<(), ServiceError> {
        if &self.user_id == resource_owner_id || matches!(self.role, UserRole::Admin) {
            Ok(())
        } else {
            Err(ServiceError::PermissionDenied(
                "You do not have permission to access this resource".to_string()
            ))
        }
    }
    
    /// Check if the user can hard delete records
    pub fn authorize_hard_delete(&self) -> Result<(), ServiceError> {
        if self.role.can_hard_delete() {
            Ok(())
        } else {
            Err(ServiceError::PermissionDenied(
                "You do not have permission to permanently delete records".to_string()
            ))
        }
    }
}

// /Users/sagarshrestha/ipad_rust_core copy/src/auth/jwt.rs
use serde::{Deserialize, Serialize};
use std::str::FromStr;
use uuid::Uuid;
use chrono::{DateTime, Utc};
use crate::errors::{ServiceError, ServiceResult, DomainError};
use crate::types::UserRole;
use std::sync::OnceLock;
use jsonwebtoken::{encode, decode, Header, EncodingKey, DecodingKey, Validation, Algorithm, TokenData};

#[derive(Debug, Serialize, Deserialize)]
pub struct Claims {
    pub sub: String,
    pub role: String,
    pub device_id: String,
    pub iat: i64,
    pub exp: i64,
    pub jti: String,
    pub refresh_exp: Option<i64>,
}

// JWT secret - in a real app this should be loaded from a secure environment variable
static JWT_SECRET: OnceLock<String> = OnceLock::new();

/// Token type
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum TokenType {
    /// Access token (short-lived)
    Access,
    /// Refresh token (long-lived)
    Refresh,
}

/// Initialize JWT module with secret
pub fn initialize(secret: &str) {
    JWT_SECRET.get_or_init(|| secret.to_string());
}

/// Get JWT secret
fn get_secret() -> ServiceResult<&'static str> {
    JWT_SECRET.get()
        .map(|s| s.as_str())
        .ok_or_else(|| ServiceError::Configuration("JWT secret not initialized".to_string()))
}

/// Generate a JWT token
pub fn generate_token(
    user_id: &Uuid,
    role: &UserRole,
    device_id: &str,
    token_type: TokenType,
) -> ServiceResult<(String, DateTime<Utc>)> {
    let secret = get_secret()?;
    
    let now = Utc::now();
    let token_id = Uuid::new_v4().to_string();
    
    // Set expiration based on token type
    let (expiry, refresh_exp) = match token_type {
        TokenType::Access => {
            // Access tokens expire in 15 minutes
            let exp = now + chrono::Duration::minutes(15);
            (exp, None)
        },
        TokenType::Refresh => {
            // Refresh tokens expire in 30 days
            let access_exp = now + chrono::Duration::minutes(15);
            let refresh_exp = now + chrono::Duration::days(30);
            (access_exp, Some(refresh_exp.timestamp()))
        }
    };
    
    // Create claims
    let claims = Claims {
        sub: user_id.to_string(),
        role: role.as_str().to_string(),
        device_id: device_id.to_string(),
        iat: now.timestamp(),
        exp: expiry.timestamp(),
        jti: token_id,
        refresh_exp,
    };
    
    // Encode token
    let token = jsonwebtoken::encode(
        &jsonwebtoken::Header::default(),
        &claims,
        &jsonwebtoken::EncodingKey::from_secret(secret.as_bytes()),
    )
    .map_err(|e| ServiceError::Domain(DomainError::Internal(format!("JWT encoding error: {}", e))))?;
    
    Ok((token, expiry))
}

/// Verify a JWT token
pub fn verify_token(token: &str) -> ServiceResult<Claims> {
    let secret = get_secret()?;
    
    // Decode and validate token
    let token_data = jsonwebtoken::decode::<Claims>(
        token,
        &jsonwebtoken::DecodingKey::from_secret(secret.as_bytes()),
        &jsonwebtoken::Validation::new(jsonwebtoken::Algorithm::HS256),
    )
    .map_err(|e| match e.kind() {
        jsonwebtoken::errors::ErrorKind::ExpiredSignature => ServiceError::SessionExpired,
        _ => ServiceError::Authentication(format!("Invalid token: {}", e)),
    })?;
    
    Ok(token_data.claims)
}

/// Generate a refresh token
pub fn generate_refresh_token(
    user_id: &Uuid,
    role: &UserRole,
    device_id: &str,
) -> ServiceResult<(String, DateTime<Utc>, DateTime<Utc>)> {
    let (token, access_expiry) = generate_token(user_id, role, device_id, TokenType::Refresh)?;
    
    // Parse claims to get refresh expiry
    let claims = verify_token(&token)?;
    let refresh_expiry = claims.refresh_exp
        .ok_or_else(|| ServiceError::Domain(DomainError::Internal("Refresh token missing refresh_exp".to_string())))?;
        
    let refresh_expiry_dt = DateTime::from_timestamp(refresh_expiry, 0)
        .ok_or_else(|| ServiceError::Domain(DomainError::Internal("Invalid refresh expiry timestamp".to_string())))?;
        
    Ok((token, access_expiry, refresh_expiry_dt))
}

/// Refresh an access token using a refresh token
pub fn refresh_access_token(refresh_token: &str) -> ServiceResult<(String, DateTime<Utc>)> {
    // Verify the refresh token first
    let claims = verify_token(refresh_token)?;
    
    // Ensure it's a refresh token
    if claims.refresh_exp.is_none() {
        return Err(ServiceError::Authentication("Not a refresh token".to_string()));
    }
    
    // Check if refresh token is expired
    let now = Utc::now().timestamp();
    if let Some(refresh_exp) = claims.refresh_exp {
        if refresh_exp < now {
            return Err(ServiceError::SessionExpired);
        }
    }
    
    // Parse user ID, role, and device ID from claims
    let user_id = Uuid::parse_str(&claims.sub)
        .map_err(|_| ServiceError::Authentication("Invalid user ID in token".to_string()))?;
        
    let role = UserRole::from_str(&claims.role)
        .ok_or_else(|| ServiceError::Authentication("Invalid role in token".to_string()))?;
        
    // Generate a new access token
    generate_token(&user_id, &role, &claims.device_id, TokenType::Access)
}

/// Decodes token claims without verifying signature or expiry.
/// Useful for retrieving JTI/expiry for logging out/revocation even if token is expired.
pub fn decode_unverified(token: &str) -> ServiceResult<Claims> {
    // Use a validation struct that ignores expiry and signature
    let mut validation = Validation::new(Algorithm::HS256);
    validation.validate_exp = false;
    validation.insecure_disable_signature_validation(); // Added to bypass signature check
    
    // Attempt to decode using a dummy key, as signature isn't checked
    // Note: The algorithm still needs to match the header for the library to proceed.
    let dummy_key = DecodingKey::from_secret(b"dummy");
    
    let token_data = decode::<Claims>(token, &dummy_key, &validation)
        .map_err(|e| {
            log::error!("Unverified token decode error: {}", e);
            // Distinguish between structural errors and others if needed
            ServiceError::Authentication(format!("Invalid token structure: {}", e))
        })?;
    
    Ok(token_data.claims)
}

/// Revoke a token (in a real app, this would add it to a blocklist)
pub fn revoke_token(token: &str) -> ServiceResult<()> {
    // Placeholder - Actual revocation logic is now in AuthService using the repository
    // This function could potentially decode the token to get JTI/expiry if needed elsewhere,
    // but currently AuthService handles that directly.
    log::warn!("jwt::revoke_token called, but it's a placeholder. Revocation handled by AuthService.");
    // You might decode here just to log the JTI being 'revoked' conceptually
    match decode_unverified(token) {
        Ok(claims) => log::info!("Placeholder revocation for JTI: {}", claims.jti),
        Err(_) => log::warn!("Could not decode token for placeholder revocation log.")
    }
    Ok(())
} 

// /Users/sagarshrestha/ipad_rust_core copy/src/domains/activity/repository.rs
use crate::auth::AuthContext;
use sqlx::{Executor, Row, Sqlite, Transaction, SqlitePool, QueryBuilder};
use crate::domains::core::delete_service::DeleteServiceRepository;
use crate::domains::core::repository::{FindById, HardDeletable, SoftDeletable};
use crate::domains::core::document_linking::DocumentLinkable;
use crate::domains::activity::types::{NewActivity, Activity, ActivityRow, UpdateActivity};
use crate::errors::{DbError, DomainError, DomainResult, ValidationError};
use crate::types::{PaginatedResult, PaginationParams, SyncPriority};
use async_trait::async_trait;
use chrono::Utc;
use sqlx::{query, query_as, query_scalar};
use uuid::Uuid;
use std::sync::Arc;
use serde_json;
use crate::domains::sync::repository::ChangeLogRepository;
use crate::domains::sync::types::{ChangeLogEntry, ChangeOperationType};

/// Trait defining activity repository operations
#[async_trait]
pub trait ActivityRepository: DeleteServiceRepository<Activity> + Send + Sync {
    async fn create(
        &self,
        new_activity: &NewActivity,
        auth: &AuthContext,
    ) -> DomainResult<Activity>;
    async fn create_with_tx<'t>(
        &self,
        new_activity: &NewActivity,
        auth: &AuthContext,
        tx: &mut Transaction<'t, Sqlite>,
    ) -> DomainResult<Activity>;

    async fn update(
        &self,
        id: Uuid,
        update_data: &UpdateActivity,
        auth: &AuthContext,
    ) -> DomainResult<Activity>;
    async fn update_with_tx<'t>(
        &self,
        id: Uuid,
        update_data: &UpdateActivity,
        auth: &AuthContext,
        tx: &mut Transaction<'t, Sqlite>,
    ) -> DomainResult<Activity>;

    async fn find_by_project_id(
        &self,
        project_id: Uuid,
        params: PaginationParams,
    ) -> DomainResult<PaginatedResult<Activity>>;
}

/// SQLite implementation for ActivityRepository
#[derive(Clone)]
pub struct SqliteActivityRepository {
    pool: SqlitePool,
    change_log_repo: Arc<dyn ChangeLogRepository + Send + Sync>,
}

impl SqliteActivityRepository {
    pub fn new(pool: SqlitePool, change_log_repo: Arc<dyn ChangeLogRepository + Send + Sync>) -> Self {
        Self { pool, change_log_repo }
    }

    fn map_row_to_entity(row: ActivityRow) -> DomainResult<Activity> {
        row.into_entity()
            .map_err(|e| DomainError::Internal(format!("Failed to map row to entity: {}", e)))
    }

    fn entity_name(&self) -> &'static str {
        "activities"
    }

    async fn find_by_id_with_tx<'t>(
        &self,
        id: Uuid,
        tx: &mut Transaction<'t, Sqlite>,
    ) -> DomainResult<Activity> {
        let row = query_as::<_, ActivityRow>(
            "SELECT * FROM activities WHERE id = ? AND deleted_at IS NULL",
        )
        .bind(id.to_string())
        .fetch_optional(&mut **tx)
        .await
        .map_err(DbError::from)?
        .ok_or_else(|| DomainError::EntityNotFound("Activity".to_string(), id))?;

        Self::map_row_to_entity(row)
    }

    async fn log_change_entry<'t>(
        &self,
        entry: ChangeLogEntry,
        tx: &mut Transaction<'t, Sqlite>,
    ) -> DomainResult<()> {
        self.change_log_repo.create_change_log_with_tx(&entry, tx).await
    }
}

#[async_trait]
impl FindById<Activity> for SqliteActivityRepository {
    async fn find_by_id(&self, id: Uuid) -> DomainResult<Activity> {
        let row = query_as::<_, ActivityRow>(
            "SELECT * FROM activities WHERE id = ? AND deleted_at IS NULL",
        )
        .bind(id.to_string())
        .fetch_optional(&self.pool)
        .await
        .map_err(DbError::from)?
        .ok_or_else(|| DomainError::EntityNotFound("Activity".to_string(), id))?;

        Self::map_row_to_entity(row)
    }
}

#[async_trait]
impl SoftDeletable for SqliteActivityRepository {
    async fn soft_delete_with_tx(
        &self,
        id: Uuid,
        auth: &AuthContext,
        tx: &mut Transaction<'_, Sqlite>,
    ) -> DomainResult<()> {
        let now = Utc::now();
        let now_str = now.to_rfc3339();
        let user_id = auth.user_id;
        let user_id_str = user_id.to_string();
     
        
        let result = query(
            "UPDATE activities SET deleted_at = ?, deleted_by_user_id = ? WHERE id = ? AND deleted_at IS NULL"
        )
        .bind(now_str)
        .bind(user_id_str)
        .bind(id.to_string())
        .execute(&mut **tx)
        .await
        .map_err(DbError::from)?;

        if result.rows_affected() == 0 {
            Err(DomainError::EntityNotFound("Activity".to_string(), id))
        } else {
            Ok(())
        }
    }

    async fn soft_delete(&self, id: Uuid, auth: &AuthContext) -> DomainResult<()> {
        let mut tx = self.pool.begin().await.map_err(DbError::from)?;
        match self.soft_delete_with_tx(id, auth, &mut tx).await {
            Ok(()) => {
                tx.commit().await.map_err(DbError::from)?;
                Ok(())
            }
            Err(e) => {
                let _ = tx.rollback().await;
                Err(e)
            }
        }
    }
}

#[async_trait]
impl HardDeletable for SqliteActivityRepository {
    fn entity_name(&self) -> &'static str {
        "activities"
    }
    
    async fn hard_delete_with_tx(
        &self,
        id: Uuid,
        auth: &AuthContext,
        tx: &mut Transaction<'_, Sqlite>,
    ) -> DomainResult<()> {
        let result = query("DELETE FROM activities WHERE id = ?")
            .bind(id.to_string())
            .execute(&mut **tx)
            .await
            .map_err(DbError::from)?;

        if result.rows_affected() == 0 {
            Err(DomainError::EntityNotFound("Activity".to_string(), id))
        } else {
            Ok(())
        }
    }

    async fn hard_delete(&self, id: Uuid, auth: &AuthContext) -> DomainResult<()> {
        let mut tx = self.pool.begin().await.map_err(DbError::from)?;
        match self.hard_delete_with_tx(id, auth, &mut tx).await {
            Ok(()) => {
                tx.commit().await.map_err(DbError::from)?;
                Ok(())
            }
            Err(e) => {
                let _ = tx.rollback().await;
                Err(e)
            }
        }
    }
}

// Blanket implementation in core::delete_service handles DeleteServiceRepository

#[async_trait]
impl ActivityRepository for SqliteActivityRepository {
    async fn create(
        &self,
        new_activity: &NewActivity,
        auth: &AuthContext,
    ) -> DomainResult<Activity> {
        let mut tx = self.pool.begin().await.map_err(DbError::from)?;
        match self.create_with_tx(new_activity, auth, &mut tx).await {
            Ok(activity) => {
                tx.commit().await.map_err(DbError::from)?;
                Ok(activity)
            }
            Err(e) => {
                let _ = tx.rollback().await;
                Err(e)
            }
        }
    }

    async fn create_with_tx<'t>(
        &self,
        new_activity: &NewActivity,
        auth: &AuthContext,
        tx: &mut Transaction<'t, Sqlite>,
    ) -> DomainResult<Activity> {
        let id = Uuid::new_v4();
        let now = Utc::now();
        let now_str = now.to_rfc3339();
        let user_id = auth.user_id; // Get Uuid directly
        let user_id_str = user_id.to_string();
        let device_uuid: Option<Uuid> = auth.device_id.parse::<Uuid>().ok();
        let project_id_str = new_activity.project_id.map(|id| id.to_string());

        query(
            r#"
            INSERT INTO activities (
                id, project_id,
                description, description_updated_at, description_updated_by,
                kpi, kpi_updated_at, kpi_updated_by,
                target_value, target_value_updated_at, target_value_updated_by,
                actual_value, actual_value_updated_at, actual_value_updated_by,
                status_id, status_id_updated_at, status_id_updated_by,
                sync_priority,
                created_at, updated_at, created_by_user_id, updated_by_user_id,
                deleted_at, deleted_by_user_id
            ) VALUES (
                ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, NULL, NULL
            )
            "#,
        )
        .bind(id.to_string())
        .bind(project_id_str)
        .bind(&new_activity.description).bind(new_activity.description.as_ref().map(|_| &now_str)).bind(new_activity.description.as_ref().map(|_| &user_id_str))
        .bind(&new_activity.kpi).bind(new_activity.kpi.as_ref().map(|_| &now_str)).bind(new_activity.kpi.as_ref().map(|_| &user_id_str))
        .bind(new_activity.target_value).bind(new_activity.target_value.map(|_| &now_str)).bind(new_activity.target_value.map(|_| &user_id_str))
        .bind(new_activity.actual_value.unwrap_or(0.0)).bind(new_activity.actual_value.map(|_| &now_str)).bind(new_activity.actual_value.map(|_| &user_id_str))
        .bind(new_activity.status_id)
        .bind(new_activity.status_id.map(|_| &now_str))
        .bind(new_activity.status_id.map(|_| &user_id_str))
        .bind(new_activity.sync_priority.as_str())
        .bind(&now_str).bind(&now_str)
        .bind(&user_id_str).bind(&user_id_str)
        .execute(&mut **tx)
        .await
        .map_err(DbError::from)?;

        // Log Create Operation
        let entry = ChangeLogEntry {
            operation_id: Uuid::new_v4(),
            entity_table: self.entity_name().to_string(),
            entity_id: id,
            operation_type: ChangeOperationType::Create,
            field_name: None,
            old_value: None,
            new_value: None,
            timestamp: now, // Use DateTime<Utc>
            user_id: user_id,
            device_id: device_uuid,
            document_metadata: None,
            sync_batch_id: None,
            processed_at: None,
            sync_error: None,
        };
        self.log_change_entry(entry, tx).await?; // Add log call here

        self.find_by_id_with_tx(id, tx).await
    }

    async fn update(
        &self,
        id: Uuid,
        update_data: &UpdateActivity,
        auth: &AuthContext,
    ) -> DomainResult<Activity> {
        let mut tx = self.pool.begin().await.map_err(DbError::from)?;
        match self.update_with_tx(id, update_data, auth, &mut tx).await {
            Ok(activity) => {
                tx.commit().await.map_err(DbError::from)?;
                Ok(activity)
            }
            Err(e) => {
                let _ = tx.rollback().await;
                Err(e)
            }
        }
    }

    async fn update_with_tx<'t>(
        &self,
        id: Uuid,
        update_data: &UpdateActivity,
        auth: &AuthContext,
        tx: &mut Transaction<'t, Sqlite>,
    ) -> DomainResult<Activity> {
        // Fetch old state first
        let old_entity = self.find_by_id_with_tx(id, tx).await?;

        let now = Utc::now();
        let now_str = now.to_rfc3339();
        let user_id = auth.user_id;
        let user_id_str = user_id.to_string();
        let id_str = id.to_string();
        let device_uuid: Option<Uuid> = auth.device_id.parse::<Uuid>().ok();

        let mut builder = QueryBuilder::new("UPDATE activities SET ");
        let mut separated = builder.separated(", ");
        let mut fields_updated = false;

        macro_rules! add_lww {
            ($field_name:ident, $field_sql:literal, $value:expr) => {
                if let Some(val) = $value { // Check if update DTO contains field
                    separated.push(concat!($field_sql, " = "));
                    separated.push_bind_unseparated(val.clone()); // Bind value
                    separated.push(concat!(" ", $field_sql, "_updated_at = "));
                    separated.push_bind_unseparated(now_str.clone()); // Bind timestamp
                    separated.push(concat!(" ", $field_sql, "_updated_by = "));
                    separated.push_bind_unseparated(user_id_str.clone()); // Bind user
                    fields_updated = true; // Mark SQL update needed
                }
            };
            // Variant for Option<Uuid> like project_id
            ($field_name:ident, $field_sql:literal, $value:expr, uuid_opt) => {
                if let Some(opt_val) = $value { // Check if update DTO contains field (even if inner value is None)
                    let val_str = opt_val.map(|id| id.to_string());
                    separated.push(concat!($field_sql, " = "));
                    separated.push_bind_unseparated(val_str); // Bind Option<String>
                    // FK changes don't have specific LWW columns in schema, rely on main updated_at
                    fields_updated = true; // Mark SQL update needed
                }
            };
        }

        // Apply updates using macros
        add_lww!(project_id, "project_id", &update_data.project_id, uuid_opt);
        add_lww!(description, "description", &update_data.description.as_ref());
        add_lww!(kpi, "kpi", &update_data.kpi.as_ref());
        add_lww!(target_value, "target_value", &update_data.target_value.as_ref());
        add_lww!(actual_value, "actual_value", &update_data.actual_value.as_ref());
        add_lww!(status_id, "status_id", &update_data.status_id.as_ref());
        // Update sync_priority if provided
        if let Some(sp) = &update_data.sync_priority {
            separated.push("sync_priority = ");
            separated.push_bind_unseparated(sp.as_str());
            fields_updated = true;
        }

        if !fields_updated {
            return Ok(old_entity); // No fields present in DTO
        }

        // Always update main timestamps
        separated.push("updated_at = ");
        separated.push_bind_unseparated(now_str.clone());
        separated.push("updated_by_user_id = ");
        separated.push_bind_unseparated(user_id_str.clone());

        // Finalize and Execute SQL
        builder.push(" WHERE id = ");
        builder.push_bind(id_str);
        builder.push(" AND deleted_at IS NULL");
        let query = builder.build();
        let result = query.execute(&mut **tx).await.map_err(DbError::from)?;
        if result.rows_affected() == 0 {
            return Err(DomainError::EntityNotFound(self.entity_name().to_string(), id));
        }

        // Fetch New State & Log Field Changes
        let new_entity = self.find_by_id_with_tx(id, tx).await?;

        macro_rules! log_if_changed {
            ($field_name:ident, $field_sql:literal) => {
                if old_entity.$field_name != new_entity.$field_name {
                    let entry = ChangeLogEntry {
                        operation_id: Uuid::new_v4(),
                        entity_table: self.entity_name().to_string(),
                        entity_id: id,
                        operation_type: ChangeOperationType::Update,
                        field_name: Some($field_sql.to_string()),
                        old_value: serde_json::to_string(&old_entity.$field_name).ok(),
                        new_value: serde_json::to_string(&new_entity.$field_name).ok(),
                        timestamp: now,
                        user_id: user_id,
                        device_id: device_uuid.clone(),
                        document_metadata: None,
                        sync_batch_id: None,
                        processed_at: None,
                        sync_error: None,
                    };
                    self.log_change_entry(entry, tx).await?;
                }
            };
        }

        log_if_changed!(project_id, "project_id");
        log_if_changed!(description, "description");
        log_if_changed!(kpi, "kpi");
        log_if_changed!(target_value, "target_value");
        log_if_changed!(actual_value, "actual_value");
        log_if_changed!(status_id, "status_id");
        
        Ok(new_entity)
    }

    async fn find_by_project_id(
        &self,
        project_id: Uuid,
        params: PaginationParams,
    ) -> DomainResult<PaginatedResult<Activity>> {
        let offset = (params.page - 1) * params.per_page;
        let project_id_str = project_id.to_string();

        let total: i64 = query_scalar(
             "SELECT COUNT(*) FROM activities WHERE project_id = ? AND deleted_at IS NULL"
         )
         .bind(&project_id_str)
         .fetch_one(&self.pool)
         .await
         .map_err(DbError::from)?;

        let rows = query_as::<_, ActivityRow>(
            "SELECT * FROM activities WHERE project_id = ? AND deleted_at IS NULL ORDER BY created_at ASC LIMIT ? OFFSET ?",
        )
        .bind(project_id_str)
        .bind(params.per_page as i64)
        .bind(offset as i64)
        .fetch_all(&self.pool)
        .await
        .map_err(DbError::from)?;

        let entities = rows
            .into_iter()
            .map(Self::map_row_to_entity)
            .collect::<DomainResult<Vec<Activity>>>()?;

        Ok(PaginatedResult::new(
            entities,
            total as u64,
            params,
        ))
    }
}


// /Users/sagarshrestha/ipad_rust_core copy/src/domains/activity/service.rs
use crate::auth::AuthContext;
use sqlx::{SqlitePool, Transaction, Sqlite};
use crate::domains::core::dependency_checker::DependencyChecker;
use crate::domains::core::delete_service::{BaseDeleteService, DeleteOptions, DeleteService, DeleteServiceRepository};
use crate::domains::core::repository::{DeleteResult, FindById, HardDeletable, SoftDeletable};
use crate::domains::core::document_linking::DocumentLinkable;
use crate::domains::permission::Permission;
use crate::domains::activity::repository::ActivityRepository;
use crate::domains::activity::types::{NewActivity, Activity, ActivityResponse, UpdateActivity, ActivityInclude};
use crate::domains::project::repository::ProjectRepository; // Needed for validation
use crate::domains::sync::repository::{ChangeLogRepository, TombstoneRepository};
use crate::errors::{DomainError, DomainResult, ServiceError, ServiceResult, ValidationError, DbError};
use crate::types::{PaginatedResult, PaginationParams};
use crate::validation::Validate;
use async_trait::async_trait;
use std::sync::Arc;
use uuid::Uuid;

// Added document/sync imports
use crate::domains::document::service::DocumentService;
use crate::domains::document::types::MediaDocumentResponse;
use crate::domains::sync::types::SyncPriority;
use crate::domains::compression::types::CompressionPriority;

/// Trait defining activity service operations
#[async_trait]
pub trait ActivityService: DeleteService<Activity> + Send + Sync {
    async fn create_activity(
        &self,
        new_activity: NewActivity,
        auth: &AuthContext,
    ) -> ServiceResult<ActivityResponse>;

    async fn get_activity_by_id(
        &self,
        id: Uuid,
        auth: &AuthContext,
    ) -> ServiceResult<ActivityResponse>;

    async fn list_activities_for_project(
        &self,
        project_id: Uuid,
        params: PaginationParams,
        auth: &AuthContext,
    ) -> ServiceResult<PaginatedResult<ActivityResponse>>;

    async fn update_activity(
        &self,
        id: Uuid,
        update_data: UpdateActivity,
        auth: &AuthContext,
    ) -> ServiceResult<ActivityResponse>;
    
    async fn delete_activity(
        &self,
        id: Uuid,
        hard_delete: bool,
        auth: &AuthContext,
    ) -> ServiceResult<DeleteResult>;

    // Add document upload methods
    async fn upload_document_for_activity(
        &self,
        activity_id: Uuid,
        file_data: Vec<u8>,
        original_filename: String,
        title: Option<String>,
        document_type_id: Uuid,
        linked_field: Option<String>,
        sync_priority: SyncPriority,
        compression_priority: Option<CompressionPriority>,
        auth: &AuthContext,
    ) -> ServiceResult<MediaDocumentResponse>;

    async fn bulk_upload_documents_for_activity(
        &self,
        activity_id: Uuid,
        files: Vec<(Vec<u8>, String)>,
        title: Option<String>,
        document_type_id: Uuid,
        sync_priority: SyncPriority,
        compression_priority: Option<CompressionPriority>,
        auth: &AuthContext,
    ) -> ServiceResult<Vec<MediaDocumentResponse>>;
    
    async fn create_activity_with_documents(
        &self,
        new_activity: NewActivity,
        documents: Vec<(Vec<u8>, String, Option<String>)>,
        document_type_id: Uuid,
        auth: &AuthContext,
    ) -> ServiceResult<(ActivityResponse, Vec<Result<MediaDocumentResponse, ServiceError>>)>;
}

/// Implementation of the activity service
#[derive(Clone)] 
pub struct ActivityServiceImpl {
    pool: SqlitePool,
    repo: Arc<dyn ActivityRepository + Send + Sync>,
    project_repo: Arc<dyn ProjectRepository + Send + Sync>,
    delete_service: Arc<BaseDeleteService<Activity>>,
    document_service: Arc<dyn DocumentService>,
}

impl ActivityServiceImpl {
    pub fn new(
        pool: SqlitePool,
        activity_repo: Arc<dyn ActivityRepository + Send + Sync>,
        project_repo: Arc<dyn ProjectRepository + Send + Sync>,
        tombstone_repo: Arc<dyn TombstoneRepository + Send + Sync>,
        change_log_repo: Arc<dyn ChangeLogRepository + Send + Sync>,
        dependency_checker: Arc<dyn DependencyChecker + Send + Sync>,
        document_service: Arc<dyn DocumentService>,
    ) -> Self {
        // Local adapter struct
        struct RepoAdapter(Arc<dyn ActivityRepository + Send + Sync>);

        #[async_trait]
        impl FindById<Activity> for RepoAdapter {
            async fn find_by_id(&self, id: Uuid) -> DomainResult<Activity> {
                self.0.find_by_id(id).await
            }
        }

        #[async_trait]
        impl SoftDeletable for RepoAdapter {
             async fn soft_delete(&self, id: Uuid, auth: &AuthContext) -> DomainResult<()> {
                 self.0.soft_delete(id, auth).await
             }
             async fn soft_delete_with_tx(
                 &self,
                 id: Uuid,
                 auth: &AuthContext,
                 tx: &mut Transaction<'_, Sqlite>,
             ) -> DomainResult<()> {
                 self.0.soft_delete_with_tx(id, auth, tx).await
             }
        }

        #[async_trait]
        impl HardDeletable for RepoAdapter {
             fn entity_name(&self) -> &'static str {
                 self.0.entity_name()
             }
             async fn hard_delete(&self, id: Uuid, auth: &AuthContext) -> DomainResult<()> {
                 self.0.hard_delete(id, auth).await
             }
             async fn hard_delete_with_tx(
                 &self,
                 id: Uuid,
                 auth: &AuthContext,
                 tx: &mut Transaction<'_, Sqlite>,
             ) -> DomainResult<()> {
                 self.0.hard_delete_with_tx(id, auth, tx).await
             }
        }
        
        // Blanket impl covers DeleteServiceRepository<Activity>

        let adapted_repo: Arc<dyn DeleteServiceRepository<Activity>> = 
            Arc::new(RepoAdapter(activity_repo.clone()));

        let delete_service = Arc::new(BaseDeleteService::new(
            pool.clone(),
            adapted_repo,
            tombstone_repo,
            change_log_repo,
            dependency_checker,
            None,
        ));
        
        Self {
            pool,
            repo: activity_repo,
            project_repo,
            delete_service,
            document_service,
        }
    }

    // Updated validation helper name for clarity
    async fn validate_project_exists_if_provided(&self, project_id: Option<Uuid>) -> DomainResult<()> {
        if let Some(id) = project_id {
             // If a project_id IS provided, it MUST exist
             match self.project_repo.find_by_id(id).await {
                 Ok(_) => Ok(()),
                 Err(DomainError::EntityNotFound(_, _)) => Err(DomainError::Validation(
                     ValidationError::relationship(&format!("Project with ID {} does not exist", id))
                 )),
                 Err(e) => Err(e), 
             }
         } else {
            // If no project_id is provided, it's valid (activity is independent)
             Ok(())
         }
    }

    // Added enrich_response helper
    async fn enrich_response(
        &self,
        mut response: ActivityResponse,
        include: Option<&[ActivityInclude]>,
        auth: &AuthContext,
    ) -> ServiceResult<ActivityResponse> {
        if let Some(includes) = include {
            let include_docs = includes.contains(&ActivityInclude::Documents);

            if include_docs {
                let doc_params = PaginationParams::default();
                let docs_result = self.document_service
                    .list_media_documents_by_related_entity(
                        auth,
                        "activities",
                        response.id,
                        doc_params,
                        None,
                    ).await?;
                response.documents = Some(docs_result.items);
            }
            // TODO: Add enrichment for Project if needed
        }
        Ok(response)
    }
    
    // Added upload_documents_for_entity helper
    async fn upload_documents_for_entity(
        &self,
        entity_id: Uuid,
        entity_type: &str,
        documents: Vec<(Vec<u8>, String, Option<String>)>,
        document_type_id: Uuid,
        sync_priority: SyncPriority,
        compression_priority: Option<CompressionPriority>,
        auth: &AuthContext,
    ) -> Vec<Result<MediaDocumentResponse, ServiceError>> {
        let mut results = Vec::new();
        for (file_data, filename, linked_field) in documents {
            let upload_result = self.document_service.upload_document(
                auth,
                file_data,
                filename,
                None,
                document_type_id,
                entity_id,
                entity_type.to_string(),
                linked_field,
                sync_priority,
                compression_priority,
                None,
            ).await;
            results.push(upload_result);
        }
        results
    }
}

// Implement DeleteService<Activity> by delegating
#[async_trait]
impl DeleteService<Activity> for ActivityServiceImpl {
    // ... (Delegation methods exactly like in ProjectServiceImpl) ...
    fn repository(&self) -> &dyn FindById<Activity> {
        self.delete_service.repository()
    }
    fn tombstone_repository(&self) -> &dyn TombstoneRepository {
        self.delete_service.tombstone_repository()
    }
    fn change_log_repository(&self) -> &dyn ChangeLogRepository {
        self.delete_service.change_log_repository()
    }
    fn dependency_checker(&self) -> &dyn DependencyChecker {
        self.delete_service.dependency_checker()
    }
    async fn delete(
        &self,
        id: Uuid,
        auth: &AuthContext,
        options: DeleteOptions,
    ) -> DomainResult<DeleteResult> {
        self.delete_service.delete(id, auth, options).await
    }
    async fn batch_delete(
        &self,
        ids: &[Uuid],
        auth: &AuthContext,
        options: DeleteOptions,
    ) -> DomainResult<crate::domains::core::delete_service::BatchDeleteResult> {
        self.delete_service.batch_delete(ids, auth, options).await
    }
    async fn delete_with_dependencies(
        &self,
        id: Uuid,
        auth: &AuthContext,
    ) -> DomainResult<DeleteResult> {
        self.delete_service.delete_with_dependencies(id, auth).await
    }
    async fn get_failed_delete_details(
        &self,
        batch_result: &crate::domains::core::delete_service::BatchDeleteResult,
        auth: &AuthContext,
    ) -> DomainResult<Vec<crate::domains::core::delete_service::FailedDeleteDetail<Activity>>> {
         self.delete_service.get_failed_delete_details(batch_result, auth).await
    }
}

#[async_trait]
impl ActivityService for ActivityServiceImpl {
    async fn create_activity(
        &self,
        new_activity: NewActivity,
        auth: &AuthContext,
    ) -> ServiceResult<ActivityResponse> {
        if !auth.has_permission(Permission::CreateActivities) {
            return Err(ServiceError::PermissionDenied(
                "User does not have permission to create activities".to_string(),
            ));
        }

        new_activity.validate()?;
        // Validate project exists ONLY if an ID was provided
        self.validate_project_exists_if_provided(new_activity.project_id).await?;

        let created_activity = self.repo.create(&new_activity, auth).await?;
        Ok(ActivityResponse::from(created_activity))
    }

    async fn get_activity_by_id(
        &self,
        id: Uuid,
        auth: &AuthContext,
    ) -> ServiceResult<ActivityResponse> {
        if !auth.has_permission(Permission::ViewActivities) {
            return Err(ServiceError::PermissionDenied(
                "User does not have permission to view activities".to_string(),
            ));
        }

        let activity = self.repo.find_by_id(id).await?;
        // Optional: Add check if user can access the activity's project (if it has one)
        // if let Some(project_id) = activity.project_id {
        //     self.validate_project_access(project_id, auth).await?;
        // }

        Ok(ActivityResponse::from(activity))
    }

    async fn list_activities_for_project(
        &self,
        project_id: Uuid, // Keep this required, as we are listing FOR a project
        params: PaginationParams,
        auth: &AuthContext,
    ) -> ServiceResult<PaginatedResult<ActivityResponse>> {
        if !auth.has_permission(Permission::ViewActivities) {
            return Err(ServiceError::PermissionDenied(
                "User does not have permission to view activities".to_string(),
            ));
        }
        
        // Validate the project exists before listing its activities
        self.validate_project_exists_if_provided(Some(project_id)).await?; 

        let paginated_result = self.repo.find_by_project_id(project_id, params).await?;

        let response_items = paginated_result
            .items
            .into_iter()
            .map(ActivityResponse::from)
            .collect();

        Ok(PaginatedResult::new(
            response_items,
            paginated_result.total,
            params,
        ))
    }

    async fn update_activity(
        &self,
        id: Uuid,
        mut update_data: UpdateActivity,
        auth: &AuthContext,
    ) -> ServiceResult<ActivityResponse> {
        if !auth.has_permission(Permission::EditActivities) {
            return Err(ServiceError::PermissionDenied(
                "User does not have permission to edit activities".to_string(),
            ));
        }

        update_data.updated_by_user_id = auth.user_id;
        update_data.validate()?;
        
        // Fetch the current activity first to check its project (if any) for access control
        let current_activity = self.repo.find_by_id(id).await?;
        // Optional: Check access to current_activity.project_id
        // if let Some(p_id) = current_activity.project_id { ... }
        
        // Validate the *new* project_id exists if it's being set
        if let Some(opt_p_id) = update_data.project_id { // Check if project_id is part of the update
             self.validate_project_exists_if_provided(opt_p_id).await?;
        }

        let updated_activity = self.repo.update(id, &update_data, auth).await?;
        Ok(ActivityResponse::from(updated_activity))
    }
    
    async fn delete_activity(
        &self,
        id: Uuid,
        hard_delete: bool,
        auth: &AuthContext,
    ) -> ServiceResult<DeleteResult> {
        let required_permission = if hard_delete {
            Permission::HardDeleteRecord
        } else {
            Permission::DeleteRecord
        };

        if !auth.has_permission(required_permission) {
             return Err(ServiceError::PermissionDenied(format!(
                 "User does not have permission to {} activities",
                 if hard_delete { "hard delete" } else { "delete" }
             )));
        }
        
        // Fetch activity first to check existence and potentially project access
        let _activity = self.repo.find_by_id(id).await?; 
        // Optional: Check access to _activity.project_id

        let options = DeleteOptions {
            allow_hard_delete: hard_delete,
            fallback_to_soft_delete: !hard_delete, 
            force: false, 
        };
        
        let result = self.delete(id, auth, options).await?;
        Ok(result)
    }

    // Implement document upload methods
    async fn upload_document_for_activity(
        &self,
        activity_id: Uuid,
        file_data: Vec<u8>,
        original_filename: String,
        title: Option<String>,
        document_type_id: Uuid,
        linked_field: Option<String>,
        sync_priority: SyncPriority,
        compression_priority: Option<CompressionPriority>,
        auth: &AuthContext,
    ) -> ServiceResult<MediaDocumentResponse> {
        // 1. Verify activity exists
        let _activity = self.repo.find_by_id(activity_id).await
            .map_err(ServiceError::Domain)?;

        // 2. Check permissions
        if !auth.has_permission(Permission::UploadDocuments) {
            return Err(ServiceError::PermissionDenied(
                "User does not have permission to upload documents".to_string(),
            ));
        }

        // 3. Validate the linked field if specified
        if let Some(field) = &linked_field {
            if !Activity::is_document_linkable_field(field) { // Use Activity::...
                let valid_fields: Vec<String> = Activity::document_linkable_fields() // Use Activity::...
                    .into_iter()
                    .collect();
                    
                // Correctly wrap the ValidationError
                return Err(ServiceError::Domain(ValidationError::Custom(format!(
                    "Field '{}' does not support document attachments for activities. Valid fields: {}",
                    field, valid_fields.join(", ")
                )).into()));
            }
        }

        // 4. Delegate to document service
        let document = self.document_service.upload_document(
            auth,
            file_data,
            original_filename,
            title,
            document_type_id,
            activity_id,
            "activities".to_string(), // Correct entity type
            linked_field.clone(), // Pass validated field
            sync_priority,
            compression_priority,
            None, // No temp ID needed here
        ).await?;

        Ok(document)
    }

    async fn bulk_upload_documents_for_activity(
        &self,
        activity_id: Uuid,
        files: Vec<(Vec<u8>, String)>,
        title: Option<String>,
        document_type_id: Uuid,
        sync_priority: SyncPriority,
        compression_priority: Option<CompressionPriority>,
        auth: &AuthContext,
    ) -> ServiceResult<Vec<MediaDocumentResponse>> {
        // 1. Verify activity exists
        let _activity = self.repo.find_by_id(activity_id).await
            .map_err(ServiceError::Domain)?;

        // 2. Check permissions
        if !auth.has_permission(Permission::UploadDocuments) {
            return Err(ServiceError::PermissionDenied(
                "User does not have permission to upload documents".to_string(),
            ));
        }

        // 3. Delegate to document service
        let documents = self.document_service.bulk_upload_documents(
            auth,
            files,
            title,
            document_type_id,
            activity_id,
            "activities".to_string(), // Correct entity type
            sync_priority,
            compression_priority,
            None,
        ).await?;

        Ok(documents)
    }
    
    async fn create_activity_with_documents(
        &self,
        new_activity: NewActivity,
        documents: Vec<(Vec<u8>, String, Option<String>)>,
        document_type_id: Uuid,
        auth: &AuthContext,
    ) -> ServiceResult<(ActivityResponse, Vec<Result<MediaDocumentResponse, ServiceError>>) > {
        if !auth.has_permission(Permission::CreateActivities) {
            return Err(ServiceError::PermissionDenied("User cannot create activities".to_string()));
        }
        if !documents.is_empty() && !auth.has_permission(Permission::UploadDocuments) {
             return Err(ServiceError::PermissionDenied("User cannot upload documents".to_string()));
        }

        new_activity.validate()?;
        self.validate_project_exists_if_provided(new_activity.project_id).await?;

        let mut tx = self.pool.begin().await.map_err(|e| ServiceError::Domain(DomainError::Database(DbError::from(e))))?;
        let created_activity = match self.repo.create_with_tx(&new_activity, auth, &mut tx).await {
            Ok(a) => a,
            Err(e) => { let _ = tx.rollback().await; return Err(ServiceError::Domain(e)); }
        };
        tx.commit().await.map_err(|e| ServiceError::Domain(DomainError::Database(DbError::from(e))))?;

        let document_results = if !documents.is_empty() {
            self.upload_documents_for_entity(
                created_activity.id,
                "activities",
                documents,
                document_type_id,
                SyncPriority::Normal, // Use a default or derive from activity?
                None, 
                auth,
            ).await
        } else {
            Vec::new()
        };

        let response = ActivityResponse::from(created_activity);
        // Potentially enrich response here if needed after create + docs
        // let enriched_response = self.enrich_response(response, Some(&[ActivityInclude::Documents]), auth).await?;
        Ok((response, document_results))
    }
}


// /Users/sagarshrestha/ipad_rust_core copy/src/domains/compression/repository.rs
//! Repository for compression queue and stats

use async_trait::async_trait;
use chrono::Utc;
use sqlx::{Pool, Sqlite, Transaction};
use uuid::Uuid;


use crate::errors::{DbError, DomainError, DomainResult};
use super::types::{
    CompressionQueueEntry, CompressionQueueStatus, CompressionStats
};

#[async_trait]
pub trait CompressionRepository: Send + Sync {
    /// Queue a document for compression
    async fn queue_document(
        &self,
        document_id: Uuid,
        priority: i32,
    ) -> DomainResult<CompressionQueueEntry>;
    
    /// Get the next document for compression
    async fn get_next_document_for_compression(&self) -> DomainResult<Option<CompressionQueueEntry>>;
    
    /// Update the status of a queued document
    async fn update_queue_entry_status(
        &self,
        queue_id: Uuid,
        status: &str,
        error_message: Option<&str>,
    ) -> DomainResult<()>;
    
    /// Update the compression priority for a document
    async fn update_compression_priority(
        &self,
        document_id: Uuid,
        priority: i32,
    ) -> DomainResult<bool>;
    
    /// Get the status of the compression queue
    async fn get_queue_status(&self) -> DomainResult<CompressionQueueStatus>;
    
    /// Get compression statistics
    async fn get_compression_stats(&self) -> DomainResult<CompressionStats>;
    
    /// Update compression statistics after successful compression
    async fn update_stats_after_compression(
        &self,
        original_size: i64,
        compressed_size: i64,
        tx: &mut Transaction<'_, Sqlite>,
    ) -> DomainResult<()>;
    
    /// Update stats when a compression job is skipped
    async fn update_stats_for_skipped(
        &self,
        tx: &mut Transaction<'_, Sqlite>,
    ) -> DomainResult<()>;
    
    /// Update stats when a compression job fails
    async fn update_stats_for_failed(
        &self,
        tx: &mut Transaction<'_, Sqlite>,
    ) -> DomainResult<()>;
    
    /// Get a queue entry by document ID
    async fn get_queue_entry_by_document_id(
        &self,
        document_id: Uuid,
    ) -> DomainResult<Option<CompressionQueueEntry>>;
    
    /// Remove a document from the compression queue
    async fn remove_from_queue(
        &self,
        document_id: Uuid,
    ) -> DomainResult<bool>;
    
    /// Bulk update compression priorities
    async fn bulk_update_compression_priority(
        &self,
        document_ids: &[Uuid],
        priority: i32,
    ) -> DomainResult<u64>;
}

pub struct SqliteCompressionRepository {
    pool: Pool<Sqlite>,
}

impl SqliteCompressionRepository {
    pub fn new(pool: Pool<Sqlite>) -> Self {
        Self { pool }
    }
}

#[async_trait]
impl CompressionRepository for SqliteCompressionRepository {
    async fn queue_document(
        &self,
        document_id: Uuid,
        priority: i32,
    ) -> DomainResult<CompressionQueueEntry> {
        let mut tx = self.pool.begin().await.map_err(DbError::from)?;
        
        // Check if document is already in queue
        let document_id_str_check = document_id.to_string();
        let existing = sqlx::query!(
            "SELECT id FROM compression_queue WHERE document_id = ?",
            document_id_str_check
        )
        .fetch_optional(&mut *tx)
        .await
        .map_err(DbError::from)?;
        
        if let Some(row) = existing {
            let updated_at_str = Utc::now().to_rfc3339();
            let row_id_str = row.id.as_deref()
                .ok_or_else(|| DomainError::Internal("Queue entry ID missing in existing row".to_string()))?;
            // Update priority if already queued
            sqlx::query!(
                "UPDATE compression_queue SET 
                 priority = ?, 
                 updated_at = ? 
                 WHERE id = ?",
                priority,
                updated_at_str,
                row_id_str
            )
            .execute(&mut *tx)
            .await
            .map_err(DbError::from)?;
            
            let queue_id = Uuid::parse_str(row_id_str)
                .map_err(|_| DomainError::InvalidUuid(row_id_str.to_string()))?;
                
            let entry = self.get_queue_entry_internal(queue_id, &mut tx).await?;
            tx.commit().await.map_err(DbError::from)?;
            return Ok(entry);
        }
        
        // Add new queue entry
        let queue_id = Uuid::new_v4();
        let now_str = Utc::now().to_rfc3339();
        let queue_id_str = queue_id.to_string();
        let document_id_str = document_id.to_string();
        
        // Use sqlx::query (unchecked) to bypass persistent prepare analysis error
        sqlx::query(
            "INSERT INTO compression_queue
             (id, document_id, priority, attempts, status, created_at, updated_at)
             VALUES (?, ?, ?, 0, 'pending', ?, ?)"
        )
        .bind(queue_id_str)
        .bind(document_id_str)
        .bind(priority)
        .bind(&now_str)
        .bind(&now_str)
        .execute(&mut *tx)
        .await
        .map_err(DbError::from)?;
        
        // Update stats
        sqlx::query!(
            "UPDATE compression_stats 
             SET total_files_pending = total_files_pending + 1,
                 updated_at = ?
             WHERE id = 'global'",
            now_str
        )
        .execute(&mut *tx)
        .await
        .map_err(DbError::from)?;
        
        let entry = self.get_queue_entry_internal(queue_id, &mut tx).await?;
        tx.commit().await.map_err(DbError::from)?;
        
        Ok(entry)
    }
    
    async fn get_next_document_for_compression(&self) -> DomainResult<Option<CompressionQueueEntry>> {
        let mut tx = self.pool.begin().await.map_err(DbError::from)?;
        
        // First, check if document is available (not in use)
        let row = sqlx::query!(
            r#"
            SELECT cq.id, cq.document_id
            FROM compression_queue cq
            LEFT JOIN active_file_usage afu ON 
                cq.document_id = afu.document_id AND 
                afu.last_active_at > datetime('now', '-5 minutes')
            WHERE cq.status = 'pending'
            AND afu.document_id IS NULL -- Skip documents that are in use
            ORDER BY cq.priority DESC, cq.created_at ASC
            LIMIT 1
            "#
        )
        .fetch_optional(&mut *tx)
        .await
        .map_err(DbError::from)?;
        
        let entry = match row {
            Some(row) => {
                 let row_id_str = row.id.as_deref()
                     .ok_or_else(|| DomainError::Internal("Queue entry ID missing when fetching next".to_string()))?;
                 let queue_id = Uuid::parse_str(row_id_str)
                    .map_err(|_| DomainError::InvalidUuid(row_id_str.to_string()))?;
                
                let updated_at_str = Utc::now().to_rfc3339();
                // Mark as processing
                sqlx::query!(
                    "UPDATE compression_queue 
                     SET status = 'processing', 
                         attempts = attempts + 1,
                         updated_at = ?
                     WHERE id = ?",
                    updated_at_str,
                    row_id_str
                )
                .execute(&mut *tx)
                .await
                .map_err(DbError::from)?;
                
                // Also update document compression status
                let document_id_str = row.document_id;
                sqlx::query!(
                    r#"
                    UPDATE media_documents
                    SET 
                        compression_status = 'in_progress',
                        updated_at = ?
                    WHERE id = ?
                    "#,
                    updated_at_str,
                    document_id_str
                )
                .execute(&mut *tx)
                .await
                .map_err(DbError::from)?;
                
                self.get_queue_entry_internal(queue_id, &mut tx).await?
            }
            None => {
                tx.commit().await.map_err(DbError::from)?;
                return Ok(None);
            }
        };
        
        tx.commit().await.map_err(DbError::from)?;
        Ok(Some(entry))
    }
    
    async fn update_queue_entry_status(
        &self,
        queue_id: Uuid,
        status: &str,
        error_message: Option<&str>,
    ) -> DomainResult<()> {
        let updated_at_str = Utc::now().to_rfc3339();
        let queue_id_str = queue_id.to_string();
        sqlx::query!(
            "UPDATE compression_queue 
             SET status = ?, 
                 error_message = ?,
                 updated_at = ?
             WHERE id = ?",
            status,
            error_message,
            updated_at_str,
            queue_id_str
        )
        .execute(&self.pool)
        .await
        .map_err(DbError::from)?;
        
        Ok(())
    }
    
    async fn update_compression_priority(
        &self,
        document_id: Uuid,
        priority: i32,
    ) -> DomainResult<bool> {
        let updated_at_str = Utc::now().to_rfc3339();
        let document_id_str = document_id.to_string();
        let result = sqlx::query!(
            "UPDATE compression_queue 
             SET priority = ?, 
                 updated_at = ?
             WHERE document_id = ? AND status = 'pending'",
            priority,
            updated_at_str,
            document_id_str
        )
        .execute(&self.pool)
        .await
        .map_err(DbError::from)?;
        
        Ok(result.rows_affected() > 0)
    }
    
    async fn get_queue_status(&self) -> DomainResult<CompressionQueueStatus> {
        let row = sqlx::query!(
            "SELECT 
                SUM(CASE WHEN status = 'pending' THEN 1 ELSE 0 END) as pending_count,
                SUM(CASE WHEN status = 'processing' THEN 1 ELSE 0 END) as processing_count,
                SUM(CASE WHEN status = 'completed' THEN 1 ELSE 0 END) as completed_count,
                SUM(CASE WHEN status = 'failed' THEN 1 ELSE 0 END) as failed_count,
                SUM(CASE WHEN status = 'skipped' THEN 1 ELSE 0 END) as skipped_count
             FROM compression_queue"
        )
        .fetch_one(&self.pool)
        .await
        .map_err(DbError::from)?;
        
        Ok(CompressionQueueStatus {
            pending_count: row.pending_count.unwrap_or(0),
            processing_count: row.processing_count.unwrap_or(0),
            completed_count: row.completed_count.unwrap_or(0),
            failed_count: row.failed_count.unwrap_or(0),
            skipped_count: row.skipped_count.unwrap_or(0),
        })
    }
    
    async fn get_compression_stats(&self) -> DomainResult<CompressionStats> {
        let row = sqlx::query!(
            "SELECT 
                total_original_size, total_compressed_size, space_saved,
                compression_ratio, total_files_compressed, total_files_pending,
                total_files_failed, total_files_skipped, last_compression_date, updated_at
             FROM compression_stats
             WHERE id = 'global'"
        )
        .fetch_one(&self.pool)
        .await
        .map_err(DbError::from)?;
        
        let last_compression_date = match &row.last_compression_date {
            Some(date_str) => Some(
                chrono::DateTime::parse_from_rfc3339(date_str)
                    .map_err(|_| DomainError::Internal(format!("Invalid date format: {}", date_str)))?
                    .with_timezone(&Utc)
            ),
            None => None,
        };
        
        let updated_at = chrono::DateTime::parse_from_rfc3339(&row.updated_at)
            .map_err(|_| DomainError::Internal(format!("Invalid date format: {}", row.updated_at)))?
            .with_timezone(&Utc);
        
        Ok(CompressionStats {
            total_original_size: row.total_original_size.unwrap_or(0),
            total_compressed_size: row.total_compressed_size.unwrap_or(0),
            space_saved: row.space_saved.unwrap_or(0),
            compression_ratio: row.compression_ratio.unwrap_or(0.0),
            total_files_compressed: row.total_files_compressed.unwrap_or(0),
            total_files_pending: row.total_files_pending.unwrap_or(0),
            total_files_failed: row.total_files_failed.unwrap_or(0),
            total_files_skipped: row.total_files_skipped,
            last_compression_date,
            updated_at,
        })
    }
    
    async fn update_stats_after_compression(
        &self,
        original_size: i64,
        compressed_size: i64,
        tx: &mut Transaction<'_, Sqlite>,
    ) -> DomainResult<()> {
        let space_saved = original_size - compressed_size;
        let now_str = Utc::now().to_rfc3339();
        
        sqlx::query!(
            "UPDATE compression_stats SET
                total_original_size = total_original_size + ?,
                total_compressed_size = total_compressed_size + ?,
                space_saved = space_saved + ?,
                compression_ratio = CASE 
                    WHEN total_original_size + ? > 0 THEN 
                        ((space_saved + ?) * 100.0) / (total_original_size + ?)
                    ELSE 0 END,
                total_files_compressed = total_files_compressed + 1,
                total_files_pending = total_files_pending - 1,
                last_compression_date = ?,
                updated_at = ?
            WHERE id = 'global'",
            original_size,
            compressed_size,
            space_saved,
            original_size,
            space_saved,
            original_size,
            now_str,
            now_str
        )
        .execute(&mut **tx)
        .await
        .map_err(DbError::from)?;
        
        Ok(())
    }
    
    async fn update_stats_for_skipped(
        &self,
        tx: &mut Transaction<'_, Sqlite>,
    ) -> DomainResult<()> {
        let now_str = Utc::now().to_rfc3339();
        sqlx::query!(
            "UPDATE compression_stats SET
                total_files_skipped = total_files_skipped + 1,
                total_files_pending = total_files_pending - 1,
                updated_at = ?
            WHERE id = 'global'",
            now_str
        )
        .execute(&mut **tx)
        .await
        .map_err(DbError::from)?;
        
        Ok(())
    }
    
    async fn update_stats_for_failed(
        &self,
        tx: &mut Transaction<'_, Sqlite>,
    ) -> DomainResult<()> {
        let now_str = Utc::now().to_rfc3339();
        sqlx::query!(
            "UPDATE compression_stats SET
                total_files_failed = total_files_failed + 1,
                total_files_pending = total_files_pending - 1,
                updated_at = ?
            WHERE id = 'global'",
            now_str
        )
        .execute(&mut **tx)
        .await
        .map_err(DbError::from)?;
        
        Ok(())
    }
    
    async fn get_queue_entry_by_document_id(
        &self,
        document_id: Uuid,
    ) -> DomainResult<Option<CompressionQueueEntry>> {
        let document_id_str = document_id.to_string();
        let row = sqlx::query!(
            "SELECT id, document_id, priority, attempts, status, created_at, updated_at, error_message
             FROM compression_queue
             WHERE document_id = ?",
            document_id_str
        )
        .fetch_optional(&self.pool)
        .await
        .map_err(DbError::from)?;
        
        match row {
            Some(row) => {
                let id_str = row.id.as_deref()
                    .ok_or_else(|| DomainError::Internal(format!("Queue entry ID missing for doc {}", document_id)))?;
                let queue_id = Uuid::parse_str(id_str)
                    .map_err(|_| DomainError::InvalidUuid(id_str.to_string()))?;
                    
                let document_id_uuid = Uuid::parse_str(&row.document_id)
                    .map_err(|_| DomainError::InvalidUuid(row.document_id.clone()))?;
                
                let created_at = chrono::DateTime::parse_from_rfc3339(&row.created_at)
                    .map_err(|_| DomainError::Internal(format!("Invalid date format: {}", row.created_at)))?
                    .with_timezone(&Utc);
                
                let updated_at = chrono::DateTime::parse_from_rfc3339(&row.updated_at)
                    .map_err(|_| DomainError::Internal(format!("Invalid date format: {}", row.updated_at)))?
                    .with_timezone(&Utc);
                
                Ok(Some(CompressionQueueEntry {
                    id: queue_id,
                    document_id: document_id_uuid,
                    priority: row.priority.unwrap_or(0) as i32,
                    attempts: row.attempts.unwrap_or(0) as i32,
                    status: row.status.clone(),
                    created_at,
                    updated_at,
                    error_message: row.error_message,
                }))
            },
            None => Ok(None),
        }
    }
    
    async fn remove_from_queue(
        &self,
        document_id: Uuid,
    ) -> DomainResult<bool> {
        let document_id_str = document_id.to_string();
        // Use sqlx::query (unchecked) to bypass persistent prepare analysis error
        let result = sqlx::query(
            "DELETE FROM compression_queue WHERE document_id = ?"
        )
        .bind(document_id_str)
        .execute(&self.pool)
        .await
        .map_err(DbError::from)?;
        
        Ok(result.rows_affected() > 0)
    }
    
    async fn bulk_update_compression_priority(
        &self,
        document_ids: &[Uuid],
        priority: i32,
    ) -> DomainResult<u64> {
        if document_ids.is_empty() {
            return Ok(0);
        }
        
        let mut affected = 0;
        let now = Utc::now().to_rfc3339();
        
        // Process in batches of 100
        for chunk in document_ids.chunks(100) {
            let placeholders = chunk.iter().map(|_| "?").collect::<Vec<_>>().join(",");
            let query_str = format!(
                "UPDATE compression_queue 
                 SET priority = ?, updated_at = ? 
                 WHERE document_id IN ({}) AND status = 'pending'",
                placeholders
            );
            
            let mut query = sqlx::query(&query_str);
            query = query.bind(priority);
            query = query.bind(&now);
            
            for id in chunk {
                query = query.bind(id.to_string());
            }
            
            let result = query.execute(&self.pool).await.map_err(DbError::from)?;
            affected += result.rows_affected();
        }
        
        Ok(affected)
    }
}

// Internal helper methods
impl SqliteCompressionRepository {
    async fn get_queue_entry_internal(
        &self,
        queue_id: Uuid,
        tx: &mut Transaction<'_, Sqlite>,
    ) -> DomainResult<CompressionQueueEntry> {
        let queue_id_str = queue_id.to_string();
        let row = sqlx::query!(
            "SELECT id, document_id, priority, attempts, status, created_at, updated_at, error_message
             FROM compression_queue
             WHERE id = ?",
            queue_id_str
        )
        .fetch_one(&mut **tx)
        .await
        .map_err(DbError::from)?;
        
        let document_id = Uuid::parse_str(&row.document_id)
            .map_err(|_| DomainError::InvalidUuid(row.document_id.clone()))?;
        
        let created_at = chrono::DateTime::parse_from_rfc3339(&row.created_at)
            .map_err(|_| DomainError::Internal(format!("Invalid date format: {}", row.created_at)))?
            .with_timezone(&Utc);
        
        let updated_at = chrono::DateTime::parse_from_rfc3339(&row.updated_at)
            .map_err(|_| DomainError::Internal(format!("Invalid date format: {}", row.updated_at)))?
            .with_timezone(&Utc);
        
        Ok(CompressionQueueEntry {
            id: queue_id,
            document_id,
            priority: row.priority.unwrap_or(0) as i32,
            attempts: row.attempts.unwrap_or(0) as i32,
            status: row.status.clone(),
            created_at,
            updated_at,
            error_message: row.error_message,
        })
    }
}

// /Users/sagarshrestha/ipad_rust_core copy/src/domains/core/delete_service.rs
use crate::errors::{DomainResult, DomainError, DbError};
use crate::auth::AuthContext;
use crate::types::UserRole;
use crate::domains::sync::types::{Tombstone, ChangeLogEntry, ChangeOperationType};
use crate::domains::sync::repository::{TombstoneRepository, ChangeLogRepository};
use crate::domains::core::dependency_checker::DependencyChecker;
use crate::domains::core::repository::{DeleteResult, HardDeletable, SoftDeletable, FindById};
use crate::domains::document::repository::MediaDocumentRepository;
use uuid::Uuid;
use chrono::Utc;
use async_trait::async_trait;
use sqlx::{SqlitePool, Transaction, Sqlite};
use std::sync::Arc;
use std::collections::HashMap;
use serde::{Serialize, Deserialize};
use serde_json; // Added for JSON manipulation
use sqlx::Row;

/// Delete options for controlling deletion behavior
#[derive(Debug, Clone)]
pub struct DeleteOptions {
    /// Whether to allow hard delete
    pub allow_hard_delete: bool,
    
    /// Whether to fall back to soft delete if hard delete fails
    pub fallback_to_soft_delete: bool,
    
    /// Whether to bypass dependency checks (admin only)
    pub force: bool,
}

impl Default for DeleteOptions {
    fn default() -> Self {
        Self {
            allow_hard_delete: false,
            fallback_to_soft_delete: true,
            force: false,
        }
    }
}

/// Details about a single record that failed to delete
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct FailedDeleteDetail<E>
where 
    E: Send + Sync,
{
    pub id: Uuid,
    pub entity_data: Option<E>,
    pub entity_type: String,
    pub reason: FailureReason,
    pub dependencies: Vec<String>,
}

/// Reason why a delete operation failed for a specific record
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum FailureReason {
    /// Failed due to existing non-cascading dependencies
    DependenciesPrevented,
    /// Could be soft-deleted, but hard delete was prevented by dependencies
    SoftDeletedDueToDependencies, 
    /// Record was not found during the operation
    NotFound,
    /// User did not have permission for the requested operation (e.g., hard delete)
    AuthorizationFailed, 
    /// An unexpected database error occurred
    DatabaseError(String),
    /// Failure reason could not be determined from batch results
    Unknown, 
}

/// Result of a batch delete operation
#[derive(Debug, Clone, Default)]
pub struct BatchDeleteResult {
    /// Successfully hard deleted record IDs
    pub hard_deleted: Vec<Uuid>,
    /// Successfully soft deleted record IDs (includes those soft-deleted due to fallback)
    pub soft_deleted: Vec<Uuid>,
    /// Failed to delete record IDs (includes dependencies prevented, not found, errors)
    pub failed: Vec<Uuid>,
    /// Map of ID to dependencies that *would have* prevented hard delete
    /// (Populated for both SoftDeletedDueToDependencies and DependenciesPrevented)
    pub dependencies: HashMap<Uuid, Vec<String>>,
    /// Map of ID to specific failure errors (if captured)
    /// Optional: Requires enhancing `batch_delete` to store errors
    pub errors: HashMap<Uuid, DomainError>, 
}

/// Trait combining repository operations needed for delete service
pub trait DeleteServiceRepository<E>: FindById<E> + SoftDeletable + HardDeletable + Send + Sync {
    // Add a method to explicitly get self as a FindById reference
    fn as_find_by_id(&self) -> &dyn FindById<E>;
}

/// Implement for any type that implements all required traits
impl<T, E> DeleteServiceRepository<E> for T 
where 
    T: FindById<E> + SoftDeletable + HardDeletable + Send + Sync,
    E: Send + Sync + 'static,
{
    fn as_find_by_id(&self) -> &dyn FindById<E> {
        self
    }
}

/// Delete service for handling delete operations
#[async_trait]
pub trait DeleteService<E>: Send + Sync 
where
    E: Send + Sync + 'static,
{
    /// Get the repository
    fn repository(&self) -> &dyn FindById<E>;
    
    /// Get the tombstone repository
    fn tombstone_repository(&self) -> &dyn TombstoneRepository;
    
    /// Get the change log repository
    fn change_log_repository(&self) -> &dyn ChangeLogRepository;
    
    /// Get the dependency checker
    fn dependency_checker(&self) -> &dyn DependencyChecker;

    /// Delete an entity with specified options
    async fn delete(
        &self,
        id: Uuid,
        auth: &AuthContext,
        options: DeleteOptions,
    ) -> DomainResult<DeleteResult>;
    
    /// Delete multiple entities with specified options
    async fn batch_delete(
        &self,
        ids: &[Uuid],
        auth: &AuthContext,
        options: DeleteOptions,
    ) -> DomainResult<BatchDeleteResult>;
    
    /// Delete multiple entities with their dependencies
    async fn delete_with_dependencies(
        &self,
        id: Uuid,
        auth: &AuthContext,
    ) -> DomainResult<DeleteResult>;

    /// Retrieve details about records that failed to delete during a batch operation
    async fn get_failed_delete_details(
        &self,
        batch_result: &BatchDeleteResult,
        auth: &AuthContext,
    ) -> DomainResult<Vec<FailedDeleteDetail<E>>>;
}

/// Base implementation of delete service
pub struct BaseDeleteService<E>
where
    E: Send + Sync + 'static,
{
    pool: SqlitePool,
    repo: Arc<dyn DeleteServiceRepository<E>>,
    tombstone_repo: Arc<dyn TombstoneRepository + Send + Sync>,
    change_log_repo: Arc<dyn ChangeLogRepository + Send + Sync>,
    dependency_checker: Arc<dyn DependencyChecker + Send + Sync>,
    media_doc_repo: Option<Arc<dyn MediaDocumentRepository>>,
    _marker: std::marker::PhantomData<E>,
}

impl<E> BaseDeleteService<E>
where
    E: Send + Sync + Clone + 'static,
{
    /// Create a new base delete service
    pub fn new(
        pool: SqlitePool,
        repo: Arc<dyn DeleteServiceRepository<E>>,
        tombstone_repo: Arc<dyn TombstoneRepository + Send + Sync>,
        change_log_repo: Arc<dyn ChangeLogRepository + Send + Sync>,
        dependency_checker: Arc<dyn DependencyChecker + Send + Sync>,
        media_doc_repo: Option<Arc<dyn MediaDocumentRepository>>,
    ) -> Self {
        Self {
            pool,
            repo,
            tombstone_repo,
            change_log_repo,
            dependency_checker,
            media_doc_repo,
            _marker: std::marker::PhantomData,
        }
    }

    /// Enhanced helper function to handle document deletion with physical file cleanup
    async fn cascade_delete_documents<'t, T>(
        &self,
        parent_table_name: &str,
        parent_id: Uuid,
        hard_delete: bool, // Indicates if the parent was hard deleted
        auth: &AuthContext,
        tx: &mut Transaction<'t, Sqlite>,
    ) -> DomainResult<()> 
    where 
        T: Send + Sync + Clone + 'static // Changed generic E to T to avoid conflict
    {
        if let Some(media_repo) = &self.media_doc_repo {
            // Fetch just the document IDs and paths for the related entity
            // using a direct query to avoid borrowing issues with the transaction
            let parent_id_str = parent_id.to_string(); // Store the string in a variable
            let document_data_result = sqlx::query!(
                r#"
                SELECT 
                    id, 
                    file_path, 
                    compressed_file_path
                FROM 
                    media_documents 
                WHERE 
                    related_table = ? AND 
                    related_id = ? AND 
                    deleted_at IS NULL
                "#,
                parent_table_name,
                parent_id_str // Use the variable here
            )
             .fetch_all(&mut **tx) // Use the transaction
             .await;
             
            let document_data = match document_data_result {
                Ok(data) => data,
                 Err(sqlx::Error::RowNotFound) => Vec::new(), // No documents found is ok
                 Err(e) => return Err(DbError::from(e).into()), // Propagate other DB errors
             };

            // Process each document
            for doc in document_data {
                // Parse the ID string to UUID
                let doc_id = match Uuid::parse_str(&doc.id) {
                    Ok(id) => id,
                    Err(_) => continue, // Skip if invalid UUID (shouldn't happen)
                };
    
                if hard_delete {
                    // --- HARD DELETE PATH ---
                    
                    // 1. Create tombstone for the document
                    let mut tombstone = Tombstone::new(doc_id, media_repo.entity_name(), auth.user_id);
                    
                    // 2. Add additional metadata for sync/cleanup tracking
                    let metadata = serde_json::json!({
                        "file_path": doc.file_path,
                        "compressed_file_path": doc.compressed_file_path,
                        "parent_table": parent_table_name,
                        "parent_id": parent_id.to_string(),
                        "deletion_type": "cascade",
                        "timestamp": Utc::now().to_rfc3339()
                    });
                    
                    tombstone.additional_metadata = Some(metadata.to_string());
                    let operation_id = tombstone.operation_id; // Capture for consistency
                    
                    // 3. Create the tombstone record
                    self.tombstone_repo.create_tombstone_with_tx(&tombstone, tx).await?;

                    // 4. Create change log entry for the document's hard delete
                     let change_log = ChangeLogEntry {
                        operation_id, // Use tombstone's ID
                        entity_table: media_repo.entity_name().to_string(),
                        entity_id: doc_id,
                        operation_type: ChangeOperationType::HardDelete,
                        field_name: None, 
                        old_value: None, 
                        new_value: None,
                        document_metadata: Some(metadata.to_string()), // Add the same metadata
                        timestamp: Utc::now(),
                        user_id: auth.user_id,
                        device_id: auth.device_id.parse().ok(),
                        sync_batch_id: None, 
                        processed_at: None, 
                        sync_error: None,
                    };
                    
                    self.change_log_repo.create_change_log_with_tx(&change_log, tx).await?;

                    // 5. Queue file for deletion
                    let queue_id_str = Uuid::new_v4().to_string();
                    let doc_id_str = doc_id.to_string();
                    let now_str = Utc::now().to_rfc3339();
                    let user_id_str = auth.user_id.to_string();
                    sqlx::query!(
                        r#"
                        INSERT INTO file_deletion_queue (
                            id, 
                            document_id,
                            file_path, 
                            compressed_file_path, 
                            requested_at, 
                            requested_by, 
                            grace_period_seconds
                        )
                        VALUES (?, ?, ?, ?, ?, ?, ?)
                        "#,
                        queue_id_str, // Use variable
                        doc_id_str,   // Use variable
                        doc.file_path,
                        doc.compressed_file_path,
                        now_str,      // Use variable
                        user_id_str,  // Use variable
                        86400 // 24 hour grace period
                    )
                    .execute(&mut **tx)
                    .await
                    .map_err(DbError::from)?; // Propagate error instead of just logging

                    // 6. Perform the actual hard delete of the document record
                    media_repo.hard_delete_with_tx(doc_id, auth, tx).await?;
                    
                    // DB ON DELETE CASCADE will handle document versions and access logs automatically

                } else {
                    // --- SOFT DELETE PATH ---
                    
                    // Only soft delete the document record - don't queue for file deletion yet
                    // since the document should still exist but be marked as deleted
                    media_repo.soft_delete_with_tx(doc_id, auth, tx).await?;
                    
                    // Log access for the soft delete
                    let log_id_str = Uuid::new_v4().to_string();
                    let log_doc_id_str = doc_id.to_string();
                    let log_user_id_str = auth.user_id.to_string();
                    let log_timestamp_str = Utc::now().to_rfc3339();
                    let log_details_str = format!("Cascade soft delete from parent: {}/{}", parent_table_name, parent_id);
                    sqlx::query!(
                        r#"
                        INSERT INTO document_access_logs (
                            id,
                            document_id,
                            user_id,
                            access_type,
                            access_date,
                            details
                        )
                        VALUES (?, ?, ?, ?, ?, ?)
                        "#,
                        log_id_str,      // Use variable
                        log_doc_id_str,  // Use variable
                        log_user_id_str, // Use variable
                        "delete", // soft delete
                        log_timestamp_str, // Use variable
                        log_details_str    // Use variable
                    )
                    .execute(&mut **tx)
                    .await
                    .map_err(DbError::from)?; // Propagate error instead of just logging
                }
            }
        }
        
        Ok(())
    }
}

#[async_trait]
impl<E> DeleteService<E> for BaseDeleteService<E>
where
    E: Send + Sync + Clone + 'static,
{
    fn repository(&self) -> &dyn FindById<E> {
        self.repo.as_find_by_id()
    }
    
    fn tombstone_repository(&self) -> &dyn TombstoneRepository {
        &*self.tombstone_repo
    }
    
    fn change_log_repository(&self) -> &dyn ChangeLogRepository {
        &*self.change_log_repo
    }
    
    fn dependency_checker(&self) -> &dyn DependencyChecker {
        &*self.dependency_checker
    }
    
    async fn delete(
        &self,
        id: Uuid,
        auth: &AuthContext,
        options: DeleteOptions,
    ) -> DomainResult<DeleteResult> {
        // 1. Permission Checks
        if options.allow_hard_delete && auth.role != UserRole::Admin {
            return Err(DomainError::AuthorizationFailed("Only admins can perform hard deletes".to_string()));
        }
        if options.force && auth.role != UserRole::Admin {
            return Err(DomainError::AuthorizationFailed("Only admins can force delete operations".to_string()));
        }

        // 2. Dependency Check
        let table_name = self.repo.entity_name();
        let all_dependencies = self.dependency_checker.check_dependencies(table_name, id).await?;
        
        let blocking_dependencies: Vec<String> = all_dependencies
            .iter()
            .filter(|dep| !dep.is_cascadable && dep.count > 0)
            .map(|dep| dep.table_name.clone())
            .collect();

        // 3. Decide Action: Hard Delete or Soft Delete/Prevent
        let can_hard_delete = options.allow_hard_delete 
                              && auth.role == UserRole::Admin
                              && (blocking_dependencies.is_empty() || options.force);

        if can_hard_delete {
            // --- Hard Delete Path --- 
            let mut tx = self.pool.begin().await.map_err(DbError::from)?; // Start transaction

            let hard_delete_result = async {
                // Create tombstone
                let tombstone = Tombstone::new(id, self.repo.entity_name(), auth.user_id);
                let operation_id = tombstone.operation_id; // Capture for changelog
                self.tombstone_repo.create_tombstone_with_tx(&tombstone, &mut tx).await?;

                // Create change log entry for HardDelete
                let change_log = ChangeLogEntry {
                    operation_id, // Use same ID as tombstone
                    entity_table: self.repo.entity_name().to_string(),
                    entity_id: id,
                    operation_type: ChangeOperationType::HardDelete,
                    field_name: None,
                    old_value: None, // Maybe store serialized entity before delete?
                    new_value: None,
                    document_metadata: None, // Not a document deletion itself
                    timestamp: Utc::now(),
                    user_id: auth.user_id,
                    device_id: auth.device_id.parse::<Uuid>().ok(),
                    sync_batch_id: None,
                    processed_at: None,
                    sync_error: None,
                };
                self.change_log_repo.create_change_log_with_tx(&change_log, &mut tx).await?;

                // Perform the actual hard delete
                self.repo.hard_delete_with_tx(id, auth, &mut tx).await?;

                // *** Cascade Delete Documents ***
                // Explicitly type the cascade function call if necessary
                self.cascade_delete_documents::<E>(table_name, id, true, auth, &mut tx).await?;

                Ok::<_, DomainError>(())
            }.await;

            match hard_delete_result {
                Ok(_) => {
                    tx.commit().await.map_err(DbError::from)?; // Commit on success
                    Ok(DeleteResult::HardDeleted)
                },
                Err(e @ DomainError::EntityNotFound(_, _)) => {
                    let _ = tx.rollback().await;
                    Err(e) 
                },
                Err(e) => {
                    let _ = tx.rollback().await; 
                    Err(e) 
                }
            }

        } else {
            // --- Soft Delete or Prevent Path --- 
            if !blocking_dependencies.is_empty() && !options.fallback_to_soft_delete {
                return Ok(DeleteResult::DependenciesPrevented { dependencies: blocking_dependencies });
            }

            let mut tx = self.pool.begin().await.map_err(DbError::from)?; 

            let soft_delete_result = self.repo.soft_delete_with_tx(id, auth, &mut tx).await;

            match soft_delete_result {
                Ok(_) => {
                     // *** Cascade Soft Delete Documents ***
                     self.cascade_delete_documents::<E>(table_name, id, false, auth, &mut tx).await?;

                    tx.commit().await.map_err(DbError::from)?; 
                    Ok(DeleteResult::SoftDeleted { dependencies: blocking_dependencies })
                },
                Err(e @ DomainError::EntityNotFound(_, _)) => {
                    let _ = tx.rollback().await;
                    Err(e) 
                },
                Err(e) => {
                    let _ = tx.rollback().await; 
                    Err(e) 
                }
            }
        }
    }
    
    /// Delete multiple entities with specified options
    async fn batch_delete(
        &self,
        ids: &[Uuid],
        auth: &AuthContext,
        options: DeleteOptions,
    ) -> DomainResult<BatchDeleteResult> {
        // Check permissions based on options
        if options.allow_hard_delete && auth.role != UserRole::Admin {
            return Err(DomainError::AuthorizationFailed("Only admins can perform hard deletes".to_string()));
        }
        
        if options.force && auth.role != UserRole::Admin {
            return Err(DomainError::AuthorizationFailed("Only admins can force delete operations".to_string()));
        }
        
        let mut result = BatchDeleteResult::default();
        
        // Process each ID
        for &id in ids {
            match self.delete(id, auth, options.clone()).await {
                Ok(DeleteResult::HardDeleted) => {
                    result.hard_deleted.push(id);
                },
                
                Ok(DeleteResult::SoftDeleted { dependencies }) => {
                    result.soft_deleted.push(id);
                    if !dependencies.is_empty() {
                        result.dependencies.insert(id, dependencies);
                    }
                },
                
                Ok(DeleteResult::DependenciesPrevented { dependencies }) => {
                    result.failed.push(id);
                    result.dependencies.insert(id, dependencies);
                },
                
                Err(e @ DomainError::AuthorizationFailed(_)) => {
                    result.failed.push(id);
                    result.errors.insert(id, e);
                },
                Err(e @ DomainError::EntityNotFound(_, _)) => {
                    result.failed.push(id);
                    result.errors.insert(id, e);
                },
                Err(e) => {
                    result.failed.push(id);
                    result.errors.insert(id, e);
                }
            }
        }
        
        Ok(result)
    }
    
    /// Delete multiple entities with their dependencies
    async fn delete_with_dependencies(
        &self,
        id: Uuid,
        auth: &AuthContext,
    ) -> DomainResult<DeleteResult> {
        // This operation is admin-only
        if auth.role != UserRole::Admin {
            return Err(DomainError::AuthorizationFailed("Only admins can delete entities with dependencies".to_string()));
        }
        
        // Check for dependencies
        let table_name = self.repo.entity_name();
        let dependencies = self.dependency_checker().check_dependencies(table_name, id).await?;
        
        // If no dependencies, just do a normal hard delete
        if dependencies.is_empty() {
            return self.delete(
                id, 
                auth, 
                DeleteOptions {
                    allow_hard_delete: true,
                    fallback_to_soft_delete: false,
                    force: false,
                }
            ).await;
        }
        
        // For each dependency, we need to handle it based on whether it's cascadable
        // This would be a more complex implementation and might require specific
        // logic per entity type to handle the dependency chain correctly
        
        // For simplicity, we'll just use the force option which will hard delete
        // regardless of dependencies (relying on database ON DELETE constraints to handle cascades)
        self.delete(
            id, 
            auth, 
            DeleteOptions {
                allow_hard_delete: true,
                fallback_to_soft_delete: false,
                force: true,
            }
        ).await
    }

    /// Retrieve details about records that failed to delete during a batch operation
    async fn get_failed_delete_details(
        &self,
        batch_result: &BatchDeleteResult,
        auth: &AuthContext,
    ) -> DomainResult<Vec<FailedDeleteDetail<E>>> {
        let mut details = Vec::new();
        let entity_type_name = self.repo.entity_name().to_string();

        for &id in &batch_result.failed {
            // Attempt to fetch the entity data (might fail if already deleted or never existed)
            // Note: This assumes find_by_id doesn't require specific permissions beyond auth context validity,
            // or that the auth context used here has sufficient read permissions.
            let entity_data_result = self.repo.find_by_id(id).await;

            let entity_data = match entity_data_result {
                 Ok(entity) => Some(entity),
                 Err(DomainError::EntityNotFound(_, _)) => None, // Expected if it was never found
                 Err(_) => None, // Other error fetching, treat as data unavailable
            };

            // Determine the failure reason
            let (reason, deps) = match batch_result.errors.get(&id) {
                Some(DomainError::EntityNotFound(_, _)) => (FailureReason::NotFound, Vec::new()),
                Some(DomainError::AuthorizationFailed(_)) => (FailureReason::AuthorizationFailed, Vec::new()),
                Some(DomainError::Database(db_err)) => (FailureReason::DatabaseError(db_err.to_string()), Vec::new()),
                // Use wildcard _ to catch any other DomainError variant or None
                Some(_) | None => { 
                    // If Conflict or no specific error, check dependencies map
                    if let Some(dep_tables) = batch_result.dependencies.get(&id) {
                         // Check if it was actually soft-deleted (meaning dependencies prevented hard delete)
                         if batch_result.soft_deleted.contains(&id) {
                              (FailureReason::SoftDeletedDueToDependencies, dep_tables.clone())
                         } else {
                             // If it's in failed *and* dependencies, it was prevented entirely
                             (FailureReason::DependenciesPrevented, dep_tables.clone())
                         }
                    } else {
                         // In failed, but no specific error and no dependencies recorded -> Unknown
                         (FailureReason::Unknown, Vec::new())
                    }
                },
                 _ => (FailureReason::Unknown, Vec::new()), // Catch-all for other DomainErrors
            };

            details.push(FailedDeleteDetail {
                id,
                entity_data,
                entity_type: entity_type_name.clone(),
                reason,
                dependencies: deps,
            });
        }

        Ok(details)
    }
}

// /Users/sagarshrestha/ipad_rust_core copy/src/domains/core/dependency_checker.rs
use crate::errors::DomainResult;
use async_trait::async_trait;
use sqlx::{Pool, Sqlite, query_as};
use uuid::Uuid;
use std::collections::HashMap;
use crate::errors::DbError;
use crate::errors::DomainError;

/// Dependency information
#[derive(Debug, Clone)]
pub struct Dependency {
    /// Name of the table with dependent records
    pub table_name: String,
    
    /// Count of dependent records
    pub count: i64,
    
    /// Name of the foreign key column
    pub foreign_key_column: String,
    
    /// Whether the dependency is cascadable (ON DELETE CASCADE)
    pub is_cascadable: bool,
}

/// Trait for dependency checking
#[async_trait]
pub trait DependencyChecker: Send + Sync {
    /// Check for dependencies for an entity
    async fn check_dependencies(&self, table_name: &str, id: Uuid) -> DomainResult<Vec<Dependency>>;
    
    /// Get a simplified list of dependency tables
    async fn get_dependency_tables(&self, table_name: &str, id: Uuid) -> DomainResult<Vec<String>> {
        let dependencies = self.check_dependencies(table_name, id).await?;
        Ok(dependencies.into_iter().map(|dep| dep.table_name).collect())
    }
}

/// SQLite implementation of the DependencyChecker
pub struct SqliteDependencyChecker {
    pool: Pool<Sqlite>,
    /// Maps table name to its cascadable dependencies
    dependency_map: HashMap<String, Vec<(String, String, bool)>>,
}

impl SqliteDependencyChecker {
    /// Create a new SQLite dependency checker
    pub fn new(pool: Pool<Sqlite>) -> Self {
        let mut dependency_map = HashMap::new();
        
        // Define dependencies based on schema
        // Format: (table_name, [(dependent_table, foreign_key_column, is_cascadable)])
        
        // Strategic goals dependencies
        dependency_map.insert(
            "strategic_goals".to_string(), 
            vec![
                ("projects".to_string(), "strategic_goal_id".to_string(), false),
            ]
        );
        
        // Projects dependencies
        dependency_map.insert(
            "projects".to_string(), 
            vec![
                ("workshops".to_string(), "project_id".to_string(), false),
                ("activities".to_string(), "project_id".to_string(), true),
                ("livelihoods".to_string(), "project_id".to_string(), false),
                ("project_funding".to_string(), "project_id".to_string(), false),
            ]
        );
        
        // Workshops dependencies
        dependency_map.insert(
            "workshops".to_string(), 
            vec![
                ("workshop_participants".to_string(), "workshop_id".to_string(), true),
            ]
        );
        
        // Participants dependencies
        dependency_map.insert(
            "participants".to_string(), 
            vec![
                ("workshop_participants".to_string(), "participant_id".to_string(), true),
                ("livelihoods".to_string(), "participant_id".to_string(), true),
            ]
        );
        
        // Livelihoods dependencies
        dependency_map.insert(
            "livelihoods".to_string(), 
            vec![
                ("subsequent_grants".to_string(), "livelihood_id".to_string(), true),
            ]
        );
        
        // Donors dependencies
        dependency_map.insert(
            "donors".to_string(), 
            vec![
                ("project_funding".to_string(), "donor_id".to_string(), false),
            ]
        );

        // Document Types dependencies
        dependency_map.insert(
            "document_types".to_string(),
            vec![
                // MediaDocuments depend on DocumentTypes, but deletion is RESTRICTED by FK.
                // Hard deleting a DocumentType SHOULD be blocked if MediaDocuments use it.
                ("media_documents".to_string(), "type_id".to_string(), false),
            ]
        );

        // NOTE: We intentionally DO NOT add entries for other tables (projects, workshops, etc.)
        // pointing to media_documents here. The check for those dependencies is handled
        // differently in check_dependencies to allow the desired cascading behavior.
        
        Self { pool, dependency_map }
    }
}

/// Query result for dependency count
#[derive(Debug, sqlx::FromRow)]
struct DependencyCount {
    count: i64,
}

#[async_trait]
impl DependencyChecker for SqliteDependencyChecker {
    async fn check_dependencies(&self, table_name: &str, id: Uuid) -> DomainResult<Vec<Dependency>> {
        let mut dependencies = Vec::new();
        let id_str = id.to_string();

        // 1. Check defined dependencies from the map
        if let Some(dependent_tables) = self.dependency_map.get(table_name) {
            for (dependent_table, foreign_key, is_cascadable) in dependent_tables {
                // Build and execute query to count dependencies
                let query = format!(
                    "SELECT COUNT(*) as count FROM {} WHERE {} = ? AND deleted_at IS NULL", // Check only non-deleted dependents
                    dependent_table, 
                    foreign_key
                );
                
                let count_result: Result<DependencyCount, sqlx::Error> = query_as(&query)
                    .bind(&id_str)
                    .fetch_one(&self.pool)
                    .await;
                    
                let count = match count_result {
                    Ok(c) => c.count,
                    Err(sqlx::Error::RowNotFound) => 0, // Treat RowNotFound as 0 count
                    Err(e) => return Err(DomainError::Database(DbError::from(e))), // Propagate other errors
                };
                
                if count > 0 {
                    dependencies.push(Dependency {
                        table_name: dependent_table.clone(),
                        count: count,
                        foreign_key_column: foreign_key.clone(),
                        is_cascadable: *is_cascadable,
                    });
                }
            }
        }

        // 2. Special check for media_documents dependency for ALL tables EXCEPT document_types itself.
        // We want to know if documents exist, but NOT treat them as blocking for parents.
        // The service layer will handle the cascade.
        if table_name != "document_types" {
            let query = "SELECT COUNT(*) as count FROM media_documents WHERE related_table = ? AND related_id = ? AND deleted_at IS NULL";
            let count_result: Result<DependencyCount, sqlx::Error> = query_as(query)
                .bind(table_name) // Bind the parent table name
                .bind(&id_str)     // Bind the parent ID
                .fetch_one(&self.pool)
                .await;

            let count = match count_result {
                 Ok(c) => c.count,
                 Err(sqlx::Error::RowNotFound) => 0,
                 Err(e) => return Err(DomainError::Database(DbError::from(e))),
            };

            if count > 0 {
                // Add this dependency, but mark it as non-blocking (is_cascadable = true)
                // even though the FK isn't CASCADE. The service layer interprets this.
                dependencies.push(Dependency {
                    table_name: "media_documents".to_string(),
                    count: count,
                    foreign_key_column: "related_id".to_string(), // Indicate the relevant key
                    is_cascadable: true, // Signal to DeleteService this is handled by service-level cascade
                });
            }
        }
        
        Ok(dependencies)
    }
}

// /Users/sagarshrestha/ipad_rust_core copy/src/domains/core/document_linking.rs
// src/domains/core/document_linking.rs
use serde::Serialize;
use std::collections::HashSet;

#[derive(Debug, Clone, PartialEq, Eq, Serialize)]
pub enum FieldType {
    Text,
    Number, // Represents i64, f64
    Boolean,
    Date, // Represents NaiveDate or String YYYY-MM-DD
    Timestamp, // Represents DateTime<Utc> or String RFC3339
    Uuid,
    Decimal,
    // Special type for fields that primarily store a document reference
    DocumentRef, 
}

#[derive(Debug, Clone)]
pub struct EntityFieldMetadata {
    /// Technical name of the field (matches struct/db potentially)
    pub field_name: &'static str,
    /// User-friendly name for UI display
    pub display_name: &'static str,
    /// Can documents be logically linked to this field?
    pub supports_documents: bool,
    /// The type of the field (for UI hints, validation)
    pub field_type: FieldType,
    /// Is this field primarily just a reference to a document?
    pub is_document_reference_only: bool,
}

/// Trait for entities that allow documents to be linked to specific fields.
pub trait DocumentLinkable {
    /// Provides metadata for all fields relevant for display or linking.
    fn field_metadata() -> Vec<EntityFieldMetadata>;

    /// Get the names of fields that support document attachments.
    fn document_linkable_fields() -> HashSet<String> {
        Self::field_metadata()
            .into_iter()
            .filter(|meta| meta.supports_documents)
            .map(|meta| meta.field_name.to_string())
            .collect()
    }

    /// Check if a specific field supports document linking.
    fn is_document_linkable_field(field: &str) -> bool {
        Self::document_linkable_fields().contains(field)
    }

    /// Get metadata for a specific field by name.
    fn get_field_metadata(field_name: &str) -> Option<EntityFieldMetadata> {
         Self::field_metadata().into_iter().find(|meta| meta.field_name == field_name)
    }
}


// API Response Structure (could live in a web/api layer types mod)
#[derive(Serialize)]
pub struct FieldMetadataResponse {
    pub field_name: String,
    pub display_name: String,
    pub supports_documents: bool,
    pub field_type: FieldType, // Serialize the enum directly
    pub is_document_reference_only: bool,
}

impl From<EntityFieldMetadata> for FieldMetadataResponse {
    fn from(meta: EntityFieldMetadata) -> Self {
        FieldMetadataResponse {
            field_name: meta.field_name.to_string(),
            display_name: meta.display_name.to_string(),
            supports_documents: meta.supports_documents,
            field_type: meta.field_type,
            is_document_reference_only: meta.is_document_reference_only,
        }
    }
} 

// /Users/sagarshrestha/ipad_rust_core copy/src/domains/core/file_storage_service.rs
use async_trait::async_trait;
use std::path::{Path, PathBuf};
use thiserror::Error;
use tokio::fs; // Use tokio::fs for async file operations
use uuid::Uuid;
use std::io;

#[derive(Debug, Error)]
pub enum FileStorageError {
    #[error("I/O error: {0}")]
    Io(#[from] std::io::Error),
    #[error("File not found: {0}")]
    NotFound(String),
    #[error("Configuration error: {0}")]
    Configuration(String),
    #[error("Permission denied: {0}")]
    PermissionDenied(String),
    #[error("Storage limit exceeded")]
    LimitExceeded,
    #[error("Invalid path component: {0}")]
    InvalidPathComponent(String),
    #[error("Unknown storage error: {0}")]
    Other(String),
}

pub type FileStorageResult<T> = Result<T, FileStorageError>;

/// Service trait for abstracting file storage operations
#[async_trait]
pub trait FileStorageService: Send + Sync {
    /// Save file data to storage, returning the relative path and size.
    /// The implementation determines the final path structure.
    async fn save_file(
        &self,
        data: Vec<u8>,
        entity_type: &str, // e.g., "strategic_goals", "documents"
        entity_or_temp_id: &str,   // Associated entity ID (Uuid as string) or temp ID
        suggested_filename: &str, // Original filename for extension/naming hint
    ) -> FileStorageResult<(String, u64)>; // Returns (relative_path, size_bytes)

    /// Delete a file from storage using its relative path.
    async fn delete_file(&self, relative_path: &str) -> FileStorageResult<()>;

    /// Get a readable stream or bytes for a file.
    /// (Using Vec<u8> for simplicity, could use streams like Tokio's AsyncRead)
    async fn get_file_data(&self, relative_path: &str) -> FileStorageResult<Vec<u8>>;
    
    /// Get the full absolute path for a given relative path (for internal use if needed)
    fn get_absolute_path(&self, relative_path: &str) -> PathBuf;
}

// --- Local File Storage Implementation ---

pub struct LocalFileStorageService {
    base_path: PathBuf,
    original_subdir: String,
    compressed_subdir: String,
}

impl LocalFileStorageService {
    /// Creates a new LocalFileStorageService.
    /// Ensures the base directory and subdirectories exist.
    pub fn new(base_path_str: &str) -> io::Result<Self> {
        let base_path = PathBuf::from(base_path_str);
        let original_subdir = "original".to_string();
        let compressed_subdir = "compressed".to_string();

        let original_path = base_path.join(&original_subdir);
        let compressed_path = base_path.join(&compressed_subdir);

        // Create directories synchronously during setup
        std::fs::create_dir_all(&original_path)?;
        std::fs::create_dir_all(&compressed_path)?;

        Ok(Self {
            base_path,
            original_subdir,
            compressed_subdir,
        })
    }

    /// Sanitizes a path component to prevent directory traversal issues.
    fn sanitize_component(component: &str) -> Result<String, FileStorageError> {
        // Check for invalid characters or patterns
        if component.is_empty() || component.contains('/') || component.contains('\\') || component == "." || component == ".." {
            Err(FileStorageError::InvalidPathComponent(component.to_string()))
        } else {
            Ok(component.to_string())
        }
    }

    /// Generates a unique filename based on suggestion and a new UUID.
    fn generate_unique_filename(suggested_filename: &str) -> String {
        let extension = Path::new(suggested_filename)
            .extension()
            .and_then(|ext| ext.to_str())
            .map(|ext| format!(".{}", ext))
            .unwrap_or_default();
        format!("{}{}", Uuid::new_v4(), extension)
    }
}

#[async_trait]
impl FileStorageService for LocalFileStorageService {
    async fn save_file(
        &self,
        data: Vec<u8>,
        entity_type: &str,
        entity_or_temp_id: &str,
        suggested_filename: &str,
    ) -> FileStorageResult<(String, u64)> {
        let sanitized_entity_type = Self::sanitize_component(entity_type)?;
        let sanitized_id = Self::sanitize_component(entity_or_temp_id)?;
        let unique_filename = Self::generate_unique_filename(suggested_filename);

        // Construct relative path: original/entity_type/entity_or_temp_id/unique_filename.ext
        let relative_path = Path::new(&self.original_subdir)
            .join(&sanitized_entity_type)
            .join(&sanitized_id)
            .join(&unique_filename);

        // Correctly get the relative path as a string slice for get_absolute_path
        let relative_path_str = relative_path.to_str().ok_or_else(|| FileStorageError::Other("Failed to convert relative path to string".to_string()))?;
        let absolute_path = self.get_absolute_path(relative_path_str);

        let parent_dir = absolute_path.parent().ok_or_else(|| FileStorageError::Other("Invalid path generated, no parent directory".to_string()))?;

        // Ensure the parent directory exists
        fs::create_dir_all(parent_dir).await?;

        let file_size = data.len() as u64;

        // Write the file asynchronously
        fs::write(&absolute_path, data).await?;

        Ok((relative_path_str.to_string(), file_size))
    }

    async fn delete_file(&self, relative_path: &str) -> FileStorageResult<()> {
        let absolute_path = self.get_absolute_path(relative_path);

        // Basic check to prevent deleting outside the base path (though sanitize helps)
        if !absolute_path.starts_with(&self.base_path) {
            return Err(FileStorageError::PermissionDenied("Attempt to delete outside base path".to_string()));
        }

        match fs::remove_file(&absolute_path).await {
            Ok(_) => Ok(()),
            Err(e) if e.kind() == io::ErrorKind::NotFound => {
                // Consider it success if the file is already gone
                Ok(())
            }
            Err(e) => Err(FileStorageError::Io(e)),
        }
        // TODO: Consider deleting empty parent directories? Maybe not necessary.
    }

    async fn get_file_data(&self, relative_path: &str) -> FileStorageResult<Vec<u8>> {
        let absolute_path = self.get_absolute_path(relative_path);

        // Basic check
        if !absolute_path.starts_with(&self.base_path) {
             return Err(FileStorageError::PermissionDenied("Attempt to read outside base path".to_string()));
        }

        match fs::read(&absolute_path).await {
            Ok(data) => Ok(data),
            Err(e) if e.kind() == io::ErrorKind::NotFound => {
                 Err(FileStorageError::NotFound(relative_path.to_string()))
            }
            Err(e) => Err(FileStorageError::Io(e)),
        }
    }

    fn get_absolute_path(&self, relative_path: &str) -> PathBuf {
        // IMPORTANT: This assumes relative_path is ALREADY somewhat sanitized
        // or comes from a trusted source (like the DB). It cleans ".." etc.
        // normalize_path from the path_clean crate could be more robust if needed.
        let mut abs_path = self.base_path.clone();
        for component in Path::new(relative_path).components() {
             match component {
                std::path::Component::Normal(comp_str) => {
                    // Convert OsStr to str for checking. Handle potential non-UTF8 gracefully.
                    if let Some(s) = comp_str.to_str() {
                        if s.is_empty() || s.contains('/') || s.contains('\\') {
                            // Skip potentially problematic components
                            // Logging this might be useful in practice
                            continue;
                        }
                        abs_path.push(comp_str);
                    } else {
                        // Handle non-UTF8 path components if necessary, 
                        // for now, we might skip them or return an error.
                        // Skipping for simplicity.
                        continue; 
                    }
                },
                _ => { /* Skip RootDir, CurDir, ParentDir safely */ }
            }
        }
        abs_path
    }
} 

// /Users/sagarshrestha/ipad_rust_core copy/src/domains/document/file_deletion_worker.rs
use crate::domains::core::file_storage_service::{FileStorageService, FileStorageError};
use crate::errors::{DomainError, DomainResult, ServiceError, ServiceResult, DbError};
use sqlx::{SqlitePool, Row};
use uuid::Uuid;
use std::sync::Arc;
use chrono::{Utc, Duration};
use tokio::time;

/// Worker for processing file deletions in the background
pub struct FileDeletionWorker {
    pool: SqlitePool,
    file_storage_service: Arc<dyn FileStorageService>,
    shutdown_signal: Option<tokio::sync::oneshot::Receiver<()>>,
    active_files_check_enabled: bool,
}

impl FileDeletionWorker {
    pub fn new(
        pool: SqlitePool,
        file_storage_service: Arc<dyn FileStorageService>,
    ) -> Self {
        Self {
            pool,
            file_storage_service,
            shutdown_signal: None,
            active_files_check_enabled: true,
        }
    }
    
    /// Set shutdown signal receiver
    pub fn with_shutdown_signal(mut self, receiver: tokio::sync::oneshot::Receiver<()>) -> Self {
        self.shutdown_signal = Some(receiver);
        self
    }
    
    /// Disable active files check (useful for testing or cleanup mode)
    pub fn disable_active_files_check(mut self) -> Self {
        self.active_files_check_enabled = false;
        self
    }
    
    /// Start the worker loop
    pub async fn start(mut self) -> Result<(), ServiceError> {
        log::info!("Starting file deletion worker");
        
        // Run every 5 minutes
        let mut interval = time::interval(time::Duration::from_secs(5 * 60));
        
        loop {
            tokio::select! {
                _ = interval.tick() => {
                    if let Err(e) = self.process_deletion_queue().await {
                        log::error!("Error processing file deletion queue: {:?}", e);
                    }
                }
                _ = async {
                    if let Some(mut signal) = self.shutdown_signal.take() {
                        let _ = signal.await;
                        true
                    } else {
                        // Never complete if no shutdown signal
                        std::future::pending::<bool>().await
                    }
                } => {
                    log::info!("Received shutdown signal, stopping file deletion worker");
                    break;
                }
            }
        }
        
        Ok(())
    }
    
    /// Process the pending items in the file deletion queue
    async fn process_deletion_queue(&self) -> Result<(), ServiceError> {
        log::info!("Processing file deletion queue");
        
        // Get pending deletions where grace period has expired
        let pending_deletions = self.get_pending_deletions().await?;
        
        if pending_deletions.is_empty() {
            log::info!("No pending file deletions to process");
            return Ok(());
        }
        
        log::info!("Found {} pending file deletions", pending_deletions.len());
        
        for deletion in pending_deletions {
            // Skip files that are currently in use if active check is enabled
            if self.active_files_check_enabled && self.is_file_in_use(&deletion.document_id).await? {
                log::info!("Skipping file in use: document_id={}", deletion.document_id);
                
                // Update the last attempt timestamp
                let now_str = Utc::now().to_rfc3339();
                sqlx::query!(
                    r#"
                    UPDATE file_deletion_queue
                    SET 
                        last_attempt_at = ?,
                        attempts = attempts + 1
                    WHERE id = ?
                    "#,
                    now_str,
                    deletion.id
                )
                .execute(&self.pool)
                .await.map_err(|e| ServiceError::Domain(DomainError::Database(DbError::from(e))))?;
                
                continue;
            }
            
            // Try to delete the original file
            let original_result = if let Some(path) = &deletion.file_path {
                self.file_storage_service.delete_file(path).await
            } else {
                Ok(()) // No path, so "success"
            };
            
            // Try to delete the compressed file if it exists
            let compressed_result = if let Some(path) = &deletion.compressed_file_path {
                self.file_storage_service.delete_file(path).await
            } else {
                Ok(()) // No compressed path, so "success"
            };
            
            // Check results and update database
            let mut error_message = None;
            
            if let Err(e) = &original_result {
                if !matches!(e, FileStorageError::NotFound(_)) {
                    // Only log real errors, not just missing files
                    error_message = Some(format!("Error deleting original file: {}", e));
                }
            }
            
            if let Err(e) = &compressed_result {
                if !matches!(e, FileStorageError::NotFound(_)) {
                    let compressed_err = format!("Error deleting compressed file: {}", e);
                    error_message = match error_message {
                        Some(msg) => Some(format!("{}; {}", msg, compressed_err)),
                        None => Some(compressed_err),
                    };
                }
            }
            
            // Update database - mark as completed if both successfully deleted or not found
            if original_result.is_ok() && compressed_result.is_ok() {
                let now_str = Utc::now().to_rfc3339();
                sqlx::query!(
                    r#"
                    UPDATE file_deletion_queue
                    SET 
                        completed_at = ?,
                        last_attempt_at = ?,
                        attempts = attempts + 1,
                        error_message = NULL
                    WHERE id = ?
                    "#,
                    now_str,
                    now_str,
                    deletion.id
                )
                .execute(&self.pool)
                .await.map_err(|e| ServiceError::Domain(DomainError::Database(DbError::from(e))))?;
                
                log::info!("Successfully deleted files for document: {}", deletion.document_id);
            } else {
                // Update attempt count and error message
                let now_str = Utc::now().to_rfc3339();
                sqlx::query!(
                    r#"
                    UPDATE file_deletion_queue
                    SET 
                        last_attempt_at = ?,
                        attempts = attempts + 1,
                        error_message = ?
                    WHERE id = ?
                    "#,
                    now_str,
                    error_message,
                    deletion.id
                )
                .execute(&self.pool)
                .await.map_err(|e| ServiceError::Domain(DomainError::Database(DbError::from(e))))?;
                
                log::warn!(
                    "Failed to delete files for document: {} - Error: {:?}",
                    deletion.document_id,
                    error_message
                );
            }
        }
        
        Ok(())
    }
    
    /// Get pending deletions that are ready to be processed (grace period expired)
    async fn get_pending_deletions(&self) -> Result<Vec<PendingFileDeletion>, ServiceError> {
        let rows = sqlx::query!(
            r#"
            SELECT 
                id as "id!",
                document_id as "document_id!",
                file_path as "file_path: Option<String>",
                compressed_file_path as "compressed_file_path: Option<String>",
                requested_at as "requested_at!",
                attempts
            FROM file_deletion_queue
            WHERE 
                completed_at IS NULL AND 
                datetime(requested_at) <= datetime('now', '-' || grace_period_seconds || ' seconds')
            ORDER BY
                attempts ASC, -- Try not-yet-attempted files first
                requested_at ASC -- Then oldest first
            LIMIT 100 -- Process in batches
            "#
        )
        .fetch_all(&self.pool)
        .await.map_err(|e| ServiceError::Domain(DomainError::Database(DbError::from(e))))?;
        
        let mut result = Vec::with_capacity(rows.len());
        
        for row in rows {
            result.push(PendingFileDeletion {
                id: row.id,
                document_id: row.document_id,
                file_path: row.file_path,
                compressed_file_path: row.compressed_file_path.flatten(),
                requested_at: row.requested_at,
                attempts: row.attempts.unwrap_or(0),
            });
        }
        
        Ok(result)
    }
    
    /// Check if file is currently in use
    async fn is_file_in_use(&self, document_id: &str) -> Result<bool, ServiceError> {
        // Check active_file_usage table
        let result = sqlx::query!(
            r#"
            SELECT EXISTS(
                SELECT 1 
                FROM active_file_usage 
                WHERE 
                    document_id = ? AND 
                    last_active_at > datetime('now', '-5 minutes')
            ) as in_use
            "#,
            document_id
        )
        .fetch_one(&self.pool)
        .await.map_err(|e| ServiceError::Domain(DomainError::Database(DbError::from(e))))?;
        
        Ok(result.in_use == 1)
    }
}

/// Struct representing a pending file deletion
struct PendingFileDeletion {
    id: String,
    document_id: String,
    file_path: Option<String>,
    compressed_file_path: Option<String>,
    requested_at: String,
    attempts: i64,
} 

// /Users/sagarshrestha/ipad_rust_core copy/src/domains/document/repository.rs
use crate::auth::AuthContext;
use crate::domains::core::repository::{FindById, SoftDeletable, HardDeletable};
use crate::domains::core::delete_service::DeleteServiceRepository;
use crate::domains::document::types::{
    DocumentType, DocumentTypeRow, NewDocumentType, UpdateDocumentType,
    MediaDocument, MediaDocumentRow, NewMediaDocument, CompressionStatus, BlobSyncStatus,
    // UpdateMediaDocument, // REMOVED
    DocumentVersion, DocumentVersionRow,
    DocumentAccessLog, DocumentAccessLogRow, NewDocumentAccessLog
};
use crate::errors::{DbError, DomainError, DomainResult, ValidationError};
use crate::domains::sync::types::SyncPriority;
use async_trait::async_trait;
use chrono::Utc;
use sqlx::{query, query_as, query_scalar, Pool, Row, Sqlite, Transaction};
use uuid::Uuid;
use std::collections::HashMap; // For update_paths_and_status
use std::str::FromStr; // Import FromStr if not already imported at the top of the file
use crate::types::{PaginatedResult, PaginationParams};
// --- Add imports for ChangeLog ---
use crate::domains::sync::repository::ChangeLogRepository;
use crate::domains::sync::types::{ChangeLogEntry, ChangeOperationType};
use std::sync::Arc;
use serde_json;

/// Temporary table identifier for linking documents before the main entity exists
pub const TEMP_RELATED_TABLE: &str = "TEMP";

// --- Document Type Repository ---

#[async_trait]
pub trait DocumentTypeRepository: DeleteServiceRepository<DocumentType> + Send + Sync {
    async fn create(
        &self,
        new_type: &NewDocumentType,
        auth: &AuthContext,
    ) -> DomainResult<DocumentType>;

    async fn update(
        &self,
        id: Uuid,
        update_data: &UpdateDocumentType,
        auth: &AuthContext,
    ) -> DomainResult<DocumentType>;

    async fn find_all(&self, params: PaginationParams) -> DomainResult<PaginatedResult<DocumentType>>;

    async fn find_by_name(&self, name: &str) -> DomainResult<Option<DocumentType>>;
}

pub struct SqliteDocumentTypeRepository {
    pool: Pool<Sqlite>,
    change_log_repo: Arc<dyn ChangeLogRepository + Send + Sync>,
}

impl SqliteDocumentTypeRepository {
    pub fn new(pool: Pool<Sqlite>, change_log_repo: Arc<dyn ChangeLogRepository + Send + Sync>) -> Self {
        Self { pool, change_log_repo }
    }
    fn entity_name() -> &'static str {
        "document_types" // Table name
    }
    fn map_row(row: DocumentTypeRow) -> DomainResult<DocumentType> {
        row.into_entity() // Use the conversion method
    }
    // Helper for internal use within transactions
    async fn find_by_id_with_tx<'t>(
        &self,
        id: Uuid,
        tx: &mut Transaction<'t, Sqlite>,
    ) -> DomainResult<DocumentType> {
        query_as::<_, DocumentTypeRow>("SELECT * FROM document_types WHERE id = ? AND deleted_at IS NULL")
            .bind(id.to_string())
            .fetch_optional(&mut **tx)
            .await
            .map_err(DbError::from)?
            .ok_or_else(|| DomainError::EntityNotFound(Self::entity_name().to_string(), id))
            .and_then(Self::map_row)
    }
    // Helper to log changes consistently
    async fn log_change_entry<'t>(
        &self,
        entry: ChangeLogEntry,
        tx: &mut Transaction<'t, Sqlite>,
    ) -> DomainResult<()> {
        self.change_log_repo.create_change_log_with_tx(&entry, tx).await
    }
}

#[async_trait]
impl FindById<DocumentType> for SqliteDocumentTypeRepository {
    async fn find_by_id(&self, id: Uuid) -> DomainResult<DocumentType> {
        query_as::<_, DocumentTypeRow>("SELECT * FROM document_types WHERE id = ? AND deleted_at IS NULL")
            .bind(id.to_string())
            .fetch_optional(&self.pool)
            .await
            .map_err(DbError::from)?
            .ok_or_else(|| DomainError::EntityNotFound(self.entity_name().to_string(), id))
            .and_then(Self::map_row)
    }
}

#[async_trait]
impl SoftDeletable for SqliteDocumentTypeRepository {
    async fn soft_delete_with_tx(
        &self,
        id: Uuid,
        auth: &AuthContext,
        tx: &mut Transaction<'_, Sqlite>,
    ) -> DomainResult<()> {
        let now = Utc::now().to_rfc3339();
        let now_dt = Utc::now(); // For logging
        let user_uuid = auth.user_id;
        let device_uuid: Option<Uuid> = auth.device_id.parse::<Uuid>().ok();

        let result = query(
            "UPDATE document_types SET deleted_at = ?, deleted_by_user_id = ?, updated_at = ? WHERE id = ? AND deleted_at IS NULL"
        )
        .bind(&now)
        .bind(auth.user_id.to_string())
        .bind(&now)
        .bind(id.to_string())
        .execute(&mut **tx)
        .await
        .map_err(DbError::from)?;

        if result.rows_affected() == 0 {
            // Could be already deleted or not found
            Err(DomainError::EntityNotFound(Self::entity_name().to_string(), id))
        } else {
            Ok(())
        }
    }
    async fn soft_delete(&self, id: Uuid, auth: &AuthContext) -> DomainResult<()> {
        let mut tx = self.pool.begin().await.map_err(DbError::from)?;
        let result = self.soft_delete_with_tx(id, auth, &mut tx).await;
        match result {
            Ok(_) => { tx.commit().await.map_err(DbError::from)?; Ok(()) },
            Err(e) => { let _ = tx.rollback().await; Err(e) },
        }
    }
}

#[async_trait]
impl HardDeletable for SqliteDocumentTypeRepository {
    fn entity_name(&self) -> &'static str {
        "document_types"
    }
    async fn hard_delete_with_tx(
        &self,
        id: Uuid,
        _auth: &AuthContext, // Auth context might be used for logging/checks later
        tx: &mut Transaction<'_, Sqlite>,
    ) -> DomainResult<()> {
        let now_dt = Utc::now(); // For logging
        let user_uuid = _auth.user_id;
        let device_uuid: Option<Uuid> = _auth.device_id.parse::<Uuid>().ok();

        let result = query("DELETE FROM document_types WHERE id = ?")
            .bind(id.to_string())
            .execute(&mut **tx)
            .await
            .map_err(DbError::from)?;

        if result.rows_affected() == 0 {
            Err(DomainError::EntityNotFound(self.entity_name().to_string(), id))
        } else {
            Ok(())
        }
    }
    async fn hard_delete(&self, id: Uuid, auth: &AuthContext) -> DomainResult<()> {
        let mut tx = self.pool.begin().await.map_err(DbError::from)?;
        // Consider adding file system cleanup logic here or in the service calling this
        let result = self.hard_delete_with_tx(id, auth, &mut tx).await;
        match result {
            Ok(_) => { tx.commit().await.map_err(DbError::from)?; Ok(()) },
            Err(e) => { let _ = tx.rollback().await; Err(e) },
        }
    }
}

#[async_trait]
impl DocumentTypeRepository for SqliteDocumentTypeRepository {
    async fn create(
        &self,
        new_type: &NewDocumentType,
        auth: &AuthContext,
    ) -> DomainResult<DocumentType> {
        let id = Uuid::new_v4();
        let mut tx = self.pool.begin().await.map_err(DbError::from)?;

        let result = async {
            let now = Utc::now().to_rfc3339();
            let now_dt = Utc::now(); // Use DateTime<Utc> for logging
            let user_id = auth.user_id.to_string();
            let user_uuid = auth.user_id;
            let device_uuid: Option<Uuid> = auth.device_id.parse::<Uuid>().ok();

            query(
                r#"INSERT INTO document_types (
                    id, name, description, icon, color, default_priority, // Now TEXT
                    created_at, updated_at, created_by_user_id, updated_by_user_id,
                    name_updated_at, name_updated_by, description_updated_at, description_updated_by,
                    icon_updated_at, icon_updated_by, color_updated_at, color_updated_by
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)"# 
            )
            .bind(id.to_string())
            .bind(&new_type.name)
            .bind(&new_type.description)
            .bind(&new_type.icon)
            .bind(&new_type.color)
            .bind(new_type.default_priority.as_str()) // Bind default_priority as TEXT
            .bind(&now).bind(&now)
            .bind(&user_id).bind(&user_id)
            // LWW fields initialization
            .bind(&now).bind(&user_id) // name
            .bind(new_type.description.as_ref().map(|_| &now))
            .bind(new_type.description.as_ref().map(|_| &user_id)) // description
            .bind(new_type.icon.as_ref().map(|_| &now))
            .bind(new_type.icon.as_ref().map(|_| &user_id)) // icon
            .bind(new_type.color.as_ref().map(|_| &now))
            .bind(new_type.color.as_ref().map(|_| &user_id)) // color
            .execute(&mut *tx)
            .await
            .map_err(DbError::from)?;

            // Log Create Operation within the transaction
            let entry = ChangeLogEntry {
                operation_id: Uuid::new_v4(),
                entity_table: Self::entity_name().to_string(),
                entity_id: id,
                operation_type: ChangeOperationType::Create,
                field_name: None,
                old_value: None,
                new_value: None, // Optionally serialize new_type
                timestamp: now_dt,
                user_id: user_uuid,
                device_id: device_uuid,
                document_metadata: None,
                sync_batch_id: None,
                processed_at: None,
                sync_error: None,
            };
            self.log_change_entry(entry, &mut tx).await?;

            // Fetch within the transaction before commit might be slightly faster
            // but requires careful handling if commit fails.
            // Fetching after commit is safer.
            Ok(id) // Return ID to fetch after commit
        }.await;

        match result {
            Ok(created_id) => {
                tx.commit().await.map_err(DbError::from)?;
                // Fetch the newly created record outside the transaction
                self.find_by_id(created_id).await
            },
            Err(e) => {
                let _ = tx.rollback().await; // Ensure rollback on error
                Err(e)
            }
        }
    }

    async fn update(
        &self,
        id: Uuid,
        update_data: &UpdateDocumentType,
        auth: &AuthContext,
    ) -> DomainResult<DocumentType> {
        let mut tx = self.pool.begin().await.map_err(DbError::from)?;
        let result = async {
            // Fetch existing to ensure it exists and for potential LWW checks (though not implemented here)
            let old_entity = self.find_by_id_with_tx(id, &mut tx).await?;
            let now = Utc::now().to_rfc3339();
            let now_dt = Utc::now(); // For logging
            let user_id = auth.user_id.to_string();
            let user_uuid = auth.user_id;
            let device_uuid: Option<Uuid> = auth.device_id.parse::<Uuid>().ok();

            // Use dynamic query building for updates
            let mut sets: Vec<String> = Vec::new();
            let mut binds: Vec<String> = Vec::new(); // Using String for simplicity, sqlx handles types

             macro_rules! add_lww_update {
                 ($field:ident, $value:expr) => {
                     sets.push(format!("{0} = ?, {0}_updated_at = ?, {0}_updated_by = ?", stringify!($field)));
                     binds.push($value.to_string()); // Value
                     binds.push(now.clone());       // Updated At
                     binds.push(user_id.clone());     // Updated By
                 };
                 ($field:ident, $value:expr, $type:ty) => { // For Option<String>
                     if let Some(val) = $value {
                        add_lww_update!($field, val);
                     }
                 };
             }

             if let Some(name) = &update_data.name {
                 add_lww_update!(name, name);
             }
             // Use explicit check for Option<String> to handle None vs Some("") if needed
             add_lww_update!(description, &update_data.description, Option<String>);
             add_lww_update!(icon, &update_data.icon, Option<String>);
             add_lww_update!(color, &update_data.color, Option<String>);

             // Non-LWW field update (if needed)
              if let Some(prio) = &update_data.default_priority {
                 sets.push("default_priority = ?".to_string());
                 binds.push(prio.as_str().to_string()); // Bind as TEXT
             }


            if sets.is_empty() {
                // No fields to update, just return existing
                return self.find_by_id_with_tx(id, &mut tx).await;
            }

            // Always update updated_at and updated_by_user_id
            sets.push("updated_at = ?".to_string());
            binds.push(now.clone());
            sets.push("updated_by_user_id = ?".to_string());
            binds.push(user_id.clone());

            let query_str = format!("UPDATE document_types SET {} WHERE id = ?", sets.join(", "));

            // Build and execute the query
            let mut q = query(&query_str);
            for bind_val in binds {
                q = q.bind(bind_val);
            }
            q = q.bind(id.to_string());

            q.execute(&mut *tx).await.map_err(DbError::from)?;

            // Fetch and return the updated record
            let new_entity = self.find_by_id_with_tx(id, &mut tx).await?;

            // Log field-level updates
            macro_rules! log_if_changed {
                ($field:ident) => {
                    if old_entity.$field != new_entity.$field {
                        let entry = ChangeLogEntry {
                            operation_id: Uuid::new_v4(),
                            entity_table: Self::entity_name().to_string(),
                            entity_id: id,
                            operation_type: ChangeOperationType::Update,
                            field_name: Some(stringify!($field).to_string()),
                            old_value: Some(serde_json::to_string(&old_entity.$field).unwrap_or_default()),
                            new_value: Some(serde_json::to_string(&new_entity.$field).unwrap_or_default()),
                            timestamp: now_dt,
                            user_id: user_uuid,
                            device_id: device_uuid,
                            document_metadata: None,
                            sync_batch_id: None,
                            processed_at: None,
                            sync_error: None,
                        };
                        self.log_change_entry(entry, &mut tx).await?;
                    }
                };
            }

            log_if_changed!(name);
            log_if_changed!(description);
            log_if_changed!(icon);
            log_if_changed!(color);
            log_if_changed!(default_priority);

            Ok(new_entity)

        }.await;

        match result {
            Ok(doc_type) => { tx.commit().await.map_err(DbError::from)?; Ok(doc_type) },
            Err(e) => { let _ = tx.rollback().await; Err(e) },
        }
    }

    async fn find_all(&self, params: PaginationParams) -> DomainResult<PaginatedResult<DocumentType>> {
        let offset = (params.page - 1) * params.per_page;
        let total: i64 = query_scalar("SELECT COUNT(*) FROM document_types WHERE deleted_at IS NULL")
            .fetch_one(&self.pool).await.map_err(DbError::from)?;

        let rows = query_as::<_, DocumentTypeRow>(
                "SELECT * FROM document_types WHERE deleted_at IS NULL ORDER BY name ASC LIMIT ? OFFSET ?"
            )
            .bind(params.per_page as i64).bind(offset as i64)
            .fetch_all(&self.pool).await.map_err(DbError::from)?;

        let items = rows.into_iter().map(Self::map_row).collect::<DomainResult<Vec<_>>>()?;
        Ok(PaginatedResult::new(items, total as u64, params))
    }

     async fn find_by_name(&self, name: &str) -> DomainResult<Option<DocumentType>> {
         query_as::<_, DocumentTypeRow>("SELECT * FROM document_types WHERE name = ? AND deleted_at IS NULL")
            .bind(name)
            .fetch_optional(&self.pool)
            .await
            .map_err(DbError::from)?
            .map(Self::map_row) // Use map_row for conversion
            .transpose() // Convert Option<Result<T, E>> to Result<Option<T>, E>
     }
}

// --- Media Document Repository ---

#[async_trait]
pub trait MediaDocumentRepository: DeleteServiceRepository<MediaDocument> + Send + Sync {
    async fn create(
        &self,
        new_doc: &NewMediaDocument,
        // file_path provided by service after saving file
    ) -> DomainResult<MediaDocument>;

    // UPDATE methods REMOVED - Documents are immutable via public API

    async fn find_by_related_entity(
        &self,
        related_table: &str,
        related_id: Uuid,
        params: PaginationParams,
    ) -> DomainResult<PaginatedResult<MediaDocument>>;

    async fn find_by_id(&self, id: Uuid) -> DomainResult<MediaDocument>;

    async fn find_by_id_with_tx<'t>(&self, id: Uuid, tx: &mut Transaction<'t, Sqlite>) -> DomainResult<MediaDocument>;

    /// Update compression status and optionally the compressed file path and size.
    /// Called internally by compression service.
    async fn update_compression_status(
        &self,
        id: Uuid,
        status: CompressionStatus,
        compressed_file_path: Option<&str>,
        compressed_size_bytes: Option<i64>, // ADDED size
    ) -> DomainResult<()>;

    /// Update blob sync status and key. Called internally by sync service.
    async fn update_blob_sync_status(
        &self,
        id: Uuid,
        status: BlobSyncStatus,
        blob_key: Option<&str>,
    ) -> DomainResult<()>;

    /// Update sync priority for one or more documents. Called internally? Or by specific API?
    async fn update_sync_priority(
        &self,
        ids: &[Uuid],
        priority: SyncPriority,
        auth: &AuthContext, // To track who initiated the change
    ) -> DomainResult<u64>;

    /// Update file paths and potentially status after sync download. Called internally by sync service.
    async fn update_paths_and_status(
        &self,
        document_id: Uuid,
        file_path: Option<&str>,
        compressed_file_path: Option<&str>,
        compressed_size_bytes: Option<i64>,
        compression_status: Option<CompressionStatus>,
    ) -> DomainResult<()>;


    /// Links documents created with a temporary ID to the actual entity ID after creation.
    async fn link_temp_documents(
        &self,
        temp_related_id: Uuid,
        final_related_table: &str,
        final_related_id: Uuid,
        tx: &mut Transaction<'_, Sqlite>, // Requires a transaction
    ) -> DomainResult<u64>; // Returns the number of documents linked
    
    /// Links documents with a temporary ID to their final entity
    /// This method handles the actual implementation and is called by link_temp_documents
    async fn link_temp_documents_with_tx(
        &self,
        temp_related_id: Uuid,
        final_related_table: &str,
        final_related_id: Uuid,
        tx: &mut Transaction<'_, Sqlite>,
    ) -> DomainResult<u64>;
}

pub struct SqliteMediaDocumentRepository {
    pool: Pool<Sqlite>,
    change_log_repo: Arc<dyn ChangeLogRepository + Send + Sync>,
}

impl SqliteMediaDocumentRepository {
    pub fn new(pool: Pool<Sqlite>, change_log_repo: Arc<dyn ChangeLogRepository + Send + Sync>) -> Self {
        Self { pool, change_log_repo }
    }
    fn entity_name() -> &'static str {
        "media_documents" // Table name
    }
     fn map_row(row: MediaDocumentRow) -> DomainResult<MediaDocument> {
        row.into_entity() // Use the conversion method
    }
    // Helper to log changes consistently
    async fn log_change_entry<'t>(
        &self,
        entry: ChangeLogEntry,
        tx: &mut Transaction<'t, Sqlite>,
    ) -> DomainResult<()> {
        self.change_log_repo.create_change_log_with_tx(&entry, tx).await
    }
}

// --- Basic trait implementations remain the same ---

#[async_trait]
impl FindById<MediaDocument> for SqliteMediaDocumentRepository {
    async fn find_by_id(&self, id: Uuid) -> DomainResult<MediaDocument> {
        query_as::<_, MediaDocumentRow>(
            "SELECT * FROM media_documents WHERE id = ? AND deleted_at IS NULL",
        )
        .bind(id.to_string())
        .fetch_optional(&self.pool)
        .await
        .map_err(DbError::from)?
        .ok_or_else(|| DomainError::EntityNotFound(Self::entity_name().to_string(), id))
        .and_then(Self::map_row)
    }
}

#[async_trait]
impl SoftDeletable for SqliteMediaDocumentRepository {
     async fn soft_delete_with_tx(
        &self,
        id: Uuid,
        auth: &AuthContext,
        tx: &mut Transaction<'_, Sqlite>,
    ) -> DomainResult<()> {
        let now = Utc::now().to_rfc3339();
        let now_dt = Utc::now(); // For logging
        let user_uuid = auth.user_id;
        let device_uuid: Option<Uuid> = auth.device_id.parse::<Uuid>().ok();

        let result = query(
            "UPDATE media_documents SET deleted_at = ?, deleted_by_user_id = ?, updated_at = ? WHERE id = ? AND deleted_at IS NULL"
        )
        .bind(&now)
        .bind(auth.user_id.to_string())
        .bind(&now)
        .bind(id.to_string())
        .execute(&mut **tx)
        .await
        .map_err(DbError::from)?;

        if result.rows_affected() == 0 {
            Err(DomainError::EntityNotFound(Self::entity_name().to_string(), id))
        } else {
            Ok(())
        }
    }
    async fn soft_delete(&self, id: Uuid, auth: &AuthContext) -> DomainResult<()> {
        let mut tx = self.pool.begin().await.map_err(DbError::from)?;
        let result = self.soft_delete_with_tx(id, auth, &mut tx).await;
        match result {
            Ok(_) => { tx.commit().await.map_err(DbError::from)?; Ok(()) },
            Err(e) => { let _ = tx.rollback().await; Err(e) },
        }
    }
}

#[async_trait]
impl HardDeletable for SqliteMediaDocumentRepository {
     fn entity_name(&self) -> &'static str {
        Self::entity_name()
    }
    async fn hard_delete_with_tx(
        &self,
        id: Uuid,
        _auth: &AuthContext,
        tx: &mut Transaction<'_, Sqlite>,
    ) -> DomainResult<()> {
         // Assumes ON DELETE CASCADE is set up for related logs/versions in DB schema
         let result = query("DELETE FROM media_documents WHERE id = ?")
            .bind(id.to_string())
            .execute(&mut **tx)
            .await
            .map_err(DbError::from)?;

        if result.rows_affected() == 0 {
            Err(DomainError::EntityNotFound(self.entity_name().to_string(), id))
        } else {
            Ok(())
        }
    }
    async fn hard_delete(&self, id: Uuid, auth: &AuthContext) -> DomainResult<()> {
        let mut tx = self.pool.begin().await.map_err(DbError::from)?;
        // Consider adding file system cleanup logic here or in the service calling this
        let result = self.hard_delete_with_tx(id, auth, &mut tx).await;
        match result {
            Ok(_) => { tx.commit().await.map_err(DbError::from)?; Ok(()) },
            Err(e) => { let _ = tx.rollback().await; Err(e) },
        }
    }
}

// --- MediaDocumentRepository implementation ---

#[async_trait]
impl MediaDocumentRepository for SqliteMediaDocumentRepository {
    async fn create(
        &self,
        new_doc: &NewMediaDocument,
        // file_path is NOT part of NewMediaDocument DTO. Assumed to be handled by service.
        // The file_path column in DB will be set by the service calling create or later update.
    ) -> DomainResult<MediaDocument> {
        let mut tx = self.pool.begin().await.map_err(DbError::from)?;
        let result = async {
            let now = Utc::now().to_rfc3339();
            let now_dt = Utc::now(); // For logging
            let user_uuid = new_doc.created_by_user_id;
            // NOTE: NewMediaDocument does not have device_id. Logging will use None.
            // If device_id logging is needed here, add it to NewMediaDocument DTO.
            let device_uuid: Option<Uuid> = None;

            // Use the user_uuid captured outside the async block for consistency
            let user_id_str_bind = user_uuid.map(|id| id.to_string());

            // Determine related_table and related_id based on temp_related_id
            let (actual_related_table, actual_related_id_str) = if new_doc.temp_related_id.is_some() {
                (TEMP_RELATED_TABLE.to_string(), None) // Store temp ID separately
            } else {
                // If temp_id is None, related_id MUST be Some (validated in DTO)
                (new_doc.related_table.clone(), new_doc.related_id.map(|id| id.to_string()))
            };
            let temp_related_id_str = new_doc.temp_related_id.map(|id| id.to_string());

            // REMOVED file_path and description from INSERT list and bind list
            // Assumes file_path will be set later, description column might not exist or is optional
            query(
                r#"INSERT INTO media_documents (
                    id, related_table, related_id, type_id,
                    original_filename, compressed_file_path, compressed_size_bytes,
                    field_identifier, title, mime_type, size_bytes,
                    compression_status, blob_key, blob_status, sync_priority,
                    temp_related_id,
                    created_at, updated_at, created_by_user_id, updated_by_user_id,
                    deleted_at, deleted_by_user_id,
                    file_path -- explicitly setting file_path to NULL initially
                ) VALUES (
                    ?, ?, ?, ?, -- id, related_table, related_id, type_id
                    ?, NULL, NULL, -- original_filename, compressed_file_path, compressed_size_bytes
                    ?, ?, ?, ?, -- field_identifier, title, mime_type, size_bytes
                    ?, NULL, ?, ?, -- compression_status, blob_key, blob_status, sync_priority
                    ?, -- temp_related_id
                    ?, ?, ?, ?, -- created_at, updated_at, created_by_user_id, updated_by_user_id
                    NULL, NULL, -- deleted_at, deleted_by_user_id
                    NULL -- file_path
                )"#
            )
            .bind(new_doc.id.to_string())
            .bind(actual_related_table) // Store actual or TEMP_RELATED_TABLE
            .bind(actual_related_id_str) // Store actual ID or NULL if temp
            .bind(new_doc.type_id.to_string())
            .bind(&new_doc.original_filename)
             // compressed fields initialized as NULL
            .bind(&new_doc.field_identifier)
            .bind(&new_doc.title)
            .bind(&new_doc.mime_type)
            .bind(new_doc.size_bytes)
            .bind(CompressionStatus::Pending.as_str()) // Default status
            .bind(BlobSyncStatus::Pending.as_str()) // Default status
            .bind(
                SyncPriority::from_str(&new_doc.sync_priority)
                    .map_err(|_| DomainError::Validation(ValidationError::custom(&format!("Invalid sync priority string: {}", new_doc.sync_priority))))?
                    .as_str() // Bind the string representation
            )
            .bind(temp_related_id_str) // Store temp ID if provided
            .bind(&now).bind(&now)
            .bind(user_id_str_bind.as_deref()).bind(user_id_str_bind.as_deref()) // Use Option<String> binding for both
            .execute(&mut *tx)
            .await
            .map_err(|e| {
                 if let sqlx::Error::Database(db_err) = &e {
                     if db_err.is_unique_violation() {
                         return DomainError::Database(DbError::Conflict(format!(
                             "MediaDocument with ID {} already exists.", new_doc.id
                         )));
                     }
                      // Check for foreign key violation on type_id
                     if db_err.message().contains("FOREIGN KEY constraint failed") {
                         // FIX: Use ValidationError::Custom instead of non-existent foreign_key variant
                         return DomainError::Validation(ValidationError::Custom(format!(
                             "Invalid document type ID ({}): Does not exist.", new_doc.type_id
                         )));
                     }
                 }
                 DomainError::Database(DbError::from(e))
             })?;

            // Log Create Operation
            let entry = ChangeLogEntry {
                operation_id: Uuid::new_v4(),
                entity_table: Self::entity_name().to_string(),
                entity_id: new_doc.id,
                operation_type: ChangeOperationType::Create,
                field_name: None,
                old_value: None,
                new_value: None, // Optionally serialize new_doc
                timestamp: now_dt,
                user_id: user_uuid.unwrap_or_else(Uuid::nil), // Provide default if None
                device_id: device_uuid,
                document_metadata: None,
                sync_batch_id: None,
                processed_at: None,
                sync_error: None,
            };
            self.log_change_entry(entry, &mut tx).await?;

            self.find_by_id_with_tx(new_doc.id, &mut tx).await
        }.await;

        match result {
            Ok(doc) => { tx.commit().await.map_err(DbError::from)?; Ok(doc) }
            Err(e) => { let _ = tx.rollback().await; Err(e) }
        }
    }

    async fn find_by_related_entity(
        &self,
        related_table: &str,
        related_id: Uuid,
        params: PaginationParams,
    ) -> DomainResult<PaginatedResult<MediaDocument>> {
        let offset = (params.page - 1) * params.per_page;
        let related_id_str = related_id.to_string();

        let count_query = query_scalar::<_, i64>(
            "SELECT COUNT(*) FROM media_documents WHERE related_table = ? AND related_id = ? AND deleted_at IS NULL"
        ).bind(related_table).bind(&related_id_str);

        let total: i64 = count_query.fetch_one(&self.pool).await.map_err(DbError::from)?;

        // Order by creation date, newest first
        let select_query = query_as::<_, MediaDocumentRow>(
            "SELECT * FROM media_documents WHERE related_table = ? AND related_id = ? AND deleted_at IS NULL ORDER BY created_at DESC LIMIT ? OFFSET ?"
        ).bind(related_table).bind(related_id_str).bind(params.per_page as i64).bind(offset as i64);

        let rows = select_query.fetch_all(&self.pool).await.map_err(DbError::from)?;
        let items = rows.into_iter().map(Self::map_row).collect::<DomainResult<Vec<_>>>()?;
        Ok(PaginatedResult::new(items, total as u64, params))
    }

    async fn update_compression_status(
        &self,
        id: Uuid,
        status: CompressionStatus,
        compressed_file_path: Option<&str>,
        compressed_size_bytes: Option<i64>, // ADDED size
    ) -> DomainResult<()> {
        let mut tx = self.pool.begin().await.map_err(DbError::from)?;
        let old_entity = self.find_by_id_with_tx(id, &mut tx).await.ok(); // Ignore error if not found for logging
        let now = Utc::now();
        let system_user_id = Uuid::nil(); // System operation
        let device_uuid: Option<Uuid> = None;

        let result = query(
            "UPDATE media_documents SET compression_status = ?, compressed_file_path = ?, compressed_size_bytes = ?, updated_at = ? WHERE id = ?"
        )
        .bind(status.as_str())
        .bind(compressed_file_path)
        .bind(compressed_size_bytes) // Bind the size
        .bind(now.to_rfc3339())
        .bind(id.to_string())
        .execute(&mut *tx) // Use transaction
        .await
        .map_err(DbError::from)?;

        if result.rows_affected() == 0 {
            let _ = tx.rollback().await;
            Err(DomainError::EntityNotFound(Self::entity_name().to_string(), id))
        } else {
            // Log changes if old entity was found
            if let Some(old) = old_entity {
                // Fetch new state within the transaction
                let new_entity = self.find_by_id_with_tx(id, &mut tx).await?;

                macro_rules! log_if_changed {
                    ($field_name:ident, $field_sql:literal) => {
                        if old.$field_name != new_entity.$field_name {
                            let entry = ChangeLogEntry {
                                operation_id: Uuid::new_v4(),
                                entity_table: Self::entity_name().to_string(),
                                entity_id: id,
                                operation_type: ChangeOperationType::Update,
                                field_name: Some($field_sql.to_string()),
                                old_value: serde_json::to_string(&old.$field_name).ok(),
                                new_value: serde_json::to_string(&new_entity.$field_name).ok(),
                                timestamp: now,
                                user_id: system_user_id,
                                device_id: device_uuid.clone(),
                                document_metadata: None,
                                sync_batch_id: None,
                                processed_at: None,
                                sync_error: None,
                            };
                            self.log_change_entry(entry, &mut tx).await?;
                        }
                    };
                }
                log_if_changed!(compression_status, "compression_status");
                log_if_changed!(compressed_file_path, "compressed_file_path");
                log_if_changed!(compressed_size_bytes, "compressed_size_bytes");
            }
            tx.commit().await.map_err(DbError::from)?;
            Ok(())
        }
    }

    async fn update_blob_sync_status(
        &self,
        id: Uuid,
        status: BlobSyncStatus,
        blob_key: Option<&str>,
    ) -> DomainResult<()> {
        let mut tx = self.pool.begin().await.map_err(DbError::from)?;
        let old_entity = self.find_by_id_with_tx(id, &mut tx).await.ok();
        let now = Utc::now();
        let system_user_id = Uuid::nil(); // System operation
        let device_uuid: Option<Uuid> = None;

        let result = query(
            "UPDATE media_documents SET blob_status = ?, blob_key = ?, updated_at = ? WHERE id = ?"
        )
        .bind(status.as_str())
        .bind(blob_key)
        .bind(now.to_rfc3339())
        .bind(id.to_string())
        .execute(&mut *tx)
        .await
        .map_err(DbError::from)?;

        if result.rows_affected() == 0 {
            let _ = tx.rollback().await;
            Err(DomainError::EntityNotFound(Self::entity_name().to_string(), id))
        } else {
            // Log changes if old entity was found
            if let Some(old) = old_entity {
                let new_entity = self.find_by_id_with_tx(id, &mut tx).await?;

                macro_rules! log_if_changed {
                    ($field_name:ident, $field_sql:literal) => {
                        if old.$field_name != new_entity.$field_name {
                            let entry = ChangeLogEntry {
                                operation_id: Uuid::new_v4(),
                                entity_table: Self::entity_name().to_string(),
                                entity_id: id,
                                operation_type: ChangeOperationType::Update,
                                field_name: Some($field_sql.to_string()),
                                old_value: serde_json::to_string(&old.$field_name).ok(),
                                new_value: serde_json::to_string(&new_entity.$field_name).ok(),
                                timestamp: now,
                                user_id: system_user_id,
                                device_id: device_uuid.clone(),
                                document_metadata: None,
                                sync_batch_id: None,
                                processed_at: None,
                                sync_error: None,
                            };
                            self.log_change_entry(entry, &mut tx).await?;
                        }
                    };
                }
                log_if_changed!(blob_status, "blob_status");
                log_if_changed!(blob_key, "blob_key");
            }
            tx.commit().await.map_err(DbError::from)?;
            Ok(())
        }
    }

    async fn update_sync_priority(
        &self,
        ids: &[Uuid],
        priority: SyncPriority,
        auth: &AuthContext, // Keep auth context for tracking who updated
    ) -> DomainResult<u64> {
        if ids.is_empty() {
            return Ok(0);
        }

        let mut tx = self.pool.begin().await.map_err(DbError::from)?;

        // --- Fetch Old Priorities ---
        let id_strings: Vec<String> = ids.iter().map(Uuid::to_string).collect();
        let select_query = format!(
            "SELECT id, sync_priority FROM media_documents WHERE id IN ({})",
            vec!["?"; ids.len()].join(", ")
        );
        let mut select_builder = query_as::<_, (String, String)>(&select_query); // Expect TEXT
        for id_str in &id_strings {
            select_builder = select_builder.bind(id_str);
        }
        let old_priorities: std::collections::HashMap<Uuid, SyncPriority> = select_builder
            .fetch_all(&mut *tx) // Use the transaction
            .await.map_err(DbError::from)?
            .into_iter()
            .filter_map(|(id_str, prio_text_opt)| { // Changed to prio_text_opt: Option<String> or String
                // Assuming sync_priority is NOT NULL in DB and TEXT
                match Uuid::parse_str(&id_str) {
                     Ok(id) => Some((id, SyncPriority::from_str(&prio_text_opt).unwrap_or(SyncPriority::Normal))),
                     Err(_) => None,
                }
            }).collect();

        let now = Utc::now().to_rfc3339();
        let now_dt = Utc::now(); // For logging
        let user_id_str = auth.user_id.to_string();
        let user_uuid = auth.user_id;
        let device_uuid: Option<Uuid> = auth.device_id.parse::<Uuid>().ok();
        let priority_str = priority.as_str(); // Store as string

        let placeholders = ids.iter().map(|_| "?").collect::<Vec<_>>().join(",");
        let query_str = format!(
            "UPDATE media_documents
             SET sync_priority = ?, updated_at = ?, updated_by_user_id = ?
             WHERE id IN ({}) AND deleted_at IS NULL",
            placeholders
        );

        let mut query_builder = sqlx::query(&query_str)
            .bind(priority_str) // Bind string
            .bind(now)
            .bind(user_id_str);

        for id in ids {
            query_builder = query_builder.bind(id.to_string());
        }

        let result = query_builder.execute(&mut *tx).await.map_err(DbError::from)?;
        let rows_affected = result.rows_affected();

        // --- Log Changes ---
        for id in ids {
            if let Some(old_priority) = old_priorities.get(id) {
                if *old_priority != priority { // Compare SyncPriority enums
                    let entry = ChangeLogEntry {
                        operation_id: Uuid::new_v4(),
                        entity_table: Self::entity_name().to_string(),
                        entity_id: *id,
                        operation_type: ChangeOperationType::Update,
                        field_name: Some("sync_priority".to_string()),
                        old_value: serde_json::to_string(old_priority.as_str()).ok(), // Log old priority as string
                        new_value: serde_json::to_string(priority_str).ok(), // Log new priority as string
                        timestamp: now_dt,
                        user_id: user_uuid,
                        device_id: device_uuid.clone(),
                        document_metadata: None,
                        sync_batch_id: None,
                        processed_at: None,
                        sync_error: None,
                    };
                    self.log_change_entry(entry, &mut tx).await?;
                }
            }
        }

        tx.commit().await.map_err(DbError::from)?;
        Ok(rows_affected)
    }

    /// Internal method to find by ID within a transaction
    async fn find_by_id_with_tx<'t>(
        &self,
        id: Uuid,
        tx: &mut Transaction<'t, Sqlite>,
    ) -> DomainResult<MediaDocument> {
        query_as::<_, MediaDocumentRow>("SELECT * FROM media_documents WHERE id = ? AND deleted_at IS NULL")
            .bind(id.to_string())
            .fetch_optional(&mut **tx) // Use &mut **tx
            .await
            .map_err(DbError::from)?
            .ok_or_else(|| DomainError::EntityNotFound(Self::entity_name().to_string(), id))
            .and_then(Self::map_row)
    }

    // Re-implement find_by_id to satisfy the trait bound, delegating to the public FindById impl
    async fn find_by_id(&self, id: Uuid) -> DomainResult<MediaDocument> {
        <Self as FindById<MediaDocument>>::find_by_id(self, id).await
    }

     /// Links documents created with a temporary ID to the actual entity ID.
     /// Implementation that delegates to link_temp_documents_with_tx
    async fn link_temp_documents(
        &self,
        temp_related_id: Uuid,
        final_related_table: &str,
        final_related_id: Uuid,
        tx: &mut Transaction<'_, Sqlite>,
    ) -> DomainResult<u64> {
        self.link_temp_documents_with_tx(
            temp_related_id,
            final_related_table,
            final_related_id,
            tx
        ).await
    }
    
    /// Implementation of the actual linking logic
    async fn link_temp_documents_with_tx(
        &self,
        temp_related_id: Uuid,
        final_related_table: &str,
        final_related_id: Uuid,
        tx: &mut Transaction<'_, Sqlite>,
    ) -> DomainResult<u64> {
        let now_dt = Utc::now(); // For logging
        // Assume this is called by a system user or within a context without specific user/device
        let system_user_id = Uuid::nil(); // Placeholder system user ID
        let device_uuid: Option<Uuid> = None;

        // --- Fetch IDs and old paths/state BEFORE updating ---
        let temp_id_str_fetch = temp_related_id.to_string();
        struct OldDocState {
            id: Uuid,
            file_path: Option<String>,
            compressed_file_path: Option<String>,
        }
        let old_docs = query_as::<_, (String, Option<String>, Option<String>)>(r#"
            SELECT id, file_path, compressed_file_path
            FROM media_documents
            WHERE temp_related_id = ? AND related_table = ?
            "#)
            .bind(&temp_id_str_fetch)
            .bind(TEMP_RELATED_TABLE)
            .fetch_all(&mut **tx)
            .await
            .map_err(DbError::from)?
            .into_iter()
            .filter_map(|(id_str, fp, cfp)| {
                Uuid::parse_str(&id_str).ok().map(|id| OldDocState {
                    id,
                    file_path: fp,
                    compressed_file_path: cfp,
                })
            })
            .collect::<Vec<_>>();

        if old_docs.is_empty() {
            return Ok(0); // No documents to link or log
        }

        // First query to count how many documents will be updated
        let temp_id_str_count = temp_related_id.to_string(); // Store in variable
        let count = sqlx::query!(
            r#"
            SELECT COUNT(*) as count
            FROM media_documents
            WHERE temp_related_id = ? AND related_table = ?
            "#,
            temp_id_str_count, // Use variable
            TEMP_RELATED_TABLE
        )
        .fetch_one(&mut **tx)
        .await
        .map_err(|e| {
            DomainError::Database(DbError::from(e)) // Simplified error mapping
        })?
        .count as u64;

        if count == 0 {
            return Ok(0); // No documents to update
        }

        // Update the documents to point to the final entity
        let final_id_str_update = final_related_id.to_string(); // Store in variable
        let now_str_update = Utc::now().to_rfc3339();        // Store in variable
        let temp_id_str_update = temp_related_id.to_string();   // Store in variable
        let rows_affected = sqlx::query!(
            r#"
            UPDATE media_documents
            SET related_id = ?, 
                related_table = ?, 
                temp_related_id = NULL,
                updated_at = ? -- Also update timestamp
            WHERE temp_related_id = ? AND related_table = ?
            "#,
            final_id_str_update,  // Use variable
            final_related_table,
            now_str_update,       // Use variable
            temp_id_str_update,   // Use variable
            TEMP_RELATED_TABLE
        )
        .execute(&mut **tx)
        .await
        .map_err(|e| {
            DomainError::Database(DbError::from(e)) // Simplified error mapping
        })?
        .rows_affected() as u64;

        // --- Log Field Changes ---
        // Fetch new paths after update
        let ids_to_fetch: Vec<String> = old_docs.iter().map(|d| d.id.to_string()).collect();
        let query_fetch_new_paths = format!(
            "SELECT id, file_path, compressed_file_path FROM media_documents WHERE id IN ({})",
            vec!["?"; ids_to_fetch.len()].join(", ")
        );
        let mut new_paths_builder = query_as::<_, (String, Option<String>, Option<String>)>(&query_fetch_new_paths);
        for id_str in &ids_to_fetch {
            new_paths_builder = new_paths_builder.bind(id_str);
        }
        let new_paths_map: HashMap<Uuid, (Option<String>, Option<String>)> = new_paths_builder
            .fetch_all(&mut **tx)
            .await
            .map_err(DbError::from)?
            .into_iter()
            .filter_map(|(id_str, fp, cfp)| Uuid::parse_str(&id_str).ok().map(|id| (id, (fp, cfp))))
            .collect();

        for old_doc in old_docs {
            let new_paths = new_paths_map.get(&old_doc.id);
            let new_file_path = new_paths.and_then(|(fp, _)| fp.clone());
            let new_compressed_path = new_paths.and_then(|(_, cfp)| cfp.clone());

            // Log related_id change
            self.log_change_entry(ChangeLogEntry {
                operation_id: Uuid::new_v4(),
                entity_table: Self::entity_name().to_string(),
                entity_id: old_doc.id,
                operation_type: ChangeOperationType::Update,
                field_name: Some("related_id".to_string()),
                old_value: serde_json::to_string(&Option::<Uuid>::None).ok(), // Was implicitly null when temp_related_id was set
                new_value: serde_json::to_string(&Some(final_related_id)).ok(),
                timestamp: now_dt,
                user_id: system_user_id,
                device_id: device_uuid.clone(),
                document_metadata: None,
                sync_batch_id: None,
                processed_at: None,
                sync_error: None,
            }, tx).await?;

            // Log related_table change
            self.log_change_entry(ChangeLogEntry {
                operation_id: Uuid::new_v4(),
                entity_table: Self::entity_name().to_string(),
                entity_id: old_doc.id,
                operation_type: ChangeOperationType::Update,
                field_name: Some("related_table".to_string()),
                old_value: Some(TEMP_RELATED_TABLE.to_string()),
                new_value: Some(final_related_table.to_string()),
                timestamp: now_dt,
                user_id: system_user_id,
                device_id: device_uuid.clone(),
                document_metadata: None,
                sync_batch_id: None,
                processed_at: None,
                sync_error: None,
            }, tx).await?;

            // Log temp_related_id change (becoming NULL)
            self.log_change_entry(ChangeLogEntry {
                operation_id: Uuid::new_v4(),
                entity_table: Self::entity_name().to_string(),
                entity_id: old_doc.id,
                operation_type: ChangeOperationType::Update,
                field_name: Some("temp_related_id".to_string()),
                old_value: serde_json::to_string(&Some(temp_related_id)).ok(),
                new_value: serde_json::to_string(&Option::<Uuid>::None).ok(),
                timestamp: now_dt,
                user_id: system_user_id,
                device_id: device_uuid.clone(),
                document_metadata: None,
                sync_batch_id: None,
                processed_at: None,
                sync_error: None,
            }, tx).await?;

            // Log file_path change if it changed
            if old_doc.file_path != new_file_path {
                self.log_change_entry(ChangeLogEntry {
                    operation_id: Uuid::new_v4(),
                    entity_table: Self::entity_name().to_string(),
                    entity_id: old_doc.id,
                    operation_type: ChangeOperationType::Update,
                    field_name: Some("file_path".to_string()),
                    old_value: serde_json::to_string(&old_doc.file_path).ok(),
                    new_value: serde_json::to_string(&new_file_path).ok(),
                    timestamp: now_dt,
                    user_id: system_user_id,
                    device_id: device_uuid.clone(),
                    document_metadata: None,
                    sync_batch_id: None,
                    processed_at: None,
                    sync_error: None,
                }, tx).await?;
            }

            // Log compressed_file_path change if it changed
            if old_doc.compressed_file_path != new_compressed_path {
                self.log_change_entry(ChangeLogEntry {
                    operation_id: Uuid::new_v4(),
                    entity_table: Self::entity_name().to_string(),
                    entity_id: old_doc.id,
                    operation_type: ChangeOperationType::Update,
                    field_name: Some("compressed_file_path".to_string()),
                    old_value: serde_json::to_string(&old_doc.compressed_file_path).ok(),
                    new_value: serde_json::to_string(&new_compressed_path).ok(),
                    timestamp: now_dt,
                    user_id: system_user_id,
                    device_id: device_uuid.clone(),
                    document_metadata: None,
                    sync_batch_id: None,
                    processed_at: None,
                    sync_error: None,
                }, tx).await?;
            }
        }

        Ok(rows_affected) // Return rows affected by the main linking update
    }

     /// Updates paths and status, typically after a sync download.
     async fn update_paths_and_status(
         &self,
         document_id: Uuid,
         file_path: Option<&str>, // New original path? Unlikely use case?
         compressed_file_path: Option<&str>,
         compressed_size_bytes: Option<i64>,
         compression_status: Option<CompressionStatus>,
     ) -> DomainResult<()> {
          let mut tx = self.pool.begin().await.map_err(DbError::from)?;

          // --- Fetch Old State ---
          let old_entity = match self.find_by_id_with_tx(document_id, &mut tx).await {
             Ok(entity) => entity,
             Err(e) => {
                 let _ = tx.rollback().await;
                 return Err(e);
             }
         };

         let mut sets: Vec<String> = Vec::new();
         let mut binds: Vec<String> = Vec::new(); // Using String for simplicity
         let mut binds_i64: HashMap<String, i64> = HashMap::new(); // For integer binds

         let now = Utc::now(); // For logging and timestamp
         // Assuming system operation - no specific user/device ID
         let system_user_id = Uuid::nil();
         let device_uuid: Option<Uuid> = None;

         // Use macro to simplify adding updates
         macro_rules! add_update {
            ($field:ident, $value:expr, $bind_vec:ident) => {
               if let Some(val) = $value {
                   sets.push(format!("{} = ?", stringify!($field)));
                   $bind_vec.push(val.to_string());
               }
            };
             ($field:ident, $value:expr, $bind_map:ident, $type:ty) => {
                if let Some(val) = $value {
                    sets.push(format!("{} = ?", stringify!($field)));
                    $bind_map.insert(stringify!($field).to_string(), val as $type);
                }
            };
         }

        // Add fields to update
        add_update!(file_path, file_path, binds);
        add_update!(compressed_file_path, compressed_file_path, binds);
        add_update!(compressed_size_bytes, compressed_size_bytes, binds_i64, i64);
        if let Some(status) = compression_status {
            sets.push("compression_status = ?".to_string());
            binds.push(status.as_str().to_string());
        }


        if sets.is_empty() {
            return Ok(()); // Nothing to update
        }

        // Always update updated_at
        sets.push("updated_at = ?".to_string());
        binds.push(now.to_rfc3339()); // Use captured 'now'
        // Optionally update updated_by_user_id if tracking sync agent ID
        // sets.push("updated_by_user_id = ?".to_string());
        // binds.push(SYSTEM_USER_ID.to_string());

        let query_str = format!("UPDATE media_documents SET {} WHERE id = ?", sets.join(", "));

        // Build and execute the query
        let mut q = query(&query_str);
        for bind_val in binds { // Bind strings first
            q = q.bind(bind_val);
        }
        // Bind integers (order matters if placeholders are mixed)
        // Assuming integer placeholders come after string ones based on SET order
        if let Some(val) = binds_i64.get("compressed_size_bytes") {
            q = q.bind(val);
        }
        // Add binds for other i64 fields if needed

        q = q.bind(document_id.to_string()); // Bind the WHERE clause ID

        let result = q.execute(&mut *tx).await.map_err(DbError::from)?;

        if result.rows_affected() == 0 {
            let _ = tx.rollback().await;
            Err(DomainError::EntityNotFound(Self::entity_name().to_string(), document_id))
        } else {
            // --- Log Field Changes ---
            let new_entity = self.find_by_id_with_tx(document_id, &mut tx).await?;

            macro_rules! log_if_changed {
                ($field_name:ident, $field_sql:literal) => {
                    if old_entity.$field_name != new_entity.$field_name {
                        let entry = ChangeLogEntry {
                            operation_id: Uuid::new_v4(),
                            entity_table: Self::entity_name().to_string(),
                            entity_id: document_id,
                            operation_type: ChangeOperationType::Update,
                            field_name: Some($field_sql.to_string()),
                            old_value: serde_json::to_string(&old_entity.$field_name).ok(),
                            new_value: serde_json::to_string(&new_entity.$field_name).ok(),
                            timestamp: now,
                            user_id: system_user_id,
                            device_id: device_uuid.clone(),
                            document_metadata: None,
                            sync_batch_id: None,
                            processed_at: None,
                            sync_error: None,
                        };
                        self.log_change_entry(entry, &mut tx).await?;
                    }
                };
            }

            log_if_changed!(file_path, "file_path");
            log_if_changed!(compressed_file_path, "compressed_file_path");
            log_if_changed!(compressed_size_bytes, "compressed_size_bytes");
            log_if_changed!(compression_status, "compression_status");

            tx.commit().await.map_err(DbError::from)?;
            Ok(())
        }
    }
}


// --- Document Version Repository --- (Assumed schema matches types)

#[async_trait]
pub trait DocumentVersionRepository: Send + Sync {
    async fn create(
        &self,
        doc_id: Uuid,
        version_number: i64,
        file_path: &str,
        file_size: i64,
        mime_type: &str,
        blob_key: Option<&str>,
        auth: &AuthContext, // Assuming versions are created by user actions
    ) -> DomainResult<DocumentVersion>;

    async fn find_by_document_id(&self, doc_id: Uuid) -> DomainResult<Vec<DocumentVersion>>;
}

pub struct SqliteDocumentVersionRepository {
    pool: Pool<Sqlite>,
    change_log_repo: Arc<dyn ChangeLogRepository + Send + Sync>,
}

impl SqliteDocumentVersionRepository {
    pub fn new(pool: Pool<Sqlite>, change_log_repo: Arc<dyn ChangeLogRepository + Send + Sync>) -> Self {
        Self { pool, change_log_repo }
    }
    fn map_row(row: DocumentVersionRow) -> DomainResult<DocumentVersion> {
        row.into_entity()
    }
}

#[async_trait]
impl DocumentVersionRepository for SqliteDocumentVersionRepository {
    async fn create(
        &self,
        doc_id: Uuid,
        version_number: i64,
        file_path: &str,
        file_size: i64,
        mime_type: &str,
        blob_key: Option<&str>,
        auth: &AuthContext,
    ) -> DomainResult<DocumentVersion> {
        let id = Uuid::new_v4();
        let now = Utc::now().to_rfc3339();
        let now_dt = Utc::now(); // For logging
        let user_id_str = auth.user_id.to_string();
        let user_uuid = auth.user_id;
        let device_uuid: Option<Uuid> = auth.device_id.parse::<Uuid>().ok();

        let mut tx = self.pool.begin().await.map_err(DbError::from)?;

        let result = async {
            query(
                r#"INSERT INTO document_versions (
                    id, document_id, version_number, file_path, file_size, mime_type, blob_storage_key,
                    created_at, created_by_user_id
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)"#
            )
            .bind(id.to_string())
            .bind(doc_id.to_string())
            .bind(version_number)
            .bind(file_path)
            .bind(file_size)
            .bind(mime_type)
            .bind(blob_key)
            .bind(&now)
            .bind(user_id_str) // Use user ID from AuthContext
            .execute(&mut *tx)
            .await
            .map_err(DbError::from)?;

            // Log Create Operation within the transaction
            let entry = ChangeLogEntry {
                operation_id: Uuid::new_v4(),
                entity_table: "document_versions".to_string(),
                entity_id: id,
                operation_type: ChangeOperationType::Create,
                field_name: None,
                old_value: None,
                new_value: None,
                timestamp: now_dt,
                user_id: user_uuid,
                device_id: device_uuid,
                document_metadata: None,
                sync_batch_id: None,
                processed_at: None,
                sync_error: None,
            };
            self.change_log_repo.create_change_log_with_tx(&entry, &mut tx).await?;

            Ok(id) // Return ID to fetch after commit
        }.await;

        match result {
            Ok(created_id) => {
                tx.commit().await.map_err(DbError::from)?;
                // Fetch the newly created record outside the transaction
                let row = query_as::<_, DocumentVersionRow>("SELECT * FROM document_versions WHERE id = ?")
                   .bind(created_id.to_string()).fetch_one(&self.pool).await.map_err(DbError::from)?;
                Self::map_row(row)
            },
            Err(e) => {
                let _ = tx.rollback().await;
                Err(e)
            }
        }
    }

    async fn find_by_document_id(&self, doc_id: Uuid) -> DomainResult<Vec<DocumentVersion>> {
        let rows = query_as::<_, DocumentVersionRow>(
            "SELECT * FROM document_versions WHERE document_id = ? ORDER BY version_number DESC"
        )
        .bind(doc_id.to_string())
        .fetch_all(&self.pool)
        .await
        .map_err(DbError::from)?;
        rows.into_iter().map(Self::map_row).collect()
    }
}

// --- Document Access Log Repository --- (Assumed schema matches types)

#[async_trait]
pub trait DocumentAccessLogRepository: Send + Sync {
     async fn create(&self, new_log: &NewDocumentAccessLog) -> DomainResult<DocumentAccessLog>;
     async fn find_by_document_id(&self, doc_id: Uuid, params: PaginationParams) -> DomainResult<PaginatedResult<DocumentAccessLog>>;
}

pub struct SqliteDocumentAccessLogRepository {
    pool: Pool<Sqlite>
}

impl SqliteDocumentAccessLogRepository {
    pub fn new(pool: Pool<Sqlite>) -> Self {
        Self { pool }
    }
    fn map_row(row: DocumentAccessLogRow) -> DomainResult<DocumentAccessLog> {
        row.into_entity()
    }
}

#[async_trait]
impl DocumentAccessLogRepository for SqliteDocumentAccessLogRepository {
    async fn create(&self, new_log: &NewDocumentAccessLog) -> DomainResult<DocumentAccessLog> {
        let id = Uuid::new_v4();
        let now = Utc::now().to_rfc3339();

        query(
            "INSERT INTO document_access_logs (id, document_id, user_id, access_type, access_date, details) VALUES (?, ?, ?, ?, ?, ?)"
        )
        .bind(id.to_string())
        .bind(new_log.document_id.to_string())
        .bind(new_log.user_id.to_string()) // Use provided user_id (could be system user)
        .bind(&new_log.access_type) // Assumes access_type is already validated string
        .bind(&now)
        .bind(&new_log.details)
        .execute(&self.pool)
        .await
        .map_err(DbError::from)?;

        // Fetch the created record
        let row = query_as::<_, DocumentAccessLogRow>("SELECT * FROM document_access_logs WHERE id = ?")
            .bind(id.to_string()).fetch_one(&self.pool).await.map_err(DbError::from)?;
        Self::map_row(row)
    }

     async fn find_by_document_id(&self, doc_id: Uuid, params: PaginationParams) -> DomainResult<PaginatedResult<DocumentAccessLog>> {
        let offset = (params.page - 1) * params.per_page;
        let doc_id_str = doc_id.to_string();

        let count_query = query_scalar::<_, i64>(
            "SELECT COUNT(*) FROM document_access_logs WHERE document_id = ?"
        ).bind(&doc_id_str);
        let total: i64 = count_query.fetch_one(&self.pool).await.map_err(DbError::from)?;

        let select_query = query_as::<_, DocumentAccessLogRow>(
            "SELECT * FROM document_access_logs WHERE document_id = ? ORDER BY access_date DESC LIMIT ? OFFSET ?"
        ).bind(doc_id_str).bind(params.per_page as i64).bind(offset as i64);

        let rows = select_query.fetch_all(&self.pool).await.map_err(DbError::from)?;
        let items = rows.into_iter().map(Self::map_row).collect::<DomainResult<Vec<_>>>()?;
        Ok(PaginatedResult::new(items, total as u64, params))
    }
}

// /Users/sagarshrestha/ipad_rust_core copy/src/domains/permission/has_permission.rs
// Adjust imports based on your actual error structure
use serde::{Deserialize, Serialize};
// Required for AuditLogger example

// --- User Role Definition ---

/// UserRole enum for authorization in the application
#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
pub enum UserRole {
    Admin,
    FieldTeamLead,
    FieldOfficer,
}

// --- Permission Enum Definition (Combined) ---

/// Permission enum representing individual permissions in the system
#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash, Serialize, Deserialize)]
pub enum Permission {
    // User management
    ManageUsers,

    // Strategic Goal permissions
    ViewStrategicGoals,
    EditStrategicGoals,
    CreateStrategicGoals,
    DeleteStrategicGoals,
    
    // Project permissions
    ViewProjects,
    EditProjects,
    CreateProjects,
    DeleteProjects,

    // Participant permissions
    ViewParticipants,
    EditParticipants,
    CreateParticipants,
    DeleteParticipants,

    // Workshop permissions
    ViewWorkshops,
    EditWorkshops,
    CreateWorkshops,
    DeleteWorkshops,

    // Activity permissions
    ViewActivities,
    EditActivities,
    CreateActivities,
    DeleteActivities,

    // Livelihood permissions
    ViewLivelihoods,
    EditLivelihoods,
    CreateLivelihoods,
    DeleteLivelihoods,

    // Document permissions
    ViewDocuments,
    EditDocuments,
    UploadDocuments,
    DeleteDocuments,

    // Donor permissions
    ViewDonors,
    CreateDonors,
    EditDonors,
    DeleteDonors,
    
    // Funding permissions (NEW)
    ViewFunding,
    CreateFunding,
    EditFunding,
    DeleteFunding,

    // System permissions
    ViewAuditLogs,
    ConfigureSystem,
    ExportData,
    ImportData,

    // Sync permissions
    SyncData,
    ConfigurePersonalSync,
    ViewSyncStatus,
    ManageGlobalSync,

    // Special permissions
    DeleteRecord,
    HardDeleteRecord,
    HardDeleteRecordWithDependencies,
}

// --- UserRole Implementation (Using Latest Logic) ---

impl UserRole {
    pub fn as_str(&self) -> &'static str {
        match self {
            UserRole::Admin => "admin",
            UserRole::FieldTeamLead => "field_tl",
            UserRole::FieldOfficer => "field",
        }
    }

    pub fn from_str(s: &str) -> Option<Self> {
        match s {
            "admin" => Some(UserRole::Admin),
            "field_tl" => Some(UserRole::FieldTeamLead),
            "field" => Some(UserRole::FieldOfficer),
            _ => None,
        }
    }

    /// Check if the user has a specific permission based on the updated logic
    pub fn has_permission(&self, permission: Permission) -> bool {
        match self {
            UserRole::Admin => true, // Admin has all permissions
            UserRole::FieldTeamLead => {
                match permission {
                    // Admin-only permissions - deny FieldTeamLead
                    Permission::ManageUsers
                    | Permission::ViewAuditLogs
                    | Permission::ConfigureSystem
                    | Permission::HardDeleteRecord
                    | Permission::HardDeleteRecordWithDependencies
                    | Permission::ManageGlobalSync
                    // Deny Donor and Funding Management
                    | Permission::ViewDonors
                    | Permission::CreateDonors
                    | Permission::EditDonors
                    | Permission::DeleteDonors
                    | Permission::ViewFunding
                    | Permission::CreateFunding
                    | Permission::EditFunding
                    | Permission::DeleteFunding => false,

                    // Personal sync config & basic sync allowed
                    Permission::SyncData
                    | Permission::ConfigurePersonalSync
                    | Permission::ViewSyncStatus => true,

                    // Everything else is assumed allowed for FieldTeamLead (including StrategicGoals)
                    _ => true,
                }
            }
            UserRole::FieldOfficer => {
                match permission {
                    // Personal sync config & basic sync allowed
                    Permission::SyncData
                    | Permission::ConfigurePersonalSync
                    | Permission::ViewSyncStatus => true,
                    
                    // Deny Strategic Goal modification for FieldOfficer by default?
                    Permission::EditStrategicGoals 
                    | Permission::CreateStrategicGoals 
                    | Permission::DeleteStrategicGoals => false, 

                    // Basic CRUD for most operational entities - allow FieldOfficer
                    Permission::ViewStrategicGoals // Allow viewing strategic goals
                    | Permission::ViewProjects | Permission::EditProjects
                    | Permission::CreateProjects | Permission::DeleteProjects
                    | Permission::ViewParticipants | Permission::EditParticipants
                    | Permission::CreateParticipants | Permission::DeleteParticipants
                    | Permission::ViewWorkshops | Permission::EditWorkshops
                    | Permission::CreateWorkshops | Permission::DeleteWorkshops
                    | Permission::ViewActivities | Permission::EditActivities
                    | Permission::CreateActivities | Permission::DeleteActivities
                    | Permission::ViewLivelihoods | Permission::EditLivelihoods
                    | Permission::CreateLivelihoods | Permission::DeleteLivelihoods
                    | Permission::ViewDocuments | Permission::EditDocuments
                    | Permission::UploadDocuments | Permission::DeleteDocuments
                    | Permission::DeleteRecord => true, // General soft delete

                    // Access to donor data, funding data, and admin functions - deny FieldOfficer
                    Permission::ViewDonors | Permission::CreateDonors
                    | Permission::EditDonors | Permission::DeleteDonors
                    | Permission::ViewFunding | Permission::CreateFunding
                    | Permission::EditFunding | Permission::DeleteFunding
                    | Permission::ManageUsers
                    | Permission::ViewAuditLogs
                    | Permission::ConfigureSystem
                    | Permission::ExportData
                    | Permission::ImportData
                    | Permission::HardDeleteRecord
                    | Permission::HardDeleteRecordWithDependencies
                    | Permission::ManageGlobalSync => false,
                }
            }
        }
    }

    /// Check if the user has all of the specified permissions
    pub fn has_permissions(&self, permissions: &[Permission]) -> bool {
        permissions.iter().all(|p| self.has_permission(*p))
    }

    /// Check if this role can perform any form of hard delete
    pub fn can_hard_delete(&self) -> bool {
        self.has_permission(Permission::HardDeleteRecord) ||
        self.has_permission(Permission::HardDeleteRecordWithDependencies)
    }
}


// --- Permission Implementation (String Conversions & Listing) ---

impl Permission {
    pub fn as_str(&self) -> &'static str {
        match self {
            // User management
            Permission::ManageUsers => "manage_users",
            // Strategic Goal permissions
            Permission::ViewStrategicGoals => "view_strategic_goals",
            Permission::EditStrategicGoals => "edit_strategic_goals",
            Permission::CreateStrategicGoals => "create_strategic_goals",
            Permission::DeleteStrategicGoals => "delete_strategic_goals",
            // Project permissions
            Permission::ViewProjects => "view_projects",
            Permission::EditProjects => "edit_projects",
            Permission::CreateProjects => "create_projects",
            Permission::DeleteProjects => "delete_projects",
            // Participant permissions
            Permission::ViewParticipants => "view_participants",
            Permission::EditParticipants => "edit_participants",
            Permission::CreateParticipants => "create_participants",
            Permission::DeleteParticipants => "delete_participants",
            // Workshop permissions
            Permission::ViewWorkshops => "view_workshops",
            Permission::EditWorkshops => "edit_workshops",
            Permission::CreateWorkshops => "create_workshops",
            Permission::DeleteWorkshops => "delete_workshops",
            // Activity permissions
            Permission::ViewActivities => "view_activities",
            Permission::EditActivities => "edit_activities",
            Permission::CreateActivities => "create_activities",
            Permission::DeleteActivities => "delete_activities",
            // Livelihood permissions
            Permission::ViewLivelihoods => "view_livelihoods",
            Permission::EditLivelihoods => "edit_livelihoods",
            Permission::CreateLivelihoods => "create_livelihoods",
            Permission::DeleteLivelihoods => "delete_livelihoods",
            // Document permissions
            Permission::ViewDocuments => "view_documents",
            Permission::EditDocuments => "edit_documents",
            Permission::UploadDocuments => "upload_documents",
            Permission::DeleteDocuments => "delete_documents",
            // Donor permissions
            Permission::ViewDonors => "view_donors",
            Permission::CreateDonors => "create_donors",
            Permission::EditDonors => "edit_donors",
            Permission::DeleteDonors => "delete_donors",
            // Funding permissions
            Permission::ViewFunding => "view_funding",
            Permission::CreateFunding => "create_funding",
            Permission::EditFunding => "edit_funding",
            Permission::DeleteFunding => "delete_funding",
            // System permissions
            Permission::ViewAuditLogs => "view_audit_logs",
            Permission::ConfigureSystem => "configure_system",
            Permission::ExportData => "export_data",
            Permission::ImportData => "import_data",
            // Sync permissions
            Permission::SyncData => "sync_data",
            Permission::ConfigurePersonalSync => "configure_personal_sync",
            Permission::ViewSyncStatus => "view_sync_status",
            Permission::ManageGlobalSync => "manage_global_sync",
            // Special permissions
            Permission::DeleteRecord => "delete_record",
            Permission::HardDeleteRecord => "hard_delete_record",
            Permission::HardDeleteRecordWithDependencies => "hard_delete_record_with_dependencies",
        }
    }

    pub fn from_str(s: &str) -> Option<Self> {
        match s {
            // User management
            "manage_users" => Some(Permission::ManageUsers),
            // Strategic Goal permissions
            "view_strategic_goals" => Some(Permission::ViewStrategicGoals),
            "edit_strategic_goals" => Some(Permission::EditStrategicGoals),
            "create_strategic_goals" => Some(Permission::CreateStrategicGoals),
            "delete_strategic_goals" => Some(Permission::DeleteStrategicGoals),
            // Project permissions
            "view_projects" => Some(Permission::ViewProjects),
            "edit_projects" => Some(Permission::EditProjects),
            "create_projects" => Some(Permission::CreateProjects),
            "delete_projects" => Some(Permission::DeleteProjects),
            // Participant permissions
            "view_participants" => Some(Permission::ViewParticipants),
            "edit_participants" => Some(Permission::EditParticipants),
            "create_participants" => Some(Permission::CreateParticipants),
            "delete_participants" => Some(Permission::DeleteParticipants),
            // Workshop permissions
            "view_workshops" => Some(Permission::ViewWorkshops),
            "edit_workshops" => Some(Permission::EditWorkshops),
            "create_workshops" => Some(Permission::CreateWorkshops),
            "delete_workshops" => Some(Permission::DeleteWorkshops),
            // Activity permissions
            "view_activities" => Some(Permission::ViewActivities),
            "edit_activities" => Some(Permission::EditActivities),
            "create_activities" => Some(Permission::CreateActivities),
            "delete_activities" => Some(Permission::DeleteActivities),
            // Livelihood permissions
            "view_livelihoods" => Some(Permission::ViewLivelihoods),
            "edit_livelihoods" => Some(Permission::EditLivelihoods),
            "create_livelihoods" => Some(Permission::CreateLivelihoods),
            "delete_livelihoods" => Some(Permission::DeleteLivelihoods),
            // Document permissions
            "view_documents" => Some(Permission::ViewDocuments),
            "edit_documents" => Some(Permission::EditDocuments),
            "upload_documents" => Some(Permission::UploadDocuments),
            "delete_documents" => Some(Permission::DeleteDocuments),
            // Donor permissions
            "view_donors" => Some(Permission::ViewDonors),
            "create_donors" => Some(Permission::CreateDonors),
            "edit_donors" => Some(Permission::EditDonors),
            "delete_donors" => Some(Permission::DeleteDonors),
            // Funding permissions
            "view_funding" => Some(Permission::ViewFunding),
            "create_funding" => Some(Permission::CreateFunding),
            "edit_funding" => Some(Permission::EditFunding),
            "delete_funding" => Some(Permission::DeleteFunding),
            // System permissions
            "view_audit_logs" => Some(Permission::ViewAuditLogs),
            "configure_system" => Some(Permission::ConfigureSystem),
            "export_data" => Some(Permission::ExportData),
            "import_data" => Some(Permission::ImportData),
            // Sync permissions
            "sync_data" => Some(Permission::SyncData),
            "configure_personal_sync" => Some(Permission::ConfigurePersonalSync),
            "view_sync_status" => Some(Permission::ViewSyncStatus),
            "manage_global_sync" => Some(Permission::ManageGlobalSync),
            // Special permissions
            "delete_record" => Some(Permission::DeleteRecord),
            "hard_delete_record" => Some(Permission::HardDeleteRecord),
            "hard_delete_record_with_dependencies" => Some(Permission::HardDeleteRecordWithDependencies),
            // Default case
            _ => None,
        }
    }

    /// Get all permissions in the system (including new ones)
    pub fn all() -> Vec<Permission> {
        vec![
            // User management
            Permission::ManageUsers,
            // Strategic Goal permissions
            Permission::ViewStrategicGoals, Permission::EditStrategicGoals, Permission::CreateStrategicGoals, Permission::DeleteStrategicGoals,
            // Project permissions
            Permission::ViewProjects, Permission::EditProjects, Permission::CreateProjects, Permission::DeleteProjects,
            // Participant permissions
            Permission::ViewParticipants, Permission::EditParticipants, Permission::CreateParticipants, Permission::DeleteParticipants,
            // Workshop permissions
            Permission::ViewWorkshops, Permission::EditWorkshops, Permission::CreateWorkshops, Permission::DeleteWorkshops,
            // Activity permissions
            Permission::ViewActivities, Permission::EditActivities, Permission::CreateActivities, Permission::DeleteActivities,
            // Livelihood permissions
            Permission::ViewLivelihoods, Permission::EditLivelihoods, Permission::CreateLivelihoods, Permission::DeleteLivelihoods,
            // Document permissions
            Permission::ViewDocuments, Permission::EditDocuments, Permission::UploadDocuments, Permission::DeleteDocuments,
            // Donor permissions
            Permission::ViewDonors, Permission::CreateDonors, Permission::EditDonors, Permission::DeleteDonors,
            // Funding permissions
            Permission::ViewFunding, Permission::CreateFunding, Permission::EditFunding, Permission::DeleteFunding,
            // System permissions
            Permission::ViewAuditLogs, Permission::ConfigureSystem, Permission::ExportData, Permission::ImportData,
            // Sync permissions
            Permission::SyncData, Permission::ConfigurePersonalSync, Permission::ViewSyncStatus, Permission::ManageGlobalSync,
            // Special permissions
            Permission::DeleteRecord, Permission::HardDeleteRecord, Permission::HardDeleteRecordWithDependencies,
        ]
    }
}



// /Users/sagarshrestha/ipad_rust_core copy/src/domains/project/repository.rs
use crate::auth::AuthContext;
use sqlx::{Executor, Row, Sqlite, Transaction, SqlitePool, Arguments, sqlite::SqliteArguments};
use sqlx::QueryBuilder;
use crate::domains::core::delete_service::DeleteServiceRepository;
use crate::domains::core::repository::{FindById, HardDeletable, SoftDeletable};
use crate::domains::core::document_linking::DocumentLinkable;
use crate::domains::project::types::{NewProject, Project, ProjectRow, UpdateProject, ProjectStatistics, ProjectStatusBreakdown, ProjectMetadataCounts, ProjectDocumentReference};
use crate::domains::sync::repository::ChangeLogRepository;
use crate::domains::sync::types::{ChangeLogEntry, ChangeOperationType};
use crate::errors::{DbError, DomainError, DomainResult, ValidationError};
use crate::types::{PaginatedResult, PaginationParams};
use crate::domains::sync::types::SyncPriority as SyncPriorityFromSyncDomain;
use std::str::FromStr;
use async_trait::async_trait;
use chrono::{Utc, DateTime};
use sqlx::{query, query_as, query_scalar};
use uuid::Uuid;
use std::collections::HashMap;
use std::sync::Arc;
use serde_json;

/// Trait defining project repository operations
#[async_trait]
pub trait ProjectRepository: DeleteServiceRepository<Project> + Send + Sync {
    async fn create(
        &self,
        new_project: &NewProject,
        auth: &AuthContext,
    ) -> DomainResult<Project>;
    async fn create_with_tx<'t>(
        &self,
        new_project: &NewProject,
        auth: &AuthContext,
        tx: &mut Transaction<'t, Sqlite>,
    ) -> DomainResult<Project>;

    async fn update(
        &self,
        id: Uuid,
        update_data: &UpdateProject,
        auth: &AuthContext,
    ) -> DomainResult<Project>;
    async fn update_with_tx<'t>(
        &self,
        id: Uuid,
        update_data: &UpdateProject,
        auth: &AuthContext,
        tx: &mut Transaction<'t, Sqlite>,
    ) -> DomainResult<Project>;

    async fn find_all(
        &self,
        params: PaginationParams,
    ) -> DomainResult<PaginatedResult<Project>>;
    
    async fn find_by_strategic_goal(
        &self,
        strategic_goal_id: Uuid,
        params: PaginationParams,
    ) -> DomainResult<PaginatedResult<Project>>;

    async fn update_sync_priority(
        &self,
        ids: &[Uuid],
        priority: SyncPriorityFromSyncDomain,
        auth: &AuthContext,
    ) -> DomainResult<u64>;

    /// Count projects by status
    async fn count_by_status(&self) -> DomainResult<Vec<(Option<i64>, i64)>>;
    
    /// Count projects by strategic goal
    async fn count_by_strategic_goal(&self) -> DomainResult<Vec<(Option<Uuid>, i64)>>;
    
    /// Count projects by responsible team
    async fn count_by_responsible_team(&self) -> DomainResult<Vec<(Option<String>, i64)>>;
    
    /// Get comprehensive project statistics
    async fn get_project_statistics(&self) -> DomainResult<ProjectStatistics>;
    
    /// Find projects by status
    async fn find_by_status(
        &self,
        status_id: i64,
        params: PaginationParams,
    ) -> DomainResult<PaginatedResult<Project>>;
    
    /// Find projects by responsible team
    async fn find_by_responsible_team(
        &self,
        team: &str,
        params: PaginationParams,
    ) -> DomainResult<PaginatedResult<Project>>;
    
    /// Get project document references
    async fn get_project_document_references(
        &self,
        project_id: Uuid,
    ) -> DomainResult<Vec<ProjectDocumentReference>>;
    
    /// Search projects by name or objective
    async fn search_projects(
        &self,
        query: &str,
        params: PaginationParams,
    ) -> DomainResult<PaginatedResult<Project>>;
    
    /// Get project status breakdown
    async fn get_project_status_breakdown(&self) -> DomainResult<Vec<ProjectStatusBreakdown>>;
    
    /// Get project metadata counts
    async fn get_project_metadata_counts(&self) -> DomainResult<ProjectMetadataCounts>;
}

/// SQLite implementation for ProjectRepository
#[derive(Clone)]
pub struct SqliteProjectRepository {
    pool: SqlitePool,
    change_log_repo: Arc<dyn ChangeLogRepository + Send + Sync>,
}

impl SqliteProjectRepository {
    pub fn new(pool: SqlitePool, change_log_repo: Arc<dyn ChangeLogRepository + Send + Sync>) -> Self {
        Self { pool, change_log_repo }
    }

    fn map_row_to_entity(row: ProjectRow) -> DomainResult<Project> {
        row.into_entity()
            .map_err(|e| DomainError::Internal(format!("Failed to map row to entity: {}", e)))
    }

    // Helper to find by ID within a transaction
    async fn find_by_id_with_tx<'t>(
        &self,
        id: Uuid,
        tx: &mut Transaction<'t, Sqlite>,
    ) -> DomainResult<Project> {
        let row = query_as::<_, ProjectRow>(
            "SELECT * FROM projects WHERE id = ? AND deleted_at IS NULL",
        )
        .bind(id.to_string())
        .fetch_optional(&mut **tx)
        .await
        .map_err(DbError::from)?
        .ok_or_else(|| DomainError::EntityNotFound("Project".to_string(), id))?;

        Self::map_row_to_entity(row)
    }
}

#[async_trait]
impl FindById<Project> for SqliteProjectRepository {
    async fn find_by_id(&self, id: Uuid) -> DomainResult<Project> {
        let row = query_as::<_, ProjectRow>(
            "SELECT * FROM projects WHERE id = ? AND deleted_at IS NULL",
        )
        .bind(id.to_string())
        .fetch_optional(&self.pool)
        .await
        .map_err(DbError::from)?
        .ok_or_else(|| DomainError::EntityNotFound("Project".to_string(), id))?;

        Self::map_row_to_entity(row)
    }
}

#[async_trait]
impl SoftDeletable for SqliteProjectRepository {
    async fn soft_delete_with_tx(
        &self,
        id: Uuid,
        auth: &AuthContext,
        tx: &mut Transaction<'_, Sqlite>,
    ) -> DomainResult<()> {
        let now = Utc::now().to_rfc3339();
        let deleted_by = auth.user_id.to_string();
        
        let result = query(
            "UPDATE projects SET deleted_at = ?, deleted_by_user_id = ? WHERE id = ? AND deleted_at IS NULL"
        )
        .bind(now)
        .bind(deleted_by)
        .bind(id.to_string())
        .execute(&mut **tx) 
        .await
        .map_err(DbError::from)?;

        if result.rows_affected() == 0 {
            Err(DomainError::EntityNotFound("Project".to_string(), id))
        } else {
            Ok(())
        }
    }

    async fn soft_delete(&self, id: Uuid, auth: &AuthContext) -> DomainResult<()> {
        let mut tx = self.pool.begin().await.map_err(DbError::from)?;
        match self.soft_delete_with_tx(id, auth, &mut tx).await {
            Ok(()) => {
                tx.commit().await.map_err(DbError::from)?;
                Ok(())
            }
            Err(e) => {
                let _ = tx.rollback().await;
                Err(e)
            }
        }
    }
}

#[async_trait]
impl HardDeletable for SqliteProjectRepository {
    fn entity_name(&self) -> &'static str {
        "projects"
    }
    
    async fn hard_delete_with_tx(
        &self,
        id: Uuid,
        _auth: &AuthContext, // Auth context might be used for logging/checks later
        tx: &mut Transaction<'_, Sqlite>,
    ) -> DomainResult<()> {
        // Note: Cascade delete for related activities is handled by DB schema
        let result = query("DELETE FROM projects WHERE id = ?")
            .bind(id.to_string())
            .execute(&mut **tx)
            .await
            .map_err(DbError::from)?;

        if result.rows_affected() == 0 {
            Err(DomainError::EntityNotFound("Project".to_string(), id))
        } else {
            Ok(())
        }
    }

    async fn hard_delete(&self, id: Uuid, auth: &AuthContext) -> DomainResult<()> {
        let mut tx = self.pool.begin().await.map_err(DbError::from)?;
        match self.hard_delete_with_tx(id, auth, &mut tx).await {
            Ok(()) => {
                tx.commit().await.map_err(DbError::from)?;
                Ok(())
            }
            Err(e) => {
                let _ = tx.rollback().await;
                Err(e)
            }
        }
    }
}

// Blanket implementation in core::delete_service handles DeleteServiceRepository

#[async_trait]
impl ProjectRepository for SqliteProjectRepository {
    async fn create(
        &self,
        new_project: &NewProject,
        auth: &AuthContext,
    ) -> DomainResult<Project> {
        let mut tx = self.pool.begin().await.map_err(DbError::from)?;
        let result = self.create_with_tx(new_project, auth, &mut tx).await;
        match result {
            Ok(project) => {
                tx.commit().await.map_err(DbError::from)?;
                Ok(project)
            }
            Err(e) => {
                let _ = tx.rollback().await;
                Err(e)
            }
        }
    }

    async fn create_with_tx<'t>(
        &self,
        new_project: &NewProject,
        auth: &AuthContext,
        tx: &mut Transaction<'t, Sqlite>,
    ) -> DomainResult<Project> {
        let id = Uuid::new_v4();
        let now = Utc::now();
        let now_str = now.to_rfc3339();
        let user_id = auth.user_id;
        let user_id_str = user_id.to_string();
        let device_uuid: Option<Uuid> = auth.device_id.parse::<Uuid>().ok();
        let strategic_goal_id_str = new_project.strategic_goal_id.map(|id| id.to_string());
        let created_by_id_str = new_project.created_by_user_id
            .map(|id| id.to_string())
            .unwrap_or_else(|| user_id_str.clone());

        let mut builder = QueryBuilder::new(
            r#"INSERT INTO projects (
                id, strategic_goal_id, 
                name, name_updated_at, name_updated_by,
                objective, objective_updated_at, objective_updated_by,
                outcome, outcome_updated_at, outcome_updated_by,
                status_id, status_id_updated_at, status_id_updated_by,
                timeline, timeline_updated_at, timeline_updated_by,
                responsible_team, responsible_team_updated_at, responsible_team_updated_by,
                sync_priority,
                created_at, updated_at, created_by_user_id, updated_by_user_id,
                deleted_at, deleted_by_user_id
            ) "#
        );

        builder.push_values([ (
            id.to_string(), strategic_goal_id_str,
            new_project.name.clone(), now_str.clone(), user_id_str.clone(),
            new_project.objective.clone(), new_project.objective.as_ref().map(|_| &now_str), new_project.objective.as_ref().map(|_| &user_id_str),
            new_project.outcome.clone(), new_project.outcome.as_ref().map(|_| &now_str), new_project.outcome.as_ref().map(|_| &user_id_str),
            new_project.status_id.clone(), new_project.status_id.as_ref().map(|_| &now_str), new_project.status_id.as_ref().map(|_| &user_id_str),
            new_project.timeline.clone(), new_project.timeline.as_ref().map(|_| &now_str), new_project.timeline.as_ref().map(|_| &user_id_str),
            new_project.responsible_team.clone(), new_project.responsible_team.as_ref().map(|_| &now_str), new_project.responsible_team.as_ref().map(|_| &user_id_str),
            new_project.sync_priority.as_str(),
            now_str.clone(), now_str.clone(), created_by_id_str, user_id_str.clone(),
            Option::<String>::None, Option::<String>::None
        )], |mut b, values| {
             b.push_bind(values.0); b.push_bind(values.1);
             b.push_bind(values.2); b.push_bind(values.3); b.push_bind(values.4);
             b.push_bind(values.5); b.push_bind(values.6); b.push_bind(values.7);
             b.push_bind(values.8); b.push_bind(values.9); b.push_bind(values.10);
             b.push_bind(values.11); b.push_bind(values.12); b.push_bind(values.13);
             b.push_bind(values.14); b.push_bind(values.15); b.push_bind(values.16);
             b.push_bind(values.17); b.push_bind(values.18); b.push_bind(values.19);
             b.push_bind(values.20);
             b.push_bind(values.21); b.push_bind(values.22); b.push_bind(values.23); b.push_bind(values.24);
             b.push_bind(values.25); b.push_bind(values.26);
        });

        let query = builder.build();
        query.execute(&mut **tx).await.map_err(DbError::from)?;

        // Log Create Operation
        let entry = ChangeLogEntry {
            operation_id: Uuid::new_v4(),
            entity_table: self.entity_name().to_string(),
            entity_id: id,
            operation_type: ChangeOperationType::Create,
            field_name: None,
            old_value: None,
            new_value: None, 
            timestamp: now,
            user_id: user_id,
            device_id: device_uuid,
            document_metadata: None,
            sync_batch_id: None,
            processed_at: None,
            sync_error: None,
        };
        self.change_log_repo.create_change_log_with_tx(&entry, tx).await?;

        self.find_by_id_with_tx(id, tx).await
    }

    async fn update(
        &self,
        id: Uuid,
        update_data: &UpdateProject,
        auth: &AuthContext,
    ) -> DomainResult<Project> {
        let mut tx = self.pool.begin().await.map_err(DbError::from)?;
        let result = self.update_with_tx(id, update_data, auth, &mut tx).await;
        match result {
            Ok(project) => {
                tx.commit().await.map_err(DbError::from)?;
                Ok(project)
            }
            Err(e) => {
                let _ = tx.rollback().await;
                Err(e)
            }
        }
    }

    async fn update_with_tx<'t>(
        &self,
        id: Uuid,
        update_data: &UpdateProject,
        auth: &AuthContext,
        tx: &mut Transaction<'t, Sqlite>,
    ) -> DomainResult<Project> {
        let old_entity = self.find_by_id_with_tx(id, tx).await?;
        
        let now = Utc::now();
        let now_str = now.to_rfc3339();
        let user_id = auth.user_id;
        let user_id_str = user_id.to_string();
        let id_str = id.to_string();
        let device_uuid: Option<Uuid> = auth.device_id.parse::<Uuid>().ok();

        let mut builder = QueryBuilder::new("UPDATE projects SET ");
        let mut separated = builder.separated(", ");
        let mut fields_updated = false;

        macro_rules! add_lww {
            ($field_name:ident, $field_sql:literal, $value:expr) => {
                if let Some(val) = $value {
                    separated.push(concat!($field_sql, " = "));
                    separated.push_bind_unseparated(val.clone());
                    separated.push(concat!(" ", $field_sql, "_updated_at = "));
                    separated.push_bind_unseparated(now_str.clone());
                    separated.push(concat!(" ", $field_sql, "_updated_by = "));
                    separated.push_bind_unseparated(user_id_str.clone());
                    fields_updated = true;
                }
            };
        }

        // Special handling for strategic_goal_id (Option<Option<Uuid>> in DTO)
        if let Some(opt_sg_id) = update_data.strategic_goal_id {
            let sg_id_str = opt_sg_id.map(|id| id.to_string());
            separated.push("strategic_goal_id = ");
            separated.push_bind_unseparated(sg_id_str); // Bind Option<String>
            // Note: We don't track LWW for foreign keys directly here, assumed handled if FK constraint changes
            fields_updated = true;
        }

        add_lww!(name, "name", &update_data.name.as_ref());
        add_lww!(objective, "objective", &update_data.objective.as_ref());
        add_lww!(outcome, "outcome", &update_data.outcome.as_ref());
        add_lww!(status_id, "status_id", &update_data.status_id.as_ref());
        add_lww!(timeline, "timeline", &update_data.timeline.as_ref());
        add_lww!(responsible_team, "responsible_team", &update_data.responsible_team.as_ref());

        if let Some(priority) = update_data.sync_priority {
            separated.push("sync_priority = ");
            separated.push_bind_unseparated(priority.as_str());
            fields_updated = true;
        }

        if !fields_updated {
            return Ok(old_entity);
        }

        separated.push("updated_at = ");
        separated.push_bind_unseparated(now_str.clone());
        separated.push("updated_by_user_id = ");
        separated.push_bind_unseparated(user_id_str.clone());

        builder.push(" WHERE id = ");
        builder.push_bind(id_str);
        builder.push(" AND deleted_at IS NULL");
        let query = builder.build();
        let result = query.execute(&mut **tx).await.map_err(DbError::from)?;
        if result.rows_affected() == 0 {
            return Err(DomainError::EntityNotFound(self.entity_name().to_string(), id));
        }

        let new_entity = self.find_by_id_with_tx(id, tx).await?;

        // Compare and log field changes
        if old_entity.name != new_entity.name { 
            let entry = ChangeLogEntry {
                operation_id: Uuid::new_v4(),
                entity_table: self.entity_name().to_string(),
                entity_id: id,
                operation_type: ChangeOperationType::Update,
                field_name: Some("name".to_string()),
                old_value: serde_json::to_string(&old_entity.name).ok(),
                new_value: serde_json::to_string(&new_entity.name).ok(),
                timestamp: now,
                user_id: user_id,
                device_id: device_uuid.clone(),
                document_metadata: None,
                sync_batch_id: None,
                processed_at: None,
                sync_error: None,
             };
             self.change_log_repo.create_change_log_with_tx(&entry, tx).await?;
        }
        if old_entity.objective != new_entity.objective { 
            let entry = ChangeLogEntry {
                operation_id: Uuid::new_v4(),
                entity_table: self.entity_name().to_string(),
                entity_id: id,
                operation_type: ChangeOperationType::Update,
                field_name: Some("objective".to_string()),
                old_value: serde_json::to_string(&old_entity.objective).ok(),
                new_value: serde_json::to_string(&new_entity.objective).ok(),
                timestamp: now,
                user_id: user_id,
                device_id: device_uuid.clone(),
                document_metadata: None,
                sync_batch_id: None,
                processed_at: None,
                sync_error: None,
             };
             self.change_log_repo.create_change_log_with_tx(&entry, tx).await?;
        }
        if old_entity.outcome != new_entity.outcome { 
            let entry = ChangeLogEntry {
                operation_id: Uuid::new_v4(),
                entity_table: self.entity_name().to_string(),
                entity_id: id,
                operation_type: ChangeOperationType::Update,
                field_name: Some("outcome".to_string()),
                old_value: serde_json::to_string(&old_entity.outcome).ok(),
                new_value: serde_json::to_string(&new_entity.outcome).ok(),
                timestamp: now,
                user_id: user_id,
                device_id: device_uuid.clone(),
                document_metadata: None,
                sync_batch_id: None,
                processed_at: None,
                sync_error: None,
             };
             self.change_log_repo.create_change_log_with_tx(&entry, tx).await?;
        }
        if old_entity.status_id != new_entity.status_id { 
            let entry = ChangeLogEntry {
                operation_id: Uuid::new_v4(),
                entity_table: self.entity_name().to_string(),
                entity_id: id,
                operation_type: ChangeOperationType::Update,
                field_name: Some("status_id".to_string()),
                old_value: serde_json::to_string(&old_entity.status_id).ok(),
                new_value: serde_json::to_string(&new_entity.status_id).ok(),
                timestamp: now,
                user_id: user_id,
                device_id: device_uuid.clone(),
                document_metadata: None,
                sync_batch_id: None,
                processed_at: None,
                sync_error: None,
             };
             self.change_log_repo.create_change_log_with_tx(&entry, tx).await?;
        }
        if old_entity.timeline != new_entity.timeline { 
            let entry = ChangeLogEntry {
                operation_id: Uuid::new_v4(),
                entity_table: self.entity_name().to_string(),
                entity_id: id,
                operation_type: ChangeOperationType::Update,
                field_name: Some("timeline".to_string()),
                old_value: serde_json::to_string(&old_entity.timeline).ok(),
                new_value: serde_json::to_string(&new_entity.timeline).ok(),
                timestamp: now,
                user_id: user_id,
                device_id: device_uuid.clone(),
                document_metadata: None,
                sync_batch_id: None,
                processed_at: None,
                sync_error: None,
             };
             self.change_log_repo.create_change_log_with_tx(&entry, tx).await?;
        }
        if old_entity.responsible_team != new_entity.responsible_team { 
            let entry = ChangeLogEntry {
                operation_id: Uuid::new_v4(),
                entity_table: self.entity_name().to_string(),
                entity_id: id,
                operation_type: ChangeOperationType::Update,
                field_name: Some("responsible_team".to_string()),
                old_value: serde_json::to_string(&old_entity.responsible_team).ok(),
                new_value: serde_json::to_string(&new_entity.responsible_team).ok(),
                timestamp: now,
                user_id: user_id,
                device_id: device_uuid.clone(),
                document_metadata: None,
                sync_batch_id: None,
                processed_at: None,
                sync_error: None,
             };
             self.change_log_repo.create_change_log_with_tx(&entry, tx).await?;
        }
        if old_entity.strategic_goal_id != new_entity.strategic_goal_id { 
            let entry = ChangeLogEntry {
                operation_id: Uuid::new_v4(),
                entity_table: self.entity_name().to_string(),
                entity_id: id,
                operation_type: ChangeOperationType::Update,
                field_name: Some("strategic_goal_id".to_string()),
                old_value: serde_json::to_string(&old_entity.strategic_goal_id).ok(),
                new_value: serde_json::to_string(&new_entity.strategic_goal_id).ok(),
                timestamp: now,
                user_id: user_id,
                device_id: device_uuid.clone(),
                document_metadata: None,
                sync_batch_id: None,
                processed_at: None,
                sync_error: None,
             };
             self.change_log_repo.create_change_log_with_tx(&entry, tx).await?;
        }
        if old_entity.sync_priority != new_entity.sync_priority { 
            let entry = ChangeLogEntry {
                operation_id: Uuid::new_v4(),
                entity_table: self.entity_name().to_string(),
                entity_id: id,
                operation_type: ChangeOperationType::Update,
                field_name: Some("sync_priority".to_string()),
                old_value: serde_json::to_string(old_entity.sync_priority.as_str()).ok(),
                new_value: serde_json::to_string(new_entity.sync_priority.as_str()).ok(),
                timestamp: now,
                user_id: user_id,
                device_id: device_uuid.clone(),
                document_metadata: None,
                sync_batch_id: None,
                processed_at: None,
                sync_error: None,
             };
             self.change_log_repo.create_change_log_with_tx(&entry, tx).await?;
        }
        
        Ok(new_entity)
    }

    async fn find_all(
        &self,
        params: PaginationParams,
    ) -> DomainResult<PaginatedResult<Project>> {
        let offset = (params.page - 1) * params.per_page;

        let total: i64 = query_scalar("SELECT COUNT(*) FROM projects WHERE deleted_at IS NULL")
            .fetch_one(&self.pool)
            .await
            .map_err(DbError::from)?;

        let rows = query_as::<_, ProjectRow>(
            "SELECT * FROM projects WHERE deleted_at IS NULL ORDER BY name ASC LIMIT ? OFFSET ?",
        )
        .bind(params.per_page as i64)
        .bind(offset as i64)
        .fetch_all(&self.pool)
        .await
        .map_err(DbError::from)?;

        let entities = rows
            .into_iter()
            .map(Self::map_row_to_entity)
            .collect::<DomainResult<Vec<Project>>>()?;

        Ok(PaginatedResult::new(
            entities,
            total as u64,
            params,
        ))
    }
    
    async fn find_by_strategic_goal(
        &self,
        strategic_goal_id: Uuid,
        params: PaginationParams,
    ) -> DomainResult<PaginatedResult<Project>> {
        let offset = (params.page - 1) * params.per_page;
        let sg_id_str = strategic_goal_id.to_string();

        let total: i64 = query_scalar(
             "SELECT COUNT(*) FROM projects WHERE strategic_goal_id = ? AND deleted_at IS NULL"
         )
         .bind(&sg_id_str)
         .fetch_one(&self.pool)
         .await
         .map_err(DbError::from)?;

        let rows = query_as::<_, ProjectRow>(
            "SELECT * FROM projects WHERE strategic_goal_id = ? AND deleted_at IS NULL ORDER BY name ASC LIMIT ? OFFSET ?",
        )
        .bind(sg_id_str)
        .bind(params.per_page as i64)
        .bind(offset as i64)
        .fetch_all(&self.pool)
        .await
        .map_err(DbError::from)?;

        let entities = rows
            .into_iter()
            .map(Self::map_row_to_entity)
            .collect::<DomainResult<Vec<Project>>>()?;

        Ok(PaginatedResult::new(
            entities,
            total as u64,
            params,
        ))
    }

    async fn update_sync_priority(
        &self,
        ids: &[Uuid],
        priority: SyncPriorityFromSyncDomain,
        auth: &AuthContext,
    ) -> DomainResult<u64> {
        if ids.is_empty() { return Ok(0); }
        
        let mut tx = self.pool.begin().await.map_err(DbError::from)?;

        // Fetch old priorities
        let id_strings: Vec<String> = ids.iter().map(Uuid::to_string).collect();
        let select_query = format!(
            "SELECT id, sync_priority FROM projects WHERE id IN ({})",
            vec!["?"; ids.len()].join(", ")
        );
        // Fetch as String
        let mut select_builder = query_as::<_, (String, String)>(&select_query);
        for id_str in &id_strings {
            select_builder = select_builder.bind(id_str);
        }
        let old_priorities: HashMap<Uuid, SyncPriorityFromSyncDomain> = select_builder
            .fetch_all(&mut *tx)
            .await.map_err(DbError::from)?
            .into_iter()
            .filter_map(|(id_str, prio_text)| {
                match Uuid::parse_str(&id_str) {
                    Ok(id) => Some((id, SyncPriorityFromSyncDomain::from_str(&prio_text).unwrap_or_default())),
                    Err(_) => None,
                }
            }).collect();

        // Perform Update
        let now = Utc::now();
        let now_str = now.to_rfc3339();
        let user_id_str = auth.user_id.to_string();
        let priority_str = priority.as_str(); // Now correctly uses the method from SyncPriorityFromSyncDomain

        let mut update_builder = QueryBuilder::new("UPDATE projects SET ");
        update_builder.push("sync_priority = "); update_builder.push_bind(priority_str); // Bind TEXT
        update_builder.push(", updated_at = "); update_builder.push_bind(now_str.clone());
        update_builder.push(", updated_by_user_id = "); update_builder.push_bind(user_id_str.clone());
        update_builder.push(" WHERE id IN (");
        let mut id_separated = update_builder.separated(",");
        for id in ids { id_separated.push_bind(id.to_string()); }
        update_builder.push(") AND deleted_at IS NULL");

        let query = update_builder.build();
        let result = query.execute(&mut *tx).await.map_err(DbError::from)?;
        let rows_affected = result.rows_affected();

        // Log changes
        for id in ids {
            if let Some(old_priority) = old_priorities.get(id) {
                if *old_priority != priority {
                    let entry = ChangeLogEntry {
                        operation_id: Uuid::new_v4(),
                        entity_table: self.entity_name().to_string(),
                        entity_id: *id,
                        operation_type: ChangeOperationType::Update,
                        field_name: Some("sync_priority".to_string()),
                        old_value: serde_json::to_string(old_priority.as_str()).ok(), // Log as TEXT
                        new_value: serde_json::to_string(priority_str).ok(), // Log as TEXT
                        timestamp: now,
                        user_id: auth.user_id,
                        device_id: auth.device_id.parse::<Uuid>().ok(),
                        document_metadata: None,
                        sync_batch_id: None,
                        processed_at: None,
                        sync_error: None,
                    };
                    self.change_log_repo.create_change_log_with_tx(&entry, &mut tx).await?;
                }
            }
        }

        tx.commit().await.map_err(DbError::from)?;
        Ok(rows_affected)
    }

    async fn count_by_status(&self) -> DomainResult<Vec<(Option<i64>, i64)>> {
        let counts = query_as::<_, (Option<i64>, i64)>(
            "SELECT status_id, COUNT(*) 
             FROM projects 
             WHERE deleted_at IS NULL 
             GROUP BY status_id"
        )
        .fetch_all(&self.pool)
        .await
        .map_err(DbError::from)?;

        Ok(counts)
    }
    
    async fn count_by_strategic_goal(&self) -> DomainResult<Vec<(Option<Uuid>, i64)>> {
        let rows = query(
            "SELECT strategic_goal_id, COUNT(*) as count
             FROM projects 
             WHERE deleted_at IS NULL 
             GROUP BY strategic_goal_id"
        )
        .fetch_all(&self.pool)
        .await
        .map_err(DbError::from)?;

        // Manual mapping to handle Option<Uuid>
        let mut results = Vec::new();
        for row in rows {
            let sg_id_str: Option<String> = row.get("strategic_goal_id");
            let count: i64 = row.get("count");
            
            let sg_id = match sg_id_str {
                Some(id_str) => Some(Uuid::parse_str(&id_str).map_err(|_| 
                    DomainError::Internal(format!("Invalid UUID in strategic_goal_id: {}", id_str))
                )?),
                None => None,
            };
            
            results.push((sg_id, count));
        }

        Ok(results)
    }
    
    async fn count_by_responsible_team(&self) -> DomainResult<Vec<(Option<String>, i64)>> {
        let counts = query_as::<_, (Option<String>, i64)>(
            "SELECT responsible_team, COUNT(*) 
             FROM projects 
             WHERE deleted_at IS NULL 
             GROUP BY responsible_team"
        )
        .fetch_all(&self.pool)
        .await
        .map_err(DbError::from)?;

        Ok(counts)
    }
    
    async fn get_project_statistics(&self) -> DomainResult<ProjectStatistics> {
        // Get total project count
        let total_projects: i64 = query_scalar(
            "SELECT COUNT(*) FROM projects WHERE deleted_at IS NULL"
        )
        .fetch_one(&self.pool)
        .await
        .map_err(DbError::from)?;
        
        // Get document count
        let document_count: i64 = query_scalar(
            "SELECT COUNT(*) 
             FROM media_documents 
             WHERE related_table = 'projects' -- Corrected entity_type to related_table
             AND deleted_at IS NULL"
        )
        .fetch_one(&self.pool)
        .await
        .map_err(DbError::from)?;
        
        // Get status distribution
        let status_counts = self.count_by_status().await?;
        let mut by_status = HashMap::new();
        for (status_id_opt, count) in status_counts {
            let status_name = match status_id_opt {
                Some(1) => "On Track".to_string(),
                Some(2) => "At Risk".to_string(),
                Some(3) => "Delayed".to_string(),
                Some(4) => "Completed".to_string(),
                Some(id) => format!("Status {}", id),
                None => "Unspecified".to_string(),
            };
            by_status.insert(status_name, count);
        }
        
        // Get strategic goal distribution
        let sg_counts = self.count_by_strategic_goal().await?;
        let mut by_strategic_goal = HashMap::new();
        for (sg_id_opt, count) in sg_counts {
            let goal_name = match sg_id_opt {
                Some(id) => {
                    match query_scalar::<_, String>(
                        "SELECT objective_code FROM strategic_goals WHERE id = ? AND deleted_at IS NULL"
                    )
                    .bind(id.to_string())
                    .fetch_optional(&self.pool)
                    .await
                    .map_err(DbError::from)? {
                        Some(code) => code,
                        None => format!("Goal {}", id), // Fallback if goal not found
                    }
                },
                None => "No Goal".to_string(),
            };
            by_strategic_goal.insert(goal_name, count);
        }
        
        // Get team distribution
        let team_counts = self.count_by_responsible_team().await?;
        let mut by_responsible_team = HashMap::new();
        for (team_opt, count) in team_counts {
            let team_name = team_opt.unwrap_or_else(|| "Unassigned".to_string());
            by_responsible_team.insert(team_name, count);
        }
        
        Ok(ProjectStatistics {
            total_projects,
            by_status,
            by_strategic_goal,
            by_responsible_team,
            document_count,
        })
    }
    
    async fn find_by_status(
        &self,
        status_id: i64,
        params: PaginationParams,
    ) -> DomainResult<PaginatedResult<Project>> {
        let offset = (params.page - 1) * params.per_page;

        // Get total count
        let total: i64 = query_scalar(
            "SELECT COUNT(*) FROM projects 
             WHERE status_id = ? AND deleted_at IS NULL"
        )
        .bind(status_id)
        .fetch_one(&self.pool)
        .await
        .map_err(DbError::from)?;

        // Fetch paginated rows
        let rows = query_as::<_, ProjectRow>(
            "SELECT * FROM projects 
             WHERE status_id = ? AND deleted_at IS NULL 
             ORDER BY name ASC LIMIT ? OFFSET ?"
        )
        .bind(status_id)
        .bind(params.per_page as i64)
        .bind(offset as i64)
        .fetch_all(&self.pool)
        .await
        .map_err(DbError::from)?;

        let entities = rows
            .into_iter()
            .map(Self::map_row_to_entity)
            .collect::<DomainResult<Vec<Project>>>()?;

        Ok(PaginatedResult::new(
            entities,
            total as u64,
            params,
        ))
    }
    
    async fn find_by_responsible_team(
        &self,
        team: &str,
        params: PaginationParams,
    ) -> DomainResult<PaginatedResult<Project>> {
        let offset = (params.page - 1) * params.per_page;

        // Get total count
        let total: i64 = query_scalar(
            "SELECT COUNT(*) FROM projects 
             WHERE responsible_team = ? AND deleted_at IS NULL"
        )
        .bind(team)
        .fetch_one(&self.pool)
        .await
        .map_err(DbError::from)?;

        // Fetch paginated rows
        let rows = query_as::<_, ProjectRow>(
            "SELECT * FROM projects 
             WHERE responsible_team = ? AND deleted_at IS NULL 
             ORDER BY name ASC LIMIT ? OFFSET ?"
        )
        .bind(team)
        .bind(params.per_page as i64)
        .bind(offset as i64)
        .fetch_all(&self.pool)
        .await
        .map_err(DbError::from)?;

        let entities = rows
            .into_iter()
            .map(Self::map_row_to_entity)
            .collect::<DomainResult<Vec<Project>>>()?;

        Ok(PaginatedResult::new(
            entities,
            total as u64,
            params,
        ))
    }
    
    async fn get_project_document_references(
        &self,
        project_id: Uuid,
    ) -> DomainResult<Vec<ProjectDocumentReference>> {
        let project_id_str = project_id.to_string();
        
        let mut references = Vec::new();
        let doc_ref_fields: Vec<_> = Project::field_metadata()
            .into_iter()
            .filter(|field| field.is_document_reference_only)
            .collect();
            
        for field in doc_ref_fields {
            let column_name = format!("{}_ref", field.field_name);
            
            // Query updated to fetch document details directly
            let query_str = format!(
                "SELECT p.{} as doc_id, m.original_filename, m.created_at, m.size_bytes as file_size 
                 FROM projects p 
                 LEFT JOIN media_documents m ON p.{} = m.id AND m.deleted_at IS NULL
                 WHERE p.id = ? AND p.deleted_at IS NULL", 
                column_name, column_name
            );
            
            let row = query(&query_str)
                .bind(&project_id_str)
                .fetch_optional(&self.pool)
                .await
                .map_err(DbError::from)?;
                
            if let Some(row) = row {
                let doc_id_str: Option<String> = row.get("doc_id");
                let doc_id = doc_id_str.map(|id_str| 
                    Uuid::parse_str(&id_str)
                        .map_err(|_| DomainError::Internal(format!("Invalid UUID: {}", id_str)))
                ).transpose()?;
                
                let (filename, upload_date, file_size) = if doc_id.is_some() {
                    (
                        row.get("original_filename"),
                        row.get::<Option<String>, _>("created_at").map(|dt_str| 
                            DateTime::parse_from_rfc3339(&dt_str)
                                .map_err(|_| DomainError::Internal(format!("Invalid datetime: {}", dt_str)))
                                .map(|dt| dt.with_timezone(&Utc))
                        ).transpose()?, // Parse and convert Option<String> to Option<DateTime<Utc>>
                        row.get::<Option<i64>, _>("file_size").map(|fs| fs as u64), // Corrected type: file_size from i64 to u64
                    )
                } else {
                    (None, None, None)
                };
                
                references.push(ProjectDocumentReference {
                    field_name: field.field_name.to_string(),
                    display_name: field.display_name.to_string(),
                    document_id: doc_id,
                    filename,
                    upload_date,
                    file_size,
                });
            }
        }
        
        Ok(references)
    }
    
    async fn search_projects(
        &self,
        query: &str,
        params: PaginationParams,
    ) -> DomainResult<PaginatedResult<Project>> {
        let offset = (params.page - 1) * params.per_page;
        let search_term = format!("%{}%", query);

        // Get total count
        let total: i64 = query_scalar(
            "SELECT COUNT(*) FROM projects 
             WHERE (name LIKE ? OR objective LIKE ? OR outcome LIKE ? OR responsible_team LIKE ?) 
             AND deleted_at IS NULL"
        )
        .bind(&search_term)
        .bind(&search_term)
        .bind(&search_term)
        .bind(&search_term)
        .fetch_one(&self.pool)
        .await
        .map_err(DbError::from)?;

        // Fetch paginated rows
        let rows = query_as::<_, ProjectRow>(
            "SELECT * FROM projects 
             WHERE (name LIKE ? OR objective LIKE ? OR outcome LIKE ? OR responsible_team LIKE ?) 
             AND deleted_at IS NULL 
             ORDER BY name ASC LIMIT ? OFFSET ?"
        )
        .bind(&search_term)
        .bind(&search_term)
        .bind(&search_term)
        .bind(&search_term)
        .bind(params.per_page as i64)
        .bind(offset as i64)
        .fetch_all(&self.pool)
        .await
        .map_err(DbError::from)?;

        let entities = rows
            .into_iter()
            .map(Self::map_row_to_entity)
            .collect::<DomainResult<Vec<Project>>>()?;

        Ok(PaginatedResult::new(
            entities,
            total as u64,
            params,
        ))
    }
    
    async fn get_project_status_breakdown(&self) -> DomainResult<Vec<ProjectStatusBreakdown>> {
        // Get status counts
        let status_counts = self.count_by_status().await?;
        
        // Get total count for percentage calculation
        let total: i64 = status_counts.iter().map(|(_, count)| count).sum();
        
        // Create breakdown objects
        let mut breakdown = Vec::new();
        for (status_id_opt, count) in status_counts {
            let status_id = status_id_opt.unwrap_or(0); // Treat None as 0 or another ID
            let status_name = match status_id {
                1 => "On Track".to_string(),
                2 => "At Risk".to_string(),
                3 => "Delayed".to_string(),
                4 => "Completed".to_string(),
                _ => "Unknown".to_string(), // Handle 0 or unexpected IDs
            };
            
            let percentage = if total > 0 {
                (count as f64 / total as f64) * 100.0
            } else {
                0.0
            };
            
            breakdown.push(ProjectStatusBreakdown {
                status_id,
                status_name,
                count,
                percentage,
            });
        }
        
        // Sort by status ID (consistent order)
        breakdown.sort_by_key(|b| b.status_id);
        
        Ok(breakdown)
    }
    
    async fn get_project_metadata_counts(&self) -> DomainResult<ProjectMetadataCounts> {
        // Get team counts
        let team_counts = self.count_by_responsible_team().await?;
        let mut projects_by_team = HashMap::new();
        for (team_opt, count) in team_counts {
            let team_name = team_opt.unwrap_or_else(|| "Unassigned".to_string());
            projects_by_team.insert(team_name, count);
        }
        
        // Get status counts
        let status_counts = self.count_by_status().await?;
        let mut projects_by_status = HashMap::new();
        for (status_id_opt, count) in status_counts {
            let status_name = match status_id_opt {
                Some(1) => "On Track".to_string(),
                Some(2) => "At Risk".to_string(),
                Some(3) => "Delayed".to_string(),
                Some(4) => "Completed".to_string(),
                Some(id) => format!("Status {}", id),
                None => "Unspecified".to_string(),
            };
            projects_by_status.insert(status_name, count);
        }
        
        // Get goal counts
        let goal_counts = self.count_by_strategic_goal().await?;
        let mut projects_by_goal = HashMap::new();
        for (goal_id_opt, count) in goal_counts {
            let goal_name = match goal_id_opt {
                Some(id) => {
                    match query_scalar::<_, String>(
                        "SELECT objective_code FROM strategic_goals WHERE id = ? AND deleted_at IS NULL"
                    )
                    .bind(id.to_string())
                    .fetch_optional(&self.pool)
                    .await
                    .map_err(DbError::from)? {
                        Some(code) => code,
                        None => format!("Goal {}", id),
                    }
                },
                None => "No Goal".to_string(),
            };
            projects_by_goal.insert(goal_name, count);
        }
        
        Ok(ProjectMetadataCounts {
            projects_by_team,
            projects_by_status,
            projects_by_goal,
        })
    }
}


// /Users/sagarshrestha/ipad_rust_core copy/src/domains/project/service.rs
use crate::auth::AuthContext;
use sqlx::{SqlitePool, Transaction, Sqlite};
use crate::domains::core::dependency_checker::DependencyChecker;
use crate::domains::core::delete_service::{BaseDeleteService, DeleteOptions, DeleteService, DeleteServiceRepository};
use crate::domains::core::repository::{DeleteResult, FindById, HardDeletable, SoftDeletable};
use crate::domains::core::document_linking::DocumentLinkable;
use crate::domains::permission::Permission;
use crate::domains::project::repository::ProjectRepository;
use crate::domains::project::types::{ // Added new types
    NewProject, Project, ProjectResponse, UpdateProject, ProjectInclude, 
    ProjectStatistics, ProjectStatusBreakdown, ProjectMetadataCounts, ProjectDocumentReference,
    ProjectWithDocumentTimeline
};
use crate::domains::strategic_goal::repository::StrategicGoalRepository;
use crate::domains::sync::repository::{ChangeLogRepository, TombstoneRepository};
use crate::errors::{DomainError, DomainResult, ServiceError, ServiceResult, ValidationError, DbError};
use crate::types::{PaginatedResult, PaginationParams};
use crate::validation::Validate;
use async_trait::async_trait;
use std::sync::Arc;
use uuid::Uuid;
use std::str::FromStr;
use std::collections::HashMap;

// Import necessary types related to documents and sync/compression
use crate::domains::document::repository::MediaDocumentRepository;
use crate::domains::document::service::DocumentService;
use crate::domains::document::types::MediaDocumentResponse; // Ensure this is imported
use crate::domains::sync::types::SyncPriority;
use crate::domains::compression::types::CompressionPriority;


/// Trait defining project service operations
#[async_trait]
pub trait ProjectService: DeleteService<Project> + Send + Sync {
    async fn create_project(
        &self,
        new_project: NewProject,
        auth: &AuthContext,
    ) -> ServiceResult<ProjectResponse>;

    // ADDED: Create with documents method
    async fn create_project_with_documents(
        &self,
        new_project: NewProject,
        documents: Vec<(Vec<u8>, String, Option<String>)>, // (file_data, filename, linked_field)
        document_type_id: Uuid,
        auth: &AuthContext,
    ) -> ServiceResult<(ProjectResponse, Vec<Result<MediaDocumentResponse, ServiceError>>)>;

    async fn get_project_by_id(
        &self,
        id: Uuid,
        include: Option<&[ProjectInclude]>, // Include is used for enrichment
        auth: &AuthContext,
    ) -> ServiceResult<ProjectResponse>;

    async fn list_projects(
        &self,
        params: PaginationParams,
        include: Option<&[ProjectInclude]>, // Include is used for enrichment
        auth: &AuthContext,
    ) -> ServiceResult<PaginatedResult<ProjectResponse>>;

    async fn update_project(
        &self,
        id: Uuid,
        update_data: UpdateProject,
        auth: &AuthContext,
    ) -> ServiceResult<ProjectResponse>;

    async fn delete_project(
        &self,
        id: Uuid,
        hard_delete: bool,
        auth: &AuthContext,
    ) -> ServiceResult<DeleteResult>;

    // ADDED: Document integration methods
    async fn upload_document_for_project(
        &self,
        project_id: Uuid,
        file_data: Vec<u8>,
        original_filename: String,
        title: Option<String>,
        document_type_id: Uuid,
        linked_field: Option<String>,
        sync_priority: SyncPriority,
        compression_priority: Option<CompressionPriority>,
        auth: &AuthContext,
    ) -> ServiceResult<MediaDocumentResponse>;

    async fn bulk_upload_documents_for_project(
        &self,
        project_id: Uuid,
        files: Vec<(Vec<u8>, String)>,
        title: Option<String>,
        document_type_id: Uuid,
        sync_priority: SyncPriority,
        compression_priority: Option<CompressionPriority>,
        auth: &AuthContext,
    ) -> ServiceResult<Vec<MediaDocumentResponse>>;

    // --- New Methods ---
    
    /// Get comprehensive project statistics for dashboard
    async fn get_project_statistics(
        &self,
        auth: &AuthContext,
    ) -> ServiceResult<ProjectStatistics>;
    
    /// Get project status breakdown
    async fn get_project_status_breakdown(
        &self,
        auth: &AuthContext,
    ) -> ServiceResult<Vec<ProjectStatusBreakdown>>;
    
    /// Get project metadata counts
    async fn get_project_metadata_counts(
        &self,
        auth: &AuthContext,
    ) -> ServiceResult<ProjectMetadataCounts>;
    
    /// Find projects by status
    async fn find_projects_by_status(
        &self,
        status_id: i64,
        params: PaginationParams,
        include: Option<&[ProjectInclude]>,
        auth: &AuthContext,
    ) -> ServiceResult<PaginatedResult<ProjectResponse>>;
    
    /// Find projects by responsible team
    async fn find_projects_by_responsible_team(
        &self,
        team: &str,
        params: PaginationParams,
        include: Option<&[ProjectInclude]>,
        auth: &AuthContext,
    ) -> ServiceResult<PaginatedResult<ProjectResponse>>;
    
    /// Get project with document timeline
    async fn get_project_with_document_timeline(
        &self,
        id: Uuid,
        auth: &AuthContext,
    ) -> ServiceResult<ProjectWithDocumentTimeline>;
    
    /// Get project document references
    async fn get_project_document_references(
        &self,
        id: Uuid,
        auth: &AuthContext,
    ) -> ServiceResult<Vec<ProjectDocumentReference>>;
    
    /// Search projects by name, objective, or outcome
    async fn search_projects(
        &self,
        query: &str,
        params: PaginationParams,
        include: Option<&[ProjectInclude]>,
        auth: &AuthContext,
    ) -> ServiceResult<PaginatedResult<ProjectResponse>>;
}

/// Implementation of the project service
#[derive(Clone)]
pub struct ProjectServiceImpl {
    // ADDED: Pool for transactions
    pool: SqlitePool,
    repo: Arc<dyn ProjectRepository + Send + Sync>,
    strategic_goal_repo: Arc<dyn StrategicGoalRepository + Send + Sync>,
    delete_service: Arc<BaseDeleteService<Project>>,
    document_service: Arc<dyn DocumentService>,
}

impl ProjectServiceImpl {
    pub fn new(
        // ADDED: pool parameter
        pool: SqlitePool,
        project_repo: Arc<dyn ProjectRepository + Send + Sync>,
        strategic_goal_repo: Arc<dyn StrategicGoalRepository + Send + Sync>,
        tombstone_repo: Arc<dyn TombstoneRepository + Send + Sync>,
        change_log_repo: Arc<dyn ChangeLogRepository + Send + Sync>,
        dependency_checker: Arc<dyn DependencyChecker + Send + Sync>,
        media_doc_repo: Arc<dyn MediaDocumentRepository>, // Still needed for BaseDeleteService
        document_service: Arc<dyn DocumentService>,
    ) -> Self {
        // --- Adapter setup remains the same ---
        struct RepoAdapter(Arc<dyn ProjectRepository + Send + Sync>);

        #[async_trait]
        impl FindById<Project> for RepoAdapter {
            async fn find_by_id(&self, id: Uuid) -> DomainResult<Project> {
                self.0.find_by_id(id).await
            }
        }

        #[async_trait]
        impl SoftDeletable for RepoAdapter {
             async fn soft_delete(&self, id: Uuid, auth: &AuthContext) -> DomainResult<()> {
                 self.0.soft_delete(id, auth).await
             }
             async fn soft_delete_with_tx(
                 &self,
                 id: Uuid,
                 auth: &AuthContext,
                 tx: &mut Transaction<'_, Sqlite>,
             ) -> DomainResult<()> {
                 self.0.soft_delete_with_tx(id, auth, tx).await
             }
        }

        #[async_trait]
        impl HardDeletable for RepoAdapter {
             fn entity_name(&self) -> &'static str {
                 "projects"
             }
             async fn hard_delete(&self, id: Uuid, auth: &AuthContext) -> DomainResult<()> {
                 self.0.hard_delete(id, auth).await
             }
             async fn hard_delete_with_tx(
                 &self,
                 id: Uuid,
                 auth: &AuthContext,
                 tx: &mut Transaction<'_, Sqlite>,
             ) -> DomainResult<()> {
                 self.0.hard_delete_with_tx(id, auth, tx).await
             }
        }

        let adapted_repo: Arc<dyn DeleteServiceRepository<Project>> =
            Arc::new(RepoAdapter(project_repo.clone()));

        let delete_service = Arc::new(BaseDeleteService::new(
            pool.clone(), // Clone pool for delete service
            adapted_repo,
            tombstone_repo,
            change_log_repo,
            dependency_checker,
            Some(media_doc_repo), // Pass media repo for delete service's file handling
        ));

        Self {
            // ADDED: store pool
            pool,
            repo: project_repo,
            strategic_goal_repo,
            delete_service,
            document_service,
        }
    }

    /// Helper to enrich ProjectResponse with included data
    /// PRESERVED: Document enrichment logic is kept
    async fn enrich_response(
        &self,
        mut response: ProjectResponse,
        include: Option<&[ProjectInclude]>,
        auth: &AuthContext, // Auth context is needed for listing documents
    ) -> ServiceResult<ProjectResponse> {
        if let Some(includes) = include {
            // --- PRESERVED Document Enrichment ---
            let include_docs = includes.contains(&ProjectInclude::All) || includes.contains(&ProjectInclude::Documents);

            if include_docs {
                // Use default pagination or define specific params if needed
                let doc_params = PaginationParams::default();
                // Fetch documents using the document service
                let docs_result = self.document_service
                    .list_media_documents_by_related_entity(
                        auth,         // Pass auth context
                        "projects",   // Correct entity type
                        response.id,  // Project ID
                        doc_params,
                        None // No nested includes needed for the document list itself here
                    ).await?;
                // Attach the fetched documents to the response
                response.documents = Some(docs_result.items);
            }
             // --- END PRESERVED Document Enrichment ---

            // TODO: Add enrichment logic for other includes like StrategicGoal, Status, CreatedBy, Counts
            let _include_strategic_goal = includes.contains(&ProjectInclude::All) || includes.contains(&ProjectInclude::StrategicGoal);
            // if _include_strategic_goal && response.strategic_goal.is_none() { ... fetch strategic goal ... }
            // if include_status && response.status.is_none() { ... fetch status ... }
            // ... etc ...
        }
        Ok(response)
    }

    // Helper to validate strategic goal existence if ID is provided - Remains the same
    async fn validate_strategic_goal_exists(&self, sg_id: Option<Uuid>) -> DomainResult<()> {
        if let Some(id) = sg_id {
            match self.strategic_goal_repo.find_by_id(id).await {
                Ok(_) => Ok(()),
                Err(DomainError::EntityNotFound(_, _)) => {
                    let error_message = format!("Strategic Goal with ID {} does not exist", id);
                    Err(DomainError::Validation(
                        ValidationError::relationship(&error_message)
                    ))
                },
                Err(e) => Err(e),
            }
        } else {
            Ok(())
        }
    }

    // ADDED: Helper method copied from StrategicGoalService
    /// Helper method to upload documents for any entity and handle errors individually
    async fn upload_documents_for_entity(
        &self,
        entity_id: Uuid,
        entity_type: &str,
        documents: Vec<(Vec<u8>, String, Option<String>)>,
        document_type_id: Uuid,
        sync_priority: SyncPriority,
        compression_priority: Option<CompressionPriority>,
        auth: &AuthContext,
    ) -> Vec<Result<MediaDocumentResponse, ServiceError>> {
        let mut results = Vec::new();

        for (file_data, filename, linked_field) in documents {
            // Use the injected document_service
            let upload_result = self.document_service.upload_document(
                auth,
                file_data,
                filename,
                None, // No title, will use filename as default
                document_type_id,
                entity_id,
                entity_type.to_string(),
                linked_field,
                sync_priority,
                compression_priority,
                None, // No temp ID needed since entity exists
            ).await;

            // Store the result (success or error) without failing the whole operation
            results.push(upload_result);
        }

        results
    }
}

// Implement DeleteService<Project> by delegating - Remains the same
#[async_trait]
impl DeleteService<Project> for ProjectServiceImpl {
    fn repository(&self) -> &dyn FindById<Project> { self.delete_service.repository() }
    fn tombstone_repository(&self) -> &dyn TombstoneRepository { self.delete_service.tombstone_repository() }
    fn change_log_repository(&self) -> &dyn ChangeLogRepository { self.delete_service.change_log_repository() }
    fn dependency_checker(&self) -> &dyn DependencyChecker { self.delete_service.dependency_checker() }
    async fn delete( &self, id: Uuid, auth: &AuthContext, options: DeleteOptions ) -> DomainResult<DeleteResult> { self.delete_service.delete(id, auth, options).await }
    async fn batch_delete( &self, ids: &[Uuid], auth: &AuthContext, options: DeleteOptions ) -> DomainResult<crate::domains::core::delete_service::BatchDeleteResult> { self.delete_service.batch_delete(ids, auth, options).await }
    async fn delete_with_dependencies( &self, id: Uuid, auth: &AuthContext ) -> DomainResult<DeleteResult> { self.delete_service.delete_with_dependencies(id, auth).await }
    async fn get_failed_delete_details( &self, batch_result: &crate::domains::core::delete_service::BatchDeleteResult, auth: &AuthContext ) -> DomainResult<Vec<crate::domains::core::delete_service::FailedDeleteDetail<Project>>> { self.delete_service.get_failed_delete_details(batch_result, auth).await }
}

#[async_trait]
impl ProjectService for ProjectServiceImpl {
    // create_project remains mostly the same (core logic unchanged)
    async fn create_project(
        &self,
        new_project: NewProject,
        auth: &AuthContext,
    ) -> ServiceResult<ProjectResponse> {
        if !auth.has_permission(Permission::CreateProjects) {
            return Err(ServiceError::PermissionDenied(
                "User does not have permission to create projects".to_string(),
            ));
        }

        new_project.validate()?;
        self.validate_strategic_goal_exists(new_project.strategic_goal_id).await?;

        // Assume repo.create doesn't use a transaction internally for this basic method
        let created_project = self.repo.create(&new_project, auth).await?;
        let response = ProjectResponse::from_project(created_project);
        // No enrichment needed on *basic* create
        Ok(response)
    }

    // ADDED: Implementation for create_project_with_documents
    async fn create_project_with_documents(
        &self,
        new_project: NewProject,
        documents: Vec<(Vec<u8>, String, Option<String>)>, // (file_data, filename, linked_field)
        document_type_id: Uuid,
        auth: &AuthContext,
    ) -> ServiceResult<(ProjectResponse, Vec<Result<MediaDocumentResponse, ServiceError>>)> {
        // 1. Check Permissions
        if !auth.has_permission(Permission::CreateProjects) {
            return Err(ServiceError::PermissionDenied(
                "User does not have permission to create projects".to_string(),
            ));
        }
        if !documents.is_empty() && !auth.has_permission(Permission::UploadDocuments) {
            return Err(ServiceError::PermissionDenied(
                "User does not have permission to upload documents".to_string(),
            ));
        }

        // 2. Validate Input DTO
        new_project.validate()?;
        self.validate_strategic_goal_exists(new_project.strategic_goal_id).await?;

        // 3. Begin transaction
        let mut tx = self.pool.begin().await
            .map_err(|e| ServiceError::Domain(DomainError::Database(DbError::from(e))))?;

        // 4. Create the project first (within transaction)
        // ASSUMPTION: self.repo has create_with_tx method
        let created_project = match self.repo.create_with_tx(&new_project, auth, &mut tx).await {
            Ok(project) => project,
            Err(e) => {
                let _ = tx.rollback().await; // Rollback on error
                return Err(ServiceError::Domain(e));
            }
        };

        // 5. Commit transaction to ensure project is created
        tx.commit().await
            .map_err(|e| ServiceError::Domain(DomainError::Database(DbError::from(e))))?;

        // 6. Now upload documents (outside transaction, linking to created_project.id)
        let document_results = if !documents.is_empty() {
            self.upload_documents_for_entity(
                created_project.id,
                "projects", // Use correct entity type string
                documents,
                document_type_id,
                SyncPriority::Normal, // Default or could be passed in
                None, // Use default compression priority
                auth,
            ).await
        } else {
            Vec::new()
        };

        // 7. Convert to Response DTO and return with document results
        let response = ProjectResponse::from_project(created_project);
        Ok((response, document_results))
    }


    // get_project_by_id - PRESERVED call to self.enrich_response
    async fn get_project_by_id(
        &self,
        id: Uuid,
        include: Option<&[ProjectInclude]>, // Include parameter IS used
        auth: &AuthContext,
    ) -> ServiceResult<ProjectResponse> {
        if !auth.has_permission(Permission::ViewProjects) {
            return Err(ServiceError::PermissionDenied(
                "User does not have permission to view projects".to_string(),
            ));
        }

        let project = self.repo.find_by_id(id).await?;
        let response = ProjectResponse::from_project(project);

        // PRESERVED: Pass auth context and include options to enrich_response
        self.enrich_response(response, include, auth).await
    }

    // list_projects - PRESERVED enrichment loop
    async fn list_projects(
        &self,
        params: PaginationParams,
        include: Option<&[ProjectInclude]>, // Include parameter IS used
        auth: &AuthContext,
    ) -> ServiceResult<PaginatedResult<ProjectResponse>> {
        if !auth.has_permission(Permission::ViewProjects) {
            return Err(ServiceError::PermissionDenied(
                "User does not have permission to list projects".to_string(),
            ));
        }

        let paginated_result = self.repo.find_all(params).await?;

        // PRESERVED: Enrich items before returning
        let mut enriched_items = Vec::new();
        for item in paginated_result.items {
            let response = ProjectResponse::from_project(item);
            // Pass auth context and include options to enrich_response for each item
            let enriched = self.enrich_response(response, include, auth).await?;
            enriched_items.push(enriched);
        }

        Ok(PaginatedResult::new(
            enriched_items,
            paginated_result.total,
            params, // Pass original params back
        ))
    }

    // update_project remains the same (core logic unchanged)
    async fn update_project(
        &self,
        id: Uuid,
        mut update_data: UpdateProject,
        auth: &AuthContext,
    ) -> ServiceResult<ProjectResponse> {
        if !auth.has_permission(Permission::EditProjects) {
            return Err(ServiceError::PermissionDenied(
                "User does not have permission to edit projects".to_string(),
            ));
        }

        update_data.updated_by_user_id = auth.user_id;
        update_data.validate()?;

        if let Some(opt_sg_id) = update_data.strategic_goal_id {
             self.validate_strategic_goal_exists(opt_sg_id).await?;
        }

        let updated_project = self.repo.update(id, &update_data, auth).await?;
        let response = ProjectResponse::from_project(updated_project);
         // No enrichment needed on update response by default
        Ok(response)
    }

    // delete_project remains the same (core logic unchanged)
    async fn delete_project(
        &self,
        id: Uuid,
        hard_delete: bool,
        auth: &AuthContext,
    ) -> ServiceResult<DeleteResult> {
        let required_permission = if hard_delete {
            Permission::HardDeleteRecord
        } else {
            Permission::DeleteProjects
        };

        if !auth.has_permission(required_permission) {
             return Err(ServiceError::PermissionDenied(format!(
                 "User does not have permission to {} projects",
                 if hard_delete { "hard delete" } else { "delete" }
             )));
        }

        let options = DeleteOptions {
            allow_hard_delete: hard_delete,
            fallback_to_soft_delete: !hard_delete,
            force: false,
        };

        let result = self.delete(id, auth, options).await?;
        Ok(result)
    }

    // --- ADDED: Document integration methods ---

    // Copied from StrategicGoalService, adapted for Project
    async fn upload_document_for_project(
        &self,
        project_id: Uuid,
        file_data: Vec<u8>,
        original_filename: String,
        title: Option<String>,
        document_type_id: Uuid,
        linked_field: Option<String>,
        sync_priority: SyncPriority,
        compression_priority: Option<CompressionPriority>,
        auth: &AuthContext,
    ) -> ServiceResult<MediaDocumentResponse> {
        // 1. Verify project exists
        let _project = self.repo.find_by_id(project_id).await
            .map_err(ServiceError::Domain)?;

        // 2. Check permissions
        if !auth.has_permission(Permission::UploadDocuments) {
            return Err(ServiceError::PermissionDenied(
                "User does not have permission to upload documents".to_string(),
            ));
        }

        // 3. Validate the linked field if specified
        if let Some(field) = &linked_field {
            if !Project::is_document_linkable_field(field) {
                let valid_fields: Vec<String> = Project::document_linkable_fields()
                    .into_iter()
                    .collect();
                    
                return Err(ServiceError::Domain(ValidationError::Custom(format!(
                    "Field '{}' does not support document attachments for projects. Valid fields: {}",
                    field, valid_fields.join(", ")
                )).into()));
            }
        }

        // 4. Delegate to document service, passing linked_field
        let document = self.document_service.upload_document(
            auth,
            file_data,
            original_filename,
            title,
            document_type_id,
            project_id,
            "projects".to_string(), // Correct entity type
            linked_field.clone(), // Pass the validated field name
            sync_priority,
            compression_priority,
            None, // No temp ID for direct uploads
        ).await?;

        // 5. --- NEW: Update entity reference if it was a document-only field ---
        // REMOVED INCORRECT BLOCK: The document service already handles the linked_field correctly
        // by storing it in media_documents.field_identifier. No direct update to the
        // project table is needed or possible according to the schema.
        // if let Some(field_name) = linked_field {
        //     if let Some(metadata) = Project::get_field_metadata(&field_name) {
        //         if metadata.is_document_reference_only {
        //             self.repo.set_document_reference(
        //                 project_id,
        //                 &field_name, // e.g., "proposal_document"
        //                 document.id, // The ID of the newly created MediaDocument
        //                 auth
        //             ).await?;
        //         }
        //     }
        // }

        Ok(document)
    }

    async fn bulk_upload_documents_for_project(
        &self,
        project_id: Uuid,
        files: Vec<(Vec<u8>, String)>,
        title: Option<String>,
        document_type_id: Uuid,
        sync_priority: SyncPriority,
        compression_priority: Option<CompressionPriority>,
        auth: &AuthContext,
    ) -> ServiceResult<Vec<MediaDocumentResponse>> {
        auth.authorize(Permission::UploadDocuments)?;

        let _project = self.repo.find_by_id(project_id).await.map_err(ServiceError::Domain)?;

        let mut results = Vec::new();
        for (file_data, original_filename) in files {
            let result = self.document_service.upload_document(
                auth,
                file_data,
                original_filename,
                title.clone(),
                document_type_id,
                project_id,
                "projects".to_string(),
                None, // No specific linked field for bulk uploads
                sync_priority,
                compression_priority,
                None, // No transaction needed here, handled by document service
            ).await?;
            results.push(result);
        }
        Ok(results)
    }

    // --- New Method Implementations ---

    async fn get_project_statistics(
        &self,
        auth: &AuthContext,
    ) -> ServiceResult<ProjectStatistics> {
        // 1. Check permissions
        auth.authorize(Permission::ViewProjects)?;

        // 2. Get statistics from repository
        let statistics = self.repo.get_project_statistics().await
            .map_err(ServiceError::Domain)?;
        
        Ok(statistics)
    }
    
    async fn get_project_status_breakdown(
        &self,
        auth: &AuthContext,
    ) -> ServiceResult<Vec<ProjectStatusBreakdown>> {
        // 1. Check permissions
        auth.authorize(Permission::ViewProjects)?;

        // 2. Get breakdown from repository
        let breakdown = self.repo.get_project_status_breakdown().await
            .map_err(ServiceError::Domain)?;
        
        Ok(breakdown)
    }
    
    async fn get_project_metadata_counts(
        &self,
        auth: &AuthContext,
    ) -> ServiceResult<ProjectMetadataCounts> {
        // 1. Check permissions
        auth.authorize(Permission::ViewProjects)?;

        // 2. Get counts from repository
        let counts = self.repo.get_project_metadata_counts().await
            .map_err(ServiceError::Domain)?;
        
        Ok(counts)
    }
    
    async fn find_projects_by_status(
        &self,
        status_id: i64,
        params: PaginationParams,
        include: Option<&[ProjectInclude]>,
        auth: &AuthContext,
    ) -> ServiceResult<PaginatedResult<ProjectResponse>> {
        // 1. Check permissions
        auth.authorize(Permission::ViewProjects)?;

        // 2. Find projects by status
        let paginated_result = self.repo.find_by_status(status_id, params).await
            .map_err(ServiceError::Domain)?;

        // 3. Convert and enrich each project
        let mut enriched_items = Vec::new();
        for project in paginated_result.items {
            let response = ProjectResponse::from_project(project);
            let enriched = self.enrich_response(response, include, auth).await?; // Pass auth
            enriched_items.push(enriched);
        }

        // 4. Return paginated result
        Ok(PaginatedResult::new(
            enriched_items,
            paginated_result.total,
            params,
        ))
    }
    
    async fn find_projects_by_responsible_team(
        &self,
        team: &str,
        params: PaginationParams,
        include: Option<&[ProjectInclude]>,
        auth: &AuthContext,
    ) -> ServiceResult<PaginatedResult<ProjectResponse>> {
        // 1. Check permissions
        auth.authorize(Permission::ViewProjects)?;

        // 2. Find projects by team
        let paginated_result = self.repo.find_by_responsible_team(team, params).await
            .map_err(ServiceError::Domain)?;

        // 3. Convert and enrich each project
        let mut enriched_items = Vec::new();
        for project in paginated_result.items {
            let response = ProjectResponse::from_project(project);
            let enriched = self.enrich_response(response, include, auth).await?; // Pass auth
            enriched_items.push(enriched);
        }

        // 4. Return paginated result
        Ok(PaginatedResult::new(
            enriched_items,
            paginated_result.total,
            params,
        ))
    }
    
    async fn get_project_with_document_timeline(
        &self,
        id: Uuid,
        auth: &AuthContext,
    ) -> ServiceResult<ProjectWithDocumentTimeline> {
        // 1. Check permissions
        auth.authorize(Permission::ViewProjects)?;
        auth.authorize(Permission::ViewDocuments)?;

        // 2. Get the project
        let project = self.repo.find_by_id(id).await
            .map_err(ServiceError::Domain)?;
            
        let project_response = ProjectResponse::from_project(project);
        
        // 3. Get all documents for this project
        let documents = self.document_service.list_media_documents_by_related_entity(
            auth,
            "projects",
            id,
            PaginationParams { page: 1, per_page: 100 }, // Use correct field name 'per_page'
            None,
        ).await?.items;
        
        // 4. Organize documents by type/category
        let mut documents_by_type: HashMap<String, Vec<MediaDocumentResponse>> = HashMap::new();
        let mut total_document_count = 0;
        
        for doc in documents {
            // Use field_identifier if available, otherwise use a default category
            let document_type: String = match &doc.field_identifier {
                Some(field) => field.clone(), // Clone here for owned String
                None => "General".to_string(), // Also creates owned String
            };
            
            documents_by_type
                .entry(document_type) // Using owned String is fine
                .or_insert_with(Vec::new)
                .push(doc);
            total_document_count += 1;
        }
        
        // 5. Create and return combined response
        Ok(ProjectWithDocumentTimeline {
            project: project_response,
            documents_by_type,
            total_document_count: total_document_count as u64,
        })
    }
    
    async fn get_project_document_references(
        &self,
        id: Uuid,
        auth: &AuthContext,
    ) -> ServiceResult<Vec<ProjectDocumentReference>> {
        // 1. Check permissions
        auth.authorize(Permission::ViewProjects)?;

        // 2. Verify project exists
        let _project = self.repo.find_by_id(id).await
            .map_err(ServiceError::Domain)?;
            
        // 3. Get document references from repository
        let references = self.repo.get_project_document_references(id).await
            .map_err(ServiceError::Domain)?;
            
        Ok(references)
    }
    
    async fn search_projects(
        &self,
        query: &str,
        params: PaginationParams,
        include: Option<&[ProjectInclude]>,
        auth: &AuthContext,
    ) -> ServiceResult<PaginatedResult<ProjectResponse>> {
        // 1. Check permissions
        auth.authorize(Permission::ViewProjects)?;

        // 2. Validate query length
        if query.trim().len() < 2 {
            return Err(ServiceError::Domain(
                DomainError::Validation(ValidationError::custom("Search query must be at least 2 characters"))
            ));
        }

        // 3. Search projects
        let paginated_result = self.repo.search_projects(query, params).await
            .map_err(ServiceError::Domain)?;

        // 4. Convert and enrich each project
        let mut enriched_items = Vec::new();
        for project in paginated_result.items {
            let response = ProjectResponse::from_project(project);
            let enriched = self.enrich_response(response, include, auth).await?; // Pass auth
            enriched_items.push(enriched);
        }

        // 5. Return paginated result
        Ok(PaginatedResult::new(
            enriched_items,
            paginated_result.total,
            params,
        ))
    }
}

// /Users/sagarshrestha/ipad_rust_core copy/src/domains/sync/cloud_storage.rs
use crate::errors::{ServiceError, ServiceResult};
use crate::domains::sync::types::{
    FetchChangesResponse, PushChangesResponse, PushPayload,
};
use async_trait::async_trait;
use serde::{Serialize, Deserialize};
use std::path::Path;
use uuid::Uuid;
use log::{info, warn, error, debug};
use reqwest::Client;
use std::time::Duration;
use reqwest::multipart::{Form, Part};
use chrono::Utc;

/// Trait for cloud storage service operations
#[async_trait]
pub trait CloudStorageService: Send + Sync {
    /// Get changes from remote storage since a specific sync token
    async fn get_changes_since(&self, api_token: &str, sync_token: Option<String>) -> ServiceResult<FetchChangesResponse>;
    
    /// Push local changes to remote storage
    async fn push_changes(&self, api_token: &str, payload: PushPayload) -> ServiceResult<PushChangesResponse>;
    
    /// Upload a document to cloud storage
    async fn upload_document(&self, document_id: Uuid, local_path: &str, mime_type: &str, size_bytes: u64) -> ServiceResult<String>;
    
    /// Download a document from cloud storage
    async fn download_document(&self, document_id: Uuid, blob_key: &str) -> ServiceResult<(String, u64, bool)>;
}

/// Implementation of CloudStorageService that communicates with an API server
pub struct ApiCloudStorageService {
    client: Client,
    base_url: String,
    local_storage_path: String,
}

impl ApiCloudStorageService {
    /// Create a new API-based cloud storage service
    pub fn new(base_url: &str, local_storage_path: &str) -> Self {
        let client = Client::builder()
            .timeout(Duration::from_secs(120)) // 2 minute timeout
            .connect_timeout(Duration::from_secs(10))
            .build()
            .unwrap_or_default();
            
        Self {
            client,
            base_url: base_url.to_string(),
            local_storage_path: local_storage_path.to_string(),
        }
    }
    
    /// Get the authorization header
    fn auth_header(&self, api_token: &str) -> String {
        format!("Bearer {}", api_token)
    }
    
    /// Ensure the local storage directory exists
    fn ensure_storage_dir(&self) -> ServiceResult<()> {
        let path = Path::new(&self.local_storage_path);
        if !path.exists() {
            std::fs::create_dir_all(path)
                .map_err(|e| ServiceError::External(format!("Failed to create local storage directory: {}", e)))?;
        }
        Ok(())
    }
}

#[async_trait]
impl CloudStorageService for ApiCloudStorageService {
    async fn get_changes_since(&self, api_token: &str, sync_token: Option<String>) -> ServiceResult<FetchChangesResponse> {
        debug!("Fetching changes since token: {:?}", sync_token);
        
        // Build the URL with optional sync token
        let url = if let Some(token) = &sync_token {
            format!("{}/api/sync/changes?since={}", self.base_url, token)
        } else {
            format!("{}/api/sync/changes", self.base_url)
        };
        
        // Make the API request
        let response = self.client.get(&url)
            .header("Authorization", self.auth_header(api_token))
            .send()
            .await
            .map_err(|e| ServiceError::External(format!("Failed to fetch changes: {}", e)))?;
            
        // Check status and parse response
        if response.status().is_success() {
            let changes_response = response.json::<FetchChangesResponse>()
                .await
                .map_err(|e| ServiceError::External(format!("Failed to parse changes response: {}", e)))?;
                
            Ok(changes_response)
        } else {
            let status = response.status();
            let error_text = response.text().await
                .unwrap_or_else(|_| "Unable to get error details".to_string());
                
            Err(ServiceError::External(format!("Server returned error {}: {}", status, error_text)))
        }
    }
    
    async fn push_changes(&self, api_token: &str, payload: PushPayload) -> ServiceResult<PushChangesResponse> {
        debug!("Pushing {} changes and {} deletions", payload.changes.len(), payload.deletions.len());
        
        let url = format!("{}/api/sync/push", self.base_url);
        
        // Make the API request
        let response = self.client.post(&url)
            .header("Authorization", self.auth_header(api_token))
            .json(&payload)
            .send()
            .await
            .map_err(|e| ServiceError::External(format!("Failed to push changes: {}", e)))?;
            
        // Check status and parse response
        if response.status().is_success() {
            let push_response = response.json::<PushChangesResponse>()
                .await
                .map_err(|e| ServiceError::External(format!("Failed to parse push response: {}", e)))?;
                
            Ok(push_response)
        } else {
            let status = response.status();
            let error_text = response.text().await
                .unwrap_or_else(|_| "Unable to get error details".to_string());
                
            Err(ServiceError::External(format!("Server returned error {}: {}", status, error_text)))
        }
    }
    
    async fn upload_document(&self, document_id: Uuid, local_path: &str, mime_type: &str, size_bytes: u64) -> ServiceResult<String> {
        debug!("Uploading document {} from {}", document_id, local_path);
        
        let doc_id_str = document_id.to_string();
        let url = format!("{}/api/documents/upload/{}", self.base_url, doc_id_str);
        
        // Read the file
        let file_content = tokio::fs::read(local_path)
            .await
            .map_err(|e| ServiceError::External(format!("Failed to read local file: {}", e)))?;
            
        // Prepare the multipart form
        let part = Part::bytes(file_content)
            .file_name(Path::new(local_path).file_name().unwrap_or_default().to_string_lossy().into_owned())
            .mime_str(mime_type)
            .map_err(|e| ServiceError::External(format!("Invalid MIME type: {}", e)))?;
            
        let form = Form::new()
            .part("file", part)
            .text("documentId", doc_id_str.clone());
            
        // Make the API request
        let response = self.client.post(&url)
            .multipart(form)
            .send()
            .await
            .map_err(|e| ServiceError::External(format!("Failed to upload document: {}", e)))?;
            
        // Check status and parse response
        if response.status().is_success() {
            #[derive(Deserialize)]
            struct UploadResponse {
                blob_key: String,
            }
            
            let upload_response = response.json::<UploadResponse>()
                .await
                .map_err(|e| ServiceError::External(format!("Failed to parse upload response: {}", e)))?;
                
            Ok(upload_response.blob_key)
        } else {
            let status = response.status();
            let error_text = response.text().await
                .unwrap_or_else(|_| "Unable to get error details".to_string());
                
            Err(ServiceError::External(format!("Server returned error {}: {}", status, error_text)))
        }
    }
    
    async fn download_document(&self, document_id: Uuid, blob_key: &str) -> ServiceResult<(String, u64, bool)> {
        debug!("Downloading document {} with key {}", document_id, blob_key);
        
        // Ensure local storage directory exists
        self.ensure_storage_dir()?;
        
        let doc_id_str = document_id.to_string();
        let url = format!("{}/api/documents/download/{}", self.base_url, blob_key);
        
        // Make the API request
        let response = self.client.get(&url)
            .send()
            .await
            .map_err(|e| ServiceError::External(format!("Failed to download document: {}", e)))?;
            
        // Check status and process response
        if response.status().is_success() {
            // Get content type to determine compression
            let content_type = response.headers()
                .get(reqwest::header::CONTENT_TYPE)
                .and_then(|v| v.to_str().ok())
                .unwrap_or("application/octet-stream");
                
            let is_compressed = content_type.contains("compressed") || 
                               content_type.contains("zip") ||
                               response.headers()
                                      .get("X-Compressed")
                                      .and_then(|v| v.to_str().ok())
                                      .map(|v| v == "true")
                                      .unwrap_or(false);
            
            // Get content length
            let size = response.content_length().unwrap_or(0);
            
            // Determine file extension from content type
            let extension = if content_type.contains("jpeg") || content_type.contains("jpg") {
                "jpg"
            } else if content_type.contains("png") {
                "png"
            } else if content_type.contains("pdf") {
                "pdf"
            } else if content_type.contains("zip") || is_compressed {
                "zip"
            } else {
                "bin" // Default binary extension
            };
            
            // Create local path
            let filename = if is_compressed {
                format!("{}_compressed.{}", doc_id_str, extension)
            } else {
                format!("{}.{}", doc_id_str, extension)
            };
            
            let local_path = format!("{}/{}", self.local_storage_path, filename);
            
            // Download and save the file
            let bytes = response.bytes()
                .await
                .map_err(|e| ServiceError::External(format!("Failed to read document bytes: {}", e)))?;
                
            tokio::fs::write(&local_path, &bytes)
                .await
                .map_err(|e| ServiceError::External(format!("Failed to write document to disk: {}", e)))?;
                
            Ok((local_path, size, is_compressed))
        } else {
            let status = response.status();
            let error_text = response.text().await
                .unwrap_or_else(|_| "Unable to get error details".to_string());
                
            Err(ServiceError::External(format!("Server returned error {}: {}", status, error_text)))
        }
    }
}

/// Mock implementation for testing
#[cfg(test)]
pub struct MockCloudStorageService {
    base_path: String,
}

#[cfg(test)]
impl MockCloudStorageService {
    pub fn new(base_path: &str) -> Self {
        Self { base_path: base_path.to_string() }
    }
}

#[cfg(test)]
#[async_trait]
impl CloudStorageService for MockCloudStorageService {
    async fn get_changes_since(&self, _api_token: &str, _sync_token: Option<String>) -> ServiceResult<FetchChangesResponse> {
        // Return empty response for tests
        Ok(FetchChangesResponse {
            batch_id: String::new(),
            changes: Vec::new(),
            tombstones: None,
            has_more: false,
            server_timestamp: Utc::now(),
            next_batch_hint: None,
        })
    }
    
    async fn push_changes(&self, _api_token: &str, payload: PushPayload) -> ServiceResult<PushChangesResponse> {
        // Mock successful push
        let change_ids = payload.changes.iter()
            .map(|change| change.operation_id)
            .collect();
            
        let tombstone_ids = payload.deletions.iter()
            .map(|tombstone| tombstone.id)
            .collect();
            
        Ok(PushChangesResponse {
            batch_id: payload.batch_id.unwrap_or_else(|| Uuid::new_v4().to_string()),
            changes_accepted: change_ids,
            changes_rejected: Vec::new(),
            conflicts_detected: false,
            conflicts: None,
            server_timestamp: Utc::now(),
        })
    }
    
    async fn upload_document(&self, document_id: Uuid, _local_path: &str, _mime_type: &str, _size_bytes: u64) -> ServiceResult<String> {
        // Mock successful upload by returning a fake blob key
        Ok(format!("test_blob_{}", document_id))
    }
    
    async fn download_document(&self, document_id: Uuid, _blob_key: &str) -> ServiceResult<(String, u64, bool)> {
        // Generate a fake local path pointing to a non-existent file for testing
        let local_path = format!("{}/{}.bin", self.base_path, document_id);
        
        // Mock successful download (0 bytes, not compressed)
        Ok((local_path, 0, false))
    }
}

// /Users/sagarshrestha/ipad_rust_core copy/src/domains/sync/utils.rs
use crate::errors::ServiceError;

/// Utility function to sanitize SQL identifiers
pub fn sanitize_identifier(identifier: &str) -> String {
    // Only allow alphanumerics and underscores in identifiers
    // This prevents SQL injection in dynamic queries
    let safe_id: String = identifier.chars()
        .filter(|c| c.is_alphanumeric() || *c == '_')
        .collect();
    
    // Ensure identifier is not empty after filtering
    if safe_id.is_empty() {
        return "_invalid".to_string();
    }
    
    // Prevent numeric-only identifiers (not valid in SQL)
    if safe_id.chars().all(|c| c.is_numeric()) {
        return format!("_{}", safe_id);
    }
    
    safe_id
}

/// Utility function to validate entity type
pub fn validate_entity_type(entity_type: &str) -> Result<(), ServiceError> {
    // Entity type must be in the allowed list
    const ALLOWED_ENTITIES: &[&str] = &[
        "media_documents", "projects", "activities", "workshops", 
        "strategic_goals", "participants", "livelihoods", "donors",
        "status_types", "document_types", "subsequent_grants", "project_funding"
    ];
    
    let sanitized = sanitize_identifier(entity_type);
    if sanitized != entity_type {
        return Err(ServiceError::InvalidEntity(format!(
            "Entity type contains invalid characters: {}", entity_type
        )));
    }
    
    if !ALLOWED_ENTITIES.contains(&entity_type) {
        return Err(ServiceError::InvalidEntity(format!(
            "Unknown entity type: {}", entity_type
        )));
    }
    
    Ok(())
}

// Optional: Utility function for logging sync operations
pub fn format_sync_operation(
    operation: &str,
    entity_type: &str,
    entity_id: &str,
    status: &str,
    error: Option<&str>,
) -> String {
    if let Some(err) = error {
        format!("{} {}:{} - {} - Error: {}", operation, entity_type, entity_id, status, err)
    } else {
        format!("{} {}:{} - {}", operation, entity_type, entity_id, status)
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_sanitize_identifier() {
        assert_eq!(sanitize_identifier("valid_table"), "valid_table");
        assert_eq!(sanitize_identifier("projects"), "projects");
        assert_eq!(sanitize_identifier("DROP TABLE users;"), "DROPTABLEusers");
        assert_eq!(sanitize_identifier("123"), "_123");
        assert_eq!(sanitize_identifier(""), "_invalid");
        assert_eq!(sanitize_identifier("!@#$"), "_invalid");
    }

    #[test]
    fn test_validate_entity_type() {
        assert!(validate_entity_type("projects").is_ok());
        assert!(validate_entity_type("media_documents").is_ok());
        assert!(validate_entity_type("invalid_entity").is_err());
        assert!(validate_entity_type("DROP TABLE;").is_err());
    }
}

// /Users/sagarshrestha/ipad_rust_core copy/src/domains/user/repository.rs
use crate::errors::{DbError, DomainError, DomainResult};
use crate::domains::user::types::{User, NewUser, UpdateUser, UserRow};
use crate::auth::AuthContext;
use crate::types::ChangeLogOperationType;
use uuid::Uuid;
use chrono::Utc;
use sqlx::{SqlitePool, query, query_as, query_scalar, Transaction, Sqlite};
use async_trait::async_trait;
use std::sync::Arc;
use crate::domains::sync::repository::ChangeLogRepository;
use crate::domains::sync::types::{ChangeLogEntry, ChangeOperationType};
use crate::domains::core::repository::{HardDeletable, FindById};

/// User repository trait
#[async_trait]
pub trait UserRepository: Send + Sync + FindById<User> + HardDeletable {
    /// Find a user by ID
    // async fn find_by_id(&self, id: Uuid) -> DomainResult<User>; // Defined by FindById
    
    /// Find a user by email
    async fn find_by_email(&self, email: &str) -> DomainResult<User>;
    
    /// Find all users
    async fn find_all(&self) -> DomainResult<Vec<User>>;
    
    /// Create a new user
    async fn create(&self, user: NewUser, auth: &AuthContext) -> DomainResult<User>;
    
    /// Update an existing user
    async fn update(&self, id: Uuid, update: UpdateUser, auth: &AuthContext) -> DomainResult<User>;
    
    /// Update last login timestamp
    async fn update_last_login(&self, id: Uuid) -> DomainResult<()>;
    
    /// Check if email is unique
    async fn is_email_unique(&self, email: &str, exclude_id: Option<Uuid>) -> DomainResult<bool>;
}

/// SQLite implementation of UserRepository
pub struct SqliteUserRepository {
    pool: SqlitePool,
    change_log_repo: Arc<dyn ChangeLogRepository + Send + Sync>,
}

impl SqliteUserRepository {
    /// Create a new repository instance
    pub fn new(pool: SqlitePool, change_log_repo: Arc<dyn ChangeLogRepository + Send + Sync>) -> Self {
        Self { pool, change_log_repo }
    }
    
    // Helper function to map UserRow to User entity
    fn map_row_to_entity(row: UserRow) -> DomainResult<User> {
        row.into_entity()
    }
    
    // Helper to find user by ID within a transaction (needed for updates)
    async fn find_by_id_with_tx<'t>(
        &self,
        id: Uuid,
        tx: &mut Transaction<'t, Sqlite>,
    ) -> DomainResult<User> {
        let row = query_as::<_, UserRow>(
            "SELECT * FROM users WHERE id = ? AND deleted_at IS NULL"
        )
        .bind(id.to_string())
        .fetch_optional(&mut **tx)
        .await
        .map_err(|e| DomainError::Database(DbError::from(e)))?
        .ok_or_else(|| DomainError::EntityNotFound("User".to_string(), id))?;
        
        Self::map_row_to_entity(row)
    }
    
    // Helper to log change entries consistently
    async fn log_change_entry<'t>(
        &self,
        entry: ChangeLogEntry,
        tx: &mut Transaction<'t, Sqlite>,
    ) -> DomainResult<()> {
        self.change_log_repo.create_change_log_with_tx(&entry, tx).await
    }
}

// Implement FindById for SqliteUserRepository
#[async_trait]
impl FindById<User> for SqliteUserRepository {
    async fn find_by_id(&self, id: Uuid) -> DomainResult<User> {
        let row = query_as::<_, UserRow>(
            "SELECT * FROM users WHERE id = ? AND deleted_at IS NULL"
        )
        .bind(id.to_string())
        .fetch_optional(&self.pool)
        .await
        .map_err(|e| DomainError::Database(DbError::from(e)))?
        .ok_or_else(|| DomainError::EntityNotFound("User".to_string(), id))?;
        
        Self::map_row_to_entity(row)
    }
}

// Implement HardDeletable for SqliteUserRepository
#[async_trait]
impl HardDeletable for SqliteUserRepository {
    fn entity_name(&self) -> &'static str {
        "users"
    }

    async fn hard_delete_with_tx(
        &self,
        id: Uuid,
        _auth: &AuthContext, // Auth context might be used later for checks
        tx: &mut Transaction<'_, Sqlite>,
    ) -> DomainResult<()> {
         // Check if user exists first to return correct error
        let _ = query_scalar::<_, String>("SELECT id FROM users WHERE id = ?")
            .bind(id.to_string())
            .fetch_optional(&mut **tx)
            .await
            .map_err(|e| DomainError::Database(DbError::from(e)))?
            .ok_or_else(|| DomainError::EntityNotFound(self.entity_name().to_string(), id))?;
            
        // Hard delete the user
        let result = query("DELETE FROM users WHERE id = ?")
            .bind(id.to_string())
            .execute(&mut **tx)
            .await
            .map_err(|e| DomainError::Database(DbError::from(e)))?;
            
        // Check rows affected to confirm deletion (optional but good practice)
        if result.rows_affected() == 0 {
            // Should not happen if fetch_optional found the user, but handle defensively
            Err(DomainError::EntityNotFound(self.entity_name().to_string(), id))
        } else {
             // No logging here - BaseDeleteService handles it
            Ok(())
        }
    }
    
    // Standalone hard_delete is removed as it's handled by the service
    async fn hard_delete(&self, id: Uuid, auth: &AuthContext) -> DomainResult<()> {
        // This implementation is now effectively unused, but kept to satisfy 
        // potential direct calls if the service pattern isn't fully adopted yet.
        // It lacks the Tombstone + ChangeLog from BaseDeleteService.
        log::warn!("Direct hard_delete called on UserRepository for {}, bypassing BaseDeleteService logic.", id);
        let mut tx = self.pool.begin().await.map_err(DbError::from)?;
        match self.hard_delete_with_tx(id, auth, &mut tx).await {
            Ok(()) => { tx.commit().await.map_err(DbError::from)?; Ok(()) },
            Err(e) => { let _ = tx.rollback().await; Err(e) }
        }
    }
}

#[async_trait]
impl UserRepository for SqliteUserRepository {
    async fn find_by_email(&self, email: &str) -> DomainResult<User> {
        let row = query_as::<_, UserRow>(
            "SELECT * FROM users WHERE email = ? AND deleted_at IS NULL"
        )
        .bind(email)
        .fetch_optional(&self.pool)
        .await
        .map_err(|e| DomainError::Database(DbError::from(e)))?
        .ok_or_else(|| DomainError::Internal(format!("User not found with email: {}", email)))?;
        
        row.into_entity()
    }
    
    async fn find_all(&self) -> DomainResult<Vec<User>> {
        let rows = query_as::<_, UserRow>(
            "SELECT * FROM users WHERE deleted_at IS NULL ORDER BY name"
        )
        .fetch_all(&self.pool)
        .await
        .map_err(|e| DomainError::Database(DbError::from(e)))?;
        
        let mut users = Vec::with_capacity(rows.len());
        for row in rows {
            users.push(row.into_entity()?);
        }
        
        Ok(users)
    }
    
    async fn create(&self, user: NewUser, auth: &AuthContext) -> DomainResult<User> {
        // Check if email is unique
        if !self.is_email_unique(&user.email, None).await? {
            return Err(DomainError::Validation(
                crate::errors::ValidationError::unique("email")
            ));
        }
        
        // --- Start Transaction --- 
        let mut tx = self.pool.begin().await.map_err(DbError::from)?;

        let create_result = async {
            // Generate ID
            let id = Uuid::new_v4();
            let now = Utc::now().to_rfc3339();
            let now_dt = Utc::now(); // For logging
            let user_uuid = auth.user_id; // Capture UUID
            let device_uuid: Option<Uuid> = auth.device_id.parse().ok(); // Capture device UUID
            
            // Set created_by to the authenticated user if not specified
            let created_by = user.created_by_user_id
                .unwrap_or(auth.user_id)
                .to_string();
            
            // Default to active if not specified
            let active = if user.active { 1 } else { 0 };
            
            // Insert user
            query(
                "INSERT INTO users (
                    id, email, email_updated_at, email_updated_by,
                    password_hash, name, name_updated_at, name_updated_by,
                    role, role_updated_at, role_updated_by,
                    active, active_updated_at, active_updated_by,
                    created_at, updated_at, created_by_user_id, updated_by_user_id
                ) VALUES (
                    ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?
                )"
            )
            .bind(id.to_string())
            .bind(&user.email)
            .bind(&now)
            .bind(auth.user_id.to_string())
            .bind(&user.password)  // Note: This should be hashed before calling repository
            .bind(&user.name)
            .bind(&now)
            .bind(auth.user_id.to_string())
            .bind(&user.role)
            .bind(&now)
            .bind(auth.user_id.to_string())
            .bind(active)
            .bind(&now)
            .bind(auth.user_id.to_string())
            .bind(&now)
            .bind(&now)
            .bind(created_by)
            .bind(auth.user_id.to_string())
            .execute(&mut *tx) // Execute within transaction
            .await
            .map_err(|e| DomainError::Database(DbError::from(e)))?;
            
            // Create changelog entry
            let entry = ChangeLogEntry {
                operation_id: Uuid::new_v4(), // Generate new op ID
                entity_table: self.entity_name().to_string(), // Use entity_name()
                entity_id: id,
                operation_type: ChangeOperationType::Create,
                field_name: None,
                old_value: None,
                new_value: None,
                timestamp: now_dt,
                user_id: user_uuid,
                device_id: device_uuid,
                document_metadata: None,
                sync_batch_id: None,
                processed_at: None,
                sync_error: None,
            };
            self.log_change_entry(entry, &mut tx).await?;
            
            // Return the ID for fetching outside the transaction
            Ok(id)
        }.await;

        // --- Commit or Rollback --- 
        match create_result {
            Ok(created_id) => {
                tx.commit().await.map_err(DbError::from)?;
                // Fetch the newly created record outside the transaction
                self.find_by_id(created_id).await
            },
            Err(e) => {
                let _ = tx.rollback().await; // Ensure rollback on error
                Err(e)
            }
        }
    }
    
    async fn update(&self, id: Uuid, update: UpdateUser, auth: &AuthContext) -> DomainResult<User> {
        // Check if user exists
        // let user = self.find_by_id(id).await?; // Fetch within transaction later
        
        // Begin transaction
        let mut tx = self.pool.begin().await
            .map_err(|e| DomainError::Database(DbError::from(e)))?;
            
        // Fetch the user within the transaction to get the old state accurately
        let user = self.find_by_id_with_tx(id, &mut tx).await?;
        
        let now = Utc::now().to_rfc3339();
        let now_dt = Utc::now(); // For logging
        let user_uuid = auth.user_id;
        let device_uuid: Option<Uuid> = auth.device_id.parse().ok();
        
        // Update email if provided
        if let Some(email) = &update.email {
            // Check if email is unique
            if email != &user.email && !self.is_email_unique(email, Some(id)).await? {
                return Err(DomainError::Validation(
                    crate::errors::ValidationError::unique("email")
                ));
            }
            
            query(
                "UPDATE users SET email = ?, email_updated_at = ?, email_updated_by = ? WHERE id = ?"
            )
            .bind(email)
            .bind(&now)
            .bind(update.updated_by_user_id.to_string())
            .bind(id.to_string())
            .execute(&mut *tx)
            .await
            .map_err(|e| DomainError::Database(DbError::from(e)))?;
            
            // Log Change
            let entry = ChangeLogEntry {
                operation_id: Uuid::new_v4(),
                entity_table: self.entity_name().to_string(),
                entity_id: id,
                operation_type: ChangeOperationType::Update,
                field_name: Some("email".to_string()),
                old_value: Some(serde_json::to_string(&user.email).unwrap_or_default()),
                new_value: Some(serde_json::to_string(email).unwrap_or_default()),
                timestamp: now_dt,
                user_id: user_uuid,
                device_id: device_uuid.clone(),
                document_metadata: None,
                sync_batch_id: None,
                processed_at: None,
                sync_error: None,
            };
            self.log_change_entry(entry, &mut tx).await?;
        }
        
        // Update password if provided
        if let Some(password) = &update.password {
            query(
                "UPDATE users SET password_hash = ? WHERE id = ?"
            )
            .bind(password)
            .bind(id.to_string())
            .execute(&mut *tx)
            .await
            .map_err(|e| DomainError::Database(DbError::from(e)))?;
            
            // No changelog for password updates for security reasons
        }
        
        // Update name if provided
        if let Some(name) = &update.name {
            query(
                "UPDATE users SET name = ?, name_updated_at = ?, name_updated_by = ? WHERE id = ?"
            )
            .bind(name)
            .bind(&now)
            .bind(update.updated_by_user_id.to_string())
            .bind(id.to_string())
            .execute(&mut *tx)
            .await
            .map_err(|e| DomainError::Database(DbError::from(e)))?;
            
            // Log Change
            let entry = ChangeLogEntry {
                operation_id: Uuid::new_v4(),
                entity_table: self.entity_name().to_string(),
                entity_id: id,
                operation_type: ChangeOperationType::Update,
                field_name: Some("name".to_string()),
                old_value: Some(serde_json::to_string(&user.name).unwrap_or_default()),
                new_value: Some(serde_json::to_string(name).unwrap_or_default()),
                timestamp: now_dt,
                user_id: user_uuid,
                device_id: device_uuid.clone(),
                document_metadata: None,
                sync_batch_id: None,
                processed_at: None,
                sync_error: None,
            };
            self.log_change_entry(entry, &mut tx).await?;
        }
        
        // Update role if provided
        if let Some(role) = &update.role {
            query(
                "UPDATE users SET role = ?, role_updated_at = ?, role_updated_by = ? WHERE id = ?"
            )
            .bind(role)
            .bind(&now)
            .bind(update.updated_by_user_id.to_string())
            .bind(id.to_string())
            .execute(&mut *tx)
            .await
            .map_err(|e| DomainError::Database(DbError::from(e)))?;
            
            // Log Change
            let entry = ChangeLogEntry {
                operation_id: Uuid::new_v4(),
                entity_table: self.entity_name().to_string(),
                entity_id: id,
                operation_type: ChangeOperationType::Update,
                field_name: Some("role".to_string()),
                old_value: Some(serde_json::to_string(user.role.as_str()).unwrap_or_default()),
                new_value: Some(serde_json::to_string(role).unwrap_or_default()),
                timestamp: now_dt,
                user_id: user_uuid,
                device_id: device_uuid.clone(),
                document_metadata: None,
                sync_batch_id: None,
                processed_at: None,
                sync_error: None,
            };
            self.log_change_entry(entry, &mut tx).await?;
        }
        
        // Update active if provided
        if let Some(active) = update.active {
            let active_value = if active { 1 } else { 0 };
            let current_active = if user.active { 1 } else { 0 };
            
            query(
                "UPDATE users SET active = ?, active_updated_at = ?, active_updated_by = ? WHERE id = ?"
            )
            .bind(active_value)
            .bind(&now)
            .bind(update.updated_by_user_id.to_string())
            .bind(id.to_string())
            .execute(&mut *tx)
            .await
            .map_err(|e| DomainError::Database(DbError::from(e)))?;
            
            // Log Change
            let entry = ChangeLogEntry {
                operation_id: Uuid::new_v4(),
                entity_table: self.entity_name().to_string(),
                entity_id: id,
                operation_type: ChangeOperationType::Update,
                field_name: Some("active".to_string()),
                old_value: Some(serde_json::to_string(&user.active).unwrap_or_default()), // Log bool directly
                new_value: Some(serde_json::to_string(&active).unwrap_or_default()), // Log bool directly
                timestamp: now_dt,
                user_id: user_uuid,
                device_id: device_uuid.clone(),
                document_metadata: None,
                sync_batch_id: None,
                processed_at: None,
                sync_error: None,
            };
            self.log_change_entry(entry, &mut tx).await?;
        }
        
        // Update the updated_at and updated_by fields
        query(
            "UPDATE users SET updated_at = ?, updated_by_user_id = ? WHERE id = ?"
        )
        .bind(&now)
        .bind(update.updated_by_user_id.to_string())
        .bind(id.to_string())
        .execute(&mut *tx)
        .await
        .map_err(|e| DomainError::Database(DbError::from(e)))?;
        
        // Commit the transaction
        tx.commit().await
            .map_err(|e| DomainError::Database(DbError::from(e)))?;
        
        // Return the updated user
        self.find_by_id(id).await
    }
    
    async fn update_last_login(&self, id: Uuid) -> DomainResult<()> {
        let now = Utc::now().to_rfc3339();
        
        query("UPDATE users SET last_login = ? WHERE id = ?")
            .bind(&now)
            .bind(id.to_string())
            .execute(&self.pool)
            .await
            .map_err(|e| DomainError::Database(DbError::from(e)))?;
            
        Ok(())
    }
    
    async fn is_email_unique(&self, email: &str, exclude_id: Option<Uuid>) -> DomainResult<bool> {
        let count: i64 = match exclude_id {
            Some(id) => {
                query_scalar(
                    "SELECT COUNT(*) FROM users WHERE email = ? AND id != ? AND deleted_at IS NULL"
                )
                .bind(email)
                .bind(id.to_string())
                .fetch_one(&self.pool)
                .await
                .map_err(|e| DomainError::Database(DbError::from(e)))?
            },
            None => {
                query_scalar(
                    "SELECT COUNT(*) FROM users WHERE email = ? AND deleted_at IS NULL"
                )
                .bind(email)
                .fetch_one(&self.pool)
                .await
                .map_err(|e| DomainError::Database(DbError::from(e)))?
            }
        };
        
        Ok(count == 0)
    }
}