use crate::errors::{DomainResult, DomainError, DbError};
use crate::auth::AuthContext;
use crate::types::UserRole;
use crate::domains::sync::types::{Tombstone, ChangeLogEntry, ChangeOperationType};
use crate::domains::sync::repository::{TombstoneRepository, ChangeLogRepository};
use crate::domains::core::dependency_checker::DependencyChecker;
use crate::domains::core::repository::{DeleteResult, HardDeletable, SoftDeletable, FindById};
use crate::domains::document::repository::MediaDocumentRepository;
use uuid::Uuid;
use chrono::Utc;
use async_trait::async_trait;
use sqlx::{SqlitePool, Transaction, Sqlite};
use std::sync::Arc;
use std::collections::HashMap;
use serde::{Serialize, Deserialize};
use serde_json; // Added for JSON manipulation
use sqlx::Row;
use tokio::sync::RwLock; // Added for RwLock

/// Helper to convert a device_id string to Uuid
fn parse_device_id(device_id_str: &str) -> Option<Uuid> {
    Uuid::parse_str(device_id_str).ok()
}

/// Holds information for a pending document file deletion.
#[derive(Debug, Clone)]
struct DocumentDeletion {
    document_id: Uuid,
    file_path: String, // file_path is NOT NULL in media_documents and file_deletion_queue
    compressed_file_path: Option<String>,
}

/// Manages document deletions that are pending based on the outcome of a main database transaction.
#[derive(Debug)]
pub struct PendingDeletionManager {
    pool: SqlitePool,
    pending_deletions: RwLock<HashMap<Uuid, Vec<DocumentDeletion>>>,
}

impl PendingDeletionManager {
    /// Creates a new PendingDeletionManager.
    pub fn new(pool: SqlitePool) -> Self {
        Self {
            pool,
            pending_deletions: RwLock::new(HashMap::new()),
        }
    }

    /// Adds a document deletion to the pending list for a given operation ID.
    pub async fn add_pending(&self, operation_id: Uuid, deletion: DocumentDeletion) {
        let mut pending = self.pending_deletions.write().await;
        pending.entry(operation_id).or_default().push(deletion);
    }

    /// Commits pending deletions for an operation ID to the file_deletion_queue.
    /// This is typically called after the main database transaction has successfully committed.
    pub async fn commit_deletions(
        &self, 
        operation_id: Uuid, 
        requested_by_user_id: Uuid,
        requested_by_device_id: &str, // Added parameter
    ) -> DomainResult<()> {
        let mut pending_map = self.pending_deletions.write().await;
        if let Some(deletions_to_commit) = pending_map.remove(&operation_id) {
            if deletions_to_commit.is_empty() {
                return Ok(());
            }

            // Start a new transaction for inserting into the file_deletion_queue
            let mut queue_tx = self.pool.begin().await.map_err(DbError::from)?;
            let requested_by_user_id_str = requested_by_user_id.to_string();
            // let requested_by_device_id_str = requested_by_device_id.to_string(); // device_id is already &str

            for deletion in deletions_to_commit {
                let queue_id_str = Uuid::new_v4().to_string();
                let doc_id_str = deletion.document_id.to_string();
                let now_str = Utc::now().to_rfc3339();

                sqlx::query!(
                    r#"
                    INSERT INTO file_deletion_queue (
                        id,
                        document_id,
                        file_path,
                        compressed_file_path,
                        requested_at,
                        requested_by,
                        requested_by_device_id, 
                        grace_period_seconds
                    )
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                    "#,
                    queue_id_str,
                    doc_id_str,
                    deletion.file_path, // Is String, matches NOT NULL schema
                    deletion.compressed_file_path,
                    now_str,
                    requested_by_user_id_str,
                    requested_by_device_id, // Bind new parameter
                    86400 // 24 hour grace period
                )
                .execute(&mut *queue_tx) // Changed &mut **tx to &mut *queue_tx
                .await
                .map_err(DbError::from)?;
            }
            queue_tx.commit().await.map_err(DbError::from)?;
        }
        Ok(())
    }

    /// Discards pending deletions for an operation ID.
    /// This is typically called if the main database transaction has rolled back.
    pub async fn discard_deletions(&self, operation_id: Uuid) {
        let mut pending = self.pending_deletions.write().await;
        pending.remove(&operation_id);
    }
}

/// Delete options for controlling deletion behavior
#[derive(Debug, Clone)]
pub struct DeleteOptions {
    /// Whether to allow hard delete
    pub allow_hard_delete: bool,
    
    /// Whether to fall back to soft delete if hard delete fails
    pub fallback_to_soft_delete: bool,
    
    /// Whether to bypass dependency checks (admin only)
    pub force: bool,
}

impl Default for DeleteOptions {
    fn default() -> Self {
        Self {
            allow_hard_delete: false,
            fallback_to_soft_delete: true,
            force: false,
        }
    }
}

/// Details about a single record that failed to delete
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct FailedDeleteDetail<E>
where 
    E: Send + Sync,
{
    pub id: Uuid,
    pub entity_data: Option<E>,
    pub entity_type: String,
    pub reason: FailureReason,
    pub dependencies: Vec<String>,
}

/// Reason why a delete operation failed for a specific record
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum FailureReason {
    /// Failed due to existing non-cascading dependencies
    DependenciesPrevented,
    /// Could be soft-deleted, but hard delete was prevented by dependencies
    SoftDeletedDueToDependencies, 
    /// Record was not found during the operation
    NotFound,
    /// User did not have permission for the requested operation (e.g., hard delete)
    AuthorizationFailed, 
    /// An unexpected database error occurred
    DatabaseError(String),
    /// Failure reason could not be determined from batch results
    Unknown, 
}

/// Result of a batch delete operation
#[derive(Debug, Clone, Default)]
pub struct BatchDeleteResult {
    /// Successfully hard deleted record IDs
    pub hard_deleted: Vec<Uuid>,
    /// Successfully soft deleted record IDs (includes those soft-deleted due to fallback)
    pub soft_deleted: Vec<Uuid>,
    /// Failed to delete record IDs (includes dependencies prevented, not found, errors)
    pub failed: Vec<Uuid>,
    /// Map of ID to dependencies that *would have* prevented hard delete
    /// (Populated for both SoftDeletedDueToDependencies and DependenciesPrevented)
    pub dependencies: HashMap<Uuid, Vec<String>>,
    /// Map of ID to specific failure errors (if captured)
    /// Optional: Requires enhancing `batch_delete` to store errors
    pub errors: HashMap<Uuid, DomainError>, 
}

/// Trait combining repository operations needed for delete service
pub trait DeleteServiceRepository<E>: FindById<E> + SoftDeletable + HardDeletable + Send + Sync {
    // Add a method to explicitly get self as a FindById reference
    fn as_find_by_id(&self) -> &dyn FindById<E>;
}

/// Implement for any type that implements all required traits
impl<T, E> DeleteServiceRepository<E> for T 
where 
    T: FindById<E> + SoftDeletable + HardDeletable + Send + Sync,
    E: Send + Sync + 'static,
{
    fn as_find_by_id(&self) -> &dyn FindById<E> {
        self
    }
}

/// Delete service for handling delete operations
#[async_trait]
pub trait DeleteService<E>: Send + Sync 
where
    E: Send + Sync + 'static,
{
    /// Get the repository
    fn repository(&self) -> &dyn FindById<E>;
    
    /// Get the tombstone repository
    fn tombstone_repository(&self) -> &dyn TombstoneRepository;
    
    /// Get the change log repository
    fn change_log_repository(&self) -> &dyn ChangeLogRepository;
    
    /// Get the dependency checker
    fn dependency_checker(&self) -> &dyn DependencyChecker;

    /// Delete an entity with specified options
    async fn delete(
        &self,
        id: Uuid,
        auth: &AuthContext,
        options: DeleteOptions,
    ) -> DomainResult<DeleteResult>;
    
    /// Delete multiple entities with specified options
    async fn batch_delete(
        &self,
        ids: &[Uuid],
        auth: &AuthContext,
        options: DeleteOptions,
    ) -> DomainResult<BatchDeleteResult>;
    
    /// Delete multiple entities with their dependencies
    async fn delete_with_dependencies(
        &self,
        id: Uuid,
        auth: &AuthContext,
    ) -> DomainResult<DeleteResult>;

    /// Retrieve details about records that failed to delete during a batch operation
    async fn get_failed_delete_details(
        &self,
        batch_result: &BatchDeleteResult,
        _auth: &AuthContext,
    ) -> DomainResult<Vec<FailedDeleteDetail<E>>>;
}

/// Base implementation of delete service
pub struct BaseDeleteService<E>
where
    E: Send + Sync + 'static,
{
    pool: SqlitePool,
    repo: Arc<dyn DeleteServiceRepository<E>>,
    tombstone_repo: Arc<dyn TombstoneRepository + Send + Sync>,
    change_log_repo: Arc<dyn ChangeLogRepository + Send + Sync>,
    dependency_checker: Arc<dyn DependencyChecker + Send + Sync>,
    media_doc_repo: Option<Arc<dyn MediaDocumentRepository>>,
    deletion_manager: Arc<PendingDeletionManager>, // Added
    _marker: std::marker::PhantomData<E>,
}

impl<E> BaseDeleteService<E>
where
    E: Send + Sync + Clone + 'static,
{
    /// Create a new base delete service
    pub fn new(
        pool: SqlitePool,
        repo: Arc<dyn DeleteServiceRepository<E>>,
        tombstone_repo: Arc<dyn TombstoneRepository + Send + Sync>,
        change_log_repo: Arc<dyn ChangeLogRepository + Send + Sync>,
        dependency_checker: Arc<dyn DependencyChecker + Send + Sync>,
        media_doc_repo: Option<Arc<dyn MediaDocumentRepository>>,
        deletion_manager: Arc<PendingDeletionManager>, // Added
    ) -> Self {
        Self {
            pool,
            repo,
            tombstone_repo,
            change_log_repo,
            dependency_checker,
            media_doc_repo,
            deletion_manager, // Added
            _marker: std::marker::PhantomData,
        }
    }

    /// Enhanced helper function to handle document deletion with physical file cleanup
    async fn cascade_delete_documents<'t, T>(
        &self,
        parent_table_name: &str,
        parent_id: Uuid,
        hard_delete: bool, // Indicates if the parent was hard deleted
        auth: &AuthContext,
        tx: &mut Transaction<'t, Sqlite>,
        pending_delete_operation_id: Uuid, // Added to associate with PendingDeletionManager
    ) -> DomainResult<()> 
    where 
        T: Send + Sync + Clone + 'static // Changed generic E to T to avoid conflict
    {
        if let Some(media_repo) = &self.media_doc_repo {
            // Fetch just the document IDs and paths for the related entity
            // using a direct query to avoid borrowing issues with the transaction
            let parent_id_str = parent_id.to_string(); // Store the string in a variable
            let document_data_result = sqlx::query!(
                r#"
                SELECT 
                    id, 
                    file_path, 
                    compressed_file_path
                FROM 
                    media_documents 
                WHERE 
                    related_table = ? AND 
                    related_id = ? AND 
                    deleted_at IS NULL
                "#,
                parent_table_name, 
                parent_id_str // Use the variable here
            )
             .fetch_all(&mut **tx) // Use the transaction
             .await;
             
            let document_data = match document_data_result {
                Ok(data) => data,
                 Err(sqlx::Error::RowNotFound) => Vec::new(), // No documents found is ok
                 Err(e) => return Err(DbError::from(e).into()), // Propagate other DB errors
             };

            // Process each document
            for doc_data_row in document_data { // Renamed `doc` to `doc_data_row`
                // Parse the ID string to UUID
                let doc_id = match Uuid::parse_str(&doc_data_row.id) {
                    Ok(id) => id,
                    Err(_) => continue, // Skip if invalid UUID (shouldn't happen)
                };
    
                if hard_delete {
                    // --- HARD DELETE PATH ---
                    
                    // 1. Create tombstone for the document
                    let mut tombstone = Tombstone::new(
                        doc_id, 
                        HardDeletable::entity_name(media_repo.as_ref()), 
                        auth.user_id,
                        parse_device_id(&auth.device_id) // Corrected: Pass Option<Uuid>
                    );
                    
                    // 2. Add additional metadata for sync/cleanup tracking
                    let metadata = serde_json::json!({
                        "file_path": doc_data_row.file_path,
                        "compressed_file_path": doc_data_row.compressed_file_path,
                        "parent_table": parent_table_name,
                        "parent_id": parent_id.to_string(),
                        "deletion_type": "cascade",
                        "timestamp": Utc::now().to_rfc3339()
                    });
                    
                    tombstone.additional_metadata = Some(metadata.to_string());
                    let operation_id_for_changelog = tombstone.operation_id; // Capture for consistency for changelog
                    
                    // 3. Create the tombstone record
                    self.tombstone_repo.create_tombstone_with_tx(&tombstone, tx).await?;

                    // 4. Create change log entry for the document's hard delete
                     let change_log = ChangeLogEntry {
                        operation_id: operation_id_for_changelog, // Use tombstone's ID
                        entity_table: HardDeletable::entity_name(media_repo.as_ref()).to_string(),
                        entity_id: doc_id,
                        operation_type: ChangeOperationType::HardDelete,
                        field_name: None, 
                        old_value: None, 
                        new_value: None,
                        document_metadata: Some(metadata.to_string()), // Add the same metadata
                        timestamp: Utc::now(),
                        user_id: auth.user_id,
                        device_id: parse_device_id(&auth.device_id),
                        sync_batch_id: None, 
                        processed_at: None, 
                        sync_error: None,
                    };
                    
                    self.change_log_repo.create_change_log_with_tx(&change_log, tx).await?;

                    // 5. Add to PendingDeletionManager instead of queuing directly
                    let deletion_info = DocumentDeletion {
                        document_id: doc_id,
                        file_path: doc_data_row.file_path.clone(), // file_path is String in query result
                        compressed_file_path: doc_data_row.compressed_file_path.clone(), // compressed_file_path is Option<String>
                    };
                    self.deletion_manager.add_pending(pending_delete_operation_id, deletion_info).await;
                    
                    // 6. Perform the actual hard delete of the document record
                    media_repo.hard_delete_with_tx(doc_id, auth, tx).await?;
                    
                    // DB ON DELETE CASCADE will handle document versions and access logs automatically

                } else {
                    // --- SOFT DELETE PATH ---
                    
                    // Only soft delete the document record - don't queue for file deletion yet
                    // since the document should still exist but be marked as deleted
                    media_repo.soft_delete_with_tx(doc_id, auth, tx).await?;
                    
                    // Log access for the soft delete
                    let log_id_str = Uuid::new_v4().to_string();
                    let log_doc_id_str = doc_id.to_string();
                    let log_user_id_str = auth.user_id.to_string();
                    let log_timestamp_str = Utc::now().to_rfc3339();
                    let log_details_str = format!("Cascade soft delete from parent: {}/{}", parent_table_name, parent_id);
                    sqlx::query!(
                        r#"
                        INSERT INTO document_access_logs (
                            id,
                            document_id,
                            user_id,
                            access_type,
                            access_date,
                            details
                        )
                        VALUES (?, ?, ?, ?, ?, ?)
                        "#,
                        log_id_str,      // Use variable
                        log_doc_id_str,  // Use variable
                        log_user_id_str, // Use variable
                        "delete", // soft delete
                        log_timestamp_str, // Use variable
                        log_details_str    // Use variable
                    )
                    .execute(&mut **tx)
                    .await
                    .map_err(DbError::from)?; // Propagate error instead of just logging
                }
            }
        }
        
        Ok(())
    }
}

#[async_trait]
impl<E> DeleteService<E> for BaseDeleteService<E>
where
    E: Send + Sync + Clone + 'static,
{
    fn repository(&self) -> &dyn FindById<E> {
        self.repo.as_find_by_id()
    }
    
    fn tombstone_repository(&self) -> &dyn TombstoneRepository {
        &*self.tombstone_repo
    }
    
    fn change_log_repository(&self) -> &dyn ChangeLogRepository {
        &*self.change_log_repo
    }
    
    fn dependency_checker(&self) -> &dyn DependencyChecker {
        &*self.dependency_checker
    }
    
    async fn delete(
        &self,
        id: Uuid,
        auth: &AuthContext,
        options: DeleteOptions,
    ) -> DomainResult<DeleteResult> {
        // 1. Permission Checks
        if options.allow_hard_delete && auth.role != UserRole::Admin {
            return Err(DomainError::AuthorizationFailed("Only admins can perform hard deletes".to_string()));
        }
        if options.force && auth.role != UserRole::Admin {
            return Err(DomainError::AuthorizationFailed("Only admins can force delete operations".to_string()));
        }

        // 2. Dependency Check
        let table_name = self.repo.entity_name(); // Corrected: Use instance method
        let all_dependencies = self.dependency_checker.check_dependencies(table_name, id).await?;
        
        let blocking_dependencies: Vec<String> = all_dependencies
            .iter()
            .filter(|dep| !dep.is_cascadable && dep.count > 0)
            .map(|dep| dep.table_name.clone())
            .collect();

        // 3. Decide Action: Hard Delete or Soft Delete/Prevent
        let can_hard_delete = options.allow_hard_delete 
                              && auth.role == UserRole::Admin
                              && (blocking_dependencies.is_empty() || options.force);

        // Unique ID for this entire delete operation, used for managing pending document deletions
        let pending_delete_operation_id = Uuid::new_v4();

        if can_hard_delete {
            // --- Hard Delete Path --- 
            let mut tx = self.pool.begin().await.map_err(DbError::from)?; // Start transaction

            let hard_delete_result = async {
                // Create tombstone
                let tombstone = Tombstone::new(
                    id, 
                    self.repo.entity_name(), // Corrected: Use instance method
                    auth.user_id,
                    parse_device_id(&auth.device_id) // Corrected: Pass Option<Uuid>
                );
                let operation_id_for_changelog = tombstone.operation_id; // Capture for changelog
                self.tombstone_repo.create_tombstone_with_tx(&tombstone, &mut tx).await?;

                // Create change log entry for HardDelete
                let change_log = ChangeLogEntry {
                    operation_id: operation_id_for_changelog, // Use same ID as tombstone
                    entity_table: self.repo.entity_name().to_string(), // Corrected: Use instance method
                    entity_id: id,
                    operation_type: ChangeOperationType::HardDelete,
                    field_name: None,
                    old_value: None, // Maybe store serialized entity before delete?
                    new_value: None,
                    document_metadata: None, // Not a document deletion itself
                    timestamp: Utc::now(),
                    user_id: auth.user_id,
                    device_id: parse_device_id(&auth.device_id), // Using the helper
                    sync_batch_id: None,
                    processed_at: None,
                    sync_error: None,
                };
                self.change_log_repo.create_change_log_with_tx(&change_log, &mut tx).await?;

                // Perform the actual hard delete of the main entity
                self.repo.hard_delete_with_tx(id, auth, &mut tx).await?;

                // Cascade delete documents, passing the pending_delete_operation_id
                self.cascade_delete_documents::<E>(table_name, id, true, auth, &mut tx, pending_delete_operation_id).await?;

                Ok::<_, DomainError>(())
            }.await;

            match hard_delete_result {
                Ok(_) => {
                    tx.commit().await.map_err(DbError::from)?;
                    // If DB commit is successful, commit the document deletions to the queue
                    self.deletion_manager.commit_deletions(pending_delete_operation_id, auth.user_id, &auth.device_id).await?; // Corrected: Pass &auth.device_id
                    Ok(DeleteResult::HardDeleted)
                },
                Err(e @ DomainError::EntityNotFound(_, _)) => {
                    let _ = tx.rollback().await;
                    // If DB rollback, discard pending document deletions
                    self.deletion_manager.discard_deletions(pending_delete_operation_id).await;
                    Err(e) 
                },
                Err(e) => {
                    let _ = tx.rollback().await; 
                    // If DB rollback, discard pending document deletions
                    self.deletion_manager.discard_deletions(pending_delete_operation_id).await;
                    Err(e) 
                }
            }

        } else {
            // --- Soft Delete Path (or fallback if hard delete not allowed/failed due to non-forced deps) ---
            // Ensure we only attempt soft delete if it's allowed by options or is a fallback
            let should_soft_delete = options.fallback_to_soft_delete || !options.allow_hard_delete;

            if !blocking_dependencies.is_empty() && !options.force && should_soft_delete {
                // Dependencies exist, hard delete not forced, soft delete is allowed/fallback
                let mut tx = self.pool.begin().await.map_err(DbError::from)?;
                let soft_delete_res = async {
                    self.repo.soft_delete_with_tx(id, auth, &mut tx).await?;
                    // NOTE: Removed ChangeLogEntry creation for soft delete as per user request.
                    // Soft delete related documents as well
                    self.cascade_delete_documents::<E>(table_name, id, false, auth, &mut tx, pending_delete_operation_id).await?;
                    Ok::<_, DomainError>(())
                }.await;

                match soft_delete_res {
                    Ok(_) => {
                        tx.commit().await.map_err(DbError::from)?;
                        // For soft deletes due to dependencies, we don't commit file deletions from PendingDeletionManager immediately
                        // We discard them as the files are still needed for the soft-deleted record.
                        self.deletion_manager.discard_deletions(pending_delete_operation_id).await;
                        Ok(DeleteResult::SoftDeleted { dependencies: blocking_dependencies }) // Corrected variant
                    },
                    Err(e) => {
                        tx.rollback().await.map_err(DbError::from)?;
                        self.deletion_manager.discard_deletions(pending_delete_operation_id).await;
                        Err(e)
                    }
                }
            } else if blocking_dependencies.is_empty() && !options.allow_hard_delete && should_soft_delete {
                 // No blocking dependencies, but hard delete not allowed by options (e.g. user not admin)
                 // Proceed with soft delete if it's the primary intention or fallback.
                let mut tx = self.pool.begin().await.map_err(DbError::from)?;
                let soft_delete_res = async {
                    self.repo.soft_delete_with_tx(id, auth, &mut tx).await?;
                    // NOTE: Removed ChangeLogEntry creation for soft delete as per user request.
                    self.cascade_delete_documents::<E>(table_name, id, false, auth, &mut tx, pending_delete_operation_id).await?;
                    Ok::<_, DomainError>(())
                }.await;
                match soft_delete_res {
                    Ok(_) => {
                        tx.commit().await.map_err(DbError::from)?;
                        self.deletion_manager.discard_deletions(pending_delete_operation_id).await; // Discard as files are kept for soft delete
                        Ok(DeleteResult::SoftDeleted { dependencies: Vec::new() }) // Corrected variant
                    },
                    Err(e) => {
                        tx.rollback().await.map_err(DbError::from)?;
                        self.deletion_manager.discard_deletions(pending_delete_operation_id).await;
                        Err(e)
                    }
                }
            } else {
                // Cannot delete due to dependencies and not forcing, or soft delete not an option
                // We still discard any pending document deletions as the main operation failed.
                self.deletion_manager.discard_deletions(pending_delete_operation_id).await;
                Ok(DeleteResult::DependenciesPrevented { dependencies: blocking_dependencies }) // Corrected: Return Ok with appropriate variant
            }
        }
    }
    
    /// Delete multiple entities with specified options
    async fn batch_delete(
        &self,
        ids: &[Uuid],
        auth: &AuthContext,
        options: DeleteOptions,
    ) -> DomainResult<BatchDeleteResult> {
        // Check permissions based on options
        if options.allow_hard_delete && auth.role != UserRole::Admin {
            return Err(DomainError::AuthorizationFailed("Only admins can perform hard deletes".to_string()));
        }
        
        if options.force && auth.role != UserRole::Admin {
            return Err(DomainError::AuthorizationFailed("Only admins can force delete operations".to_string()));
        }
        
        let mut result = BatchDeleteResult::default();
        
        // Process each ID
        for &id in ids {
            match self.delete(id, auth, options.clone()).await {
                Ok(DeleteResult::HardDeleted) => {
                    result.hard_deleted.push(id);
                },
                
                Ok(DeleteResult::SoftDeleted { dependencies }) => {
                    result.soft_deleted.push(id);
                    if !dependencies.is_empty() {
                        result.dependencies.insert(id, dependencies);
                    }
                },
                
                Ok(DeleteResult::DependenciesPrevented { dependencies }) => {
                    result.failed.push(id);
                    result.dependencies.insert(id, dependencies);
                },
                
                Err(e @ DomainError::AuthorizationFailed(_)) => {
                    result.failed.push(id);
                    result.errors.insert(id, e);
                },
                Err(e @ DomainError::EntityNotFound(_, _)) => {
                    result.failed.push(id);
                    result.errors.insert(id, e);
                },
                Err(e) => {
                    result.failed.push(id);
                    result.errors.insert(id, e);
                }
            }
        }
        
        Ok(result)
    }
    
    /// Delete multiple entities with their dependencies
    async fn delete_with_dependencies(
        &self,
        id: Uuid,
        auth: &AuthContext,
    ) -> DomainResult<DeleteResult> {
        // This operation is admin-only
        if auth.role != UserRole::Admin {
            return Err(DomainError::AuthorizationFailed("Only admins can delete entities with dependencies".to_string()));
        }
        
        // Check for dependencies
        let table_name = self.repo.entity_name(); // Corrected: Use instance method
        let dependencies = self.dependency_checker().check_dependencies(table_name, id).await?;
        
        // If no dependencies, just do a normal hard delete
        if dependencies.is_empty() {
            return self.delete(
                id, 
                auth, 
                DeleteOptions {
                    allow_hard_delete: true,
                    fallback_to_soft_delete: false,
                    force: false,
                }
            ).await;
        }
        
        // For each dependency, we need to handle it based on whether it's cascadable
        // This would be a more complex implementation and might require specific
        // logic per entity type to handle the dependency chain correctly
        
        // For simplicity, we'll just use the force option which will hard delete
        // regardless of dependencies (relying on database ON DELETE constraints to handle cascades)
        self.delete(
            id, 
            auth, 
            DeleteOptions {
                allow_hard_delete: true,
                fallback_to_soft_delete: false,
                force: true,
            }
        ).await
    }

    /// Retrieve details about records that failed to delete during a batch operation
    async fn get_failed_delete_details(
        &self,
        batch_result: &BatchDeleteResult,
        _auth: &AuthContext,
    ) -> DomainResult<Vec<FailedDeleteDetail<E>>> {
        let mut details = Vec::new();
        let entity_type_name = self.repo.entity_name().to_string(); // Corrected: Use instance method

        for &id in &batch_result.failed {
            // Attempt to fetch the entity data (might fail if already deleted or never existed)
            // Note: This assumes find_by_id doesn't require specific permissions beyond auth context validity,
            // or that the auth context used here has sufficient read permissions.
            let entity_data_result = self.repo.find_by_id(id).await;

            let entity_data = match entity_data_result {
                 Ok(entity) => Some(entity),
                 Err(DomainError::EntityNotFound(_, _)) => None, // Expected if it was never found
                 Err(_) => None, // Other error fetching, treat as data unavailable
            };

            // Determine the failure reason
            let (reason, deps) = match batch_result.errors.get(&id) {
                Some(DomainError::EntityNotFound(_, _)) => (FailureReason::NotFound, Vec::new()),
                Some(DomainError::AuthorizationFailed(_)) => (FailureReason::AuthorizationFailed, Vec::new()),
                Some(DomainError::Database(db_err)) => (FailureReason::DatabaseError(db_err.to_string()), Vec::new()),
                // Use wildcard _ to catch any other DomainError variant or None
                Some(_) | None => { 
                    // If Conflict or no specific error, check dependencies map
                    if let Some(dep_tables) = batch_result.dependencies.get(&id) {
                         // Check if it was actually soft-deleted (meaning dependencies prevented hard delete)
                         if batch_result.soft_deleted.contains(&id) {
                              (FailureReason::SoftDeletedDueToDependencies, dep_tables.clone())
                         } else {
                             // If it's in failed *and* dependencies, it was prevented entirely
                             (FailureReason::DependenciesPrevented, dep_tables.clone())
                         }
                    } else {
                         // In failed, but no specific error and no dependencies recorded -> Unknown
                         (FailureReason::Unknown, Vec::new())
                    }
                },
                 // _ => (FailureReason::Unknown, Vec::new()), // This arm is unreachable and removed
            };

            details.push(FailedDeleteDetail {
                id,
                entity_data,
                entity_type: entity_type_name.clone(),
                reason,
                dependencies: deps,
            });
        }

        Ok(details)
    }
}// src/domains/core/document_linking.rs
use serde::Serialize;
use std::collections::HashSet;

#[derive(Debug, Clone, PartialEq, Eq, Serialize)]
pub enum FieldType {
    Text,
    Number, // Represents i64, f64
    Boolean,
    Date, // Represents NaiveDate or String YYYY-MM-DD
    Timestamp, // Represents DateTime<Utc> or String RFC3339
    Uuid,
    Decimal,
    // Special type for fields that primarily store a document reference
    DocumentRef, 
}

#[derive(Debug, Clone)]
pub struct EntityFieldMetadata {
    /// Technical name of the field (matches struct/db potentially)
    pub field_name: &'static str,
    /// User-friendly name for UI display
    pub display_name: &'static str,
    /// Can documents be logically linked to this field?
    pub supports_documents: bool,
    /// The type of the field (for UI hints, validation)
    pub field_type: FieldType,
    /// Is this field primarily just a reference to a document?
    pub is_document_reference_only: bool,
}

/// Trait for entities that allow documents to be linked to specific fields.
pub trait DocumentLinkable {
    /// Provides metadata for all fields relevant for display or linking.
    fn field_metadata() -> Vec<EntityFieldMetadata>;

    /// Get the names of fields that support document attachments.
    fn document_linkable_fields() -> HashSet<String> {
        Self::field_metadata()
            .into_iter()
            .filter(|meta| meta.supports_documents)
            .map(|meta| meta.field_name.to_string())
            .collect()
    }

    /// Check if a specific field supports document linking.
    fn is_document_linkable_field(field: &str) -> bool {
        Self::document_linkable_fields().contains(field)
    }

    /// Get metadata for a specific field by name.
    fn get_field_metadata(field_name: &str) -> Option<EntityFieldMetadata> {
         Self::field_metadata().into_iter().find(|meta| meta.field_name == field_name)
    }
}


// API Response Structure (could live in a web/api layer types mod)
#[derive(Serialize)]
pub struct FieldMetadataResponse {
    pub field_name: String,
    pub display_name: String,
    pub supports_documents: bool,
    pub field_type: FieldType, // Serialize the enum directly
    pub is_document_reference_only: bool,
}

impl From<EntityFieldMetadata> for FieldMetadataResponse {
    fn from(meta: EntityFieldMetadata) -> Self {
        FieldMetadataResponse {
            field_name: meta.field_name.to_string(),
            display_name: meta.display_name.to_string(),
            supports_documents: meta.supports_documents,
            field_type: meta.field_type,
            is_document_reference_only: meta.is_document_reference_only,
        }
    }
} use crate::errors::DomainResult;
use async_trait::async_trait;
use sqlx::{Pool, Sqlite, query_as};
use uuid::Uuid;
use std::collections::HashMap;
use crate::errors::DbError;
use crate::errors::DomainError;

/// Dependency information
#[derive(Debug, Clone)]
pub struct Dependency {
    /// Name of the table with dependent records
    pub table_name: String,
    
    /// Count of dependent records
    pub count: i64,
    
    /// Name of the foreign key column
    pub foreign_key_column: String,
    
    /// Whether the dependency is cascadable (ON DELETE CASCADE)
    pub is_cascadable: bool,
}

/// Trait for dependency checking
#[async_trait]
pub trait DependencyChecker: Send + Sync {
    /// Check for dependencies for an entity
    async fn check_dependencies(&self, table_name: &str, id: Uuid) -> DomainResult<Vec<Dependency>>;
    
    /// Get a simplified list of dependency tables
    async fn get_dependency_tables(&self, table_name: &str, id: Uuid) -> DomainResult<Vec<String>> {
        let dependencies = self.check_dependencies(table_name, id).await?;
        Ok(dependencies.into_iter().map(|dep| dep.table_name).collect())
    }
}

/// SQLite implementation of the DependencyChecker
pub struct SqliteDependencyChecker {
    pool: Pool<Sqlite>,
    /// Maps table name to its cascadable dependencies
    dependency_map: HashMap<String, Vec<(String, String, bool)>>,
}

impl SqliteDependencyChecker {
    /// Create a new SQLite dependency checker
    pub fn new(pool: Pool<Sqlite>) -> Self {
        let mut dependency_map = HashMap::new();
        
        // Define dependencies based on schema
        // Format: (table_name, [(dependent_table, foreign_key_column, is_cascadable)])
        
        // Strategic goals dependencies
        dependency_map.insert(
            "strategic_goals".to_string(), 
            vec![
                ("projects".to_string(), "strategic_goal_id".to_string(), false),
            ]
        );
        
        // Projects dependencies
        dependency_map.insert(
            "projects".to_string(), 
            vec![
                ("workshops".to_string(), "project_id".to_string(), false),
                ("activities".to_string(), "project_id".to_string(), true),
                ("livelihoods".to_string(), "project_id".to_string(), false),
                ("project_funding".to_string(), "project_id".to_string(), false),
            ]
        );
        
        // Workshops dependencies
        dependency_map.insert(
            "workshops".to_string(), 
            vec![
                ("workshop_participants".to_string(), "workshop_id".to_string(), true),
            ]
        );
        
        // Participants dependencies
        dependency_map.insert(
            "participants".to_string(), 
            vec![
                ("workshop_participants".to_string(), "participant_id".to_string(), true),
                ("livelihoods".to_string(), "participant_id".to_string(), true),
            ]
        );
        
        // Livelihoods dependencies
        dependency_map.insert(
            "livelihoods".to_string(), 
            vec![
                ("subsequent_grants".to_string(), "livelihood_id".to_string(), true),
            ]
        );
        
        // Donors dependencies
        dependency_map.insert(
            "donors".to_string(), 
            vec![
                ("project_funding".to_string(), "donor_id".to_string(), false),
            ]
        );

        // Document Types dependencies
        dependency_map.insert(
            "document_types".to_string(),
            vec![
                // MediaDocuments depend on DocumentTypes, but deletion is RESTRICTED by FK.
                // Hard deleting a DocumentType SHOULD be blocked if MediaDocuments use it.
                ("media_documents".to_string(), "type_id".to_string(), false),
            ]
        );

        // NOTE: We intentionally DO NOT add entries for other tables (projects, workshops, etc.)
        // pointing to media_documents here. The check for those dependencies is handled
        // differently in check_dependencies to allow the desired cascading behavior.
        
        Self { pool, dependency_map }
    }
}

/// Query result for dependency count
#[derive(Debug, sqlx::FromRow)]
struct DependencyCount {
    count: i64,
}

#[async_trait]
impl DependencyChecker for SqliteDependencyChecker {
    async fn check_dependencies(&self, table_name: &str, id: Uuid) -> DomainResult<Vec<Dependency>> {
        let mut dependencies = Vec::new();
        let id_str = id.to_string();

        // 1. Check defined dependencies from the map
        if let Some(dependent_tables) = self.dependency_map.get(table_name) {
            for (dependent_table, foreign_key, is_cascadable) in dependent_tables {
                // Build and execute query to count dependencies
                let query = format!(
                    "SELECT COUNT(*) as count FROM {} WHERE {} = ? AND deleted_at IS NULL", // Check only non-deleted dependents
                    dependent_table, 
                    foreign_key
                );
                
                let count_result: Result<DependencyCount, sqlx::Error> = query_as(&query)
                    .bind(&id_str)
                    .fetch_one(&self.pool)
                    .await;
                    
                let count = match count_result {
                    Ok(c) => c.count,
                    Err(sqlx::Error::RowNotFound) => 0, // Treat RowNotFound as 0 count
                    Err(e) => return Err(DomainError::Database(DbError::from(e))), // Propagate other errors
                };
                
                if count > 0 {
                    dependencies.push(Dependency {
                        table_name: dependent_table.clone(),
                        count: count,
                        foreign_key_column: foreign_key.clone(),
                        is_cascadable: *is_cascadable,
                    });
                }
            }
        }

        // 2. Special check for media_documents dependency for ALL tables EXCEPT document_types itself.
        // We want to know if documents exist, but NOT treat them as blocking for parents.
        // The service layer will handle the cascade.
        if table_name != "document_types" {
            let query = "SELECT COUNT(*) as count FROM media_documents WHERE related_table = ? AND related_id = ? AND deleted_at IS NULL";
            let count_result: Result<DependencyCount, sqlx::Error> = query_as(query)
                .bind(table_name) // Bind the parent table name
                .bind(&id_str)     // Bind the parent ID
                .fetch_one(&self.pool)
                .await;

            let count = match count_result {
                 Ok(c) => c.count,
                 Err(sqlx::Error::RowNotFound) => 0,
                 Err(e) => return Err(DomainError::Database(DbError::from(e))),
            };

            if count > 0 {
                // Add this dependency, but mark it as non-blocking (is_cascadable = true)
                // even though the FK isn't CASCADE. The service layer interprets this.
                dependencies.push(Dependency {
                    table_name: "media_documents".to_string(),
                    count: count,
                    foreign_key_column: "related_id".to_string(), // Indicate the relevant key
                    is_cascadable: true, // Signal to DeleteService this is handled by service-level cascade
                });
            }
        }
        
        Ok(dependencies)
    }
}use crate::domains::core::file_storage_service::{FileStorageService, FileStorageError};
use crate::errors::{DomainError, DomainResult, ServiceError, ServiceResult, DbError};
use sqlx::{SqlitePool, Row};
use uuid::Uuid;
use std::sync::Arc;
use chrono::{Utc, Duration};
use tokio::time;

/// Worker for processing file deletions in the background
pub struct FileDeletionWorker {
    pool: SqlitePool,
    file_storage_service: Arc<dyn FileStorageService>,
    shutdown_signal: Option<tokio::sync::oneshot::Receiver<()>>,
    active_files_check_enabled: bool,
}

impl FileDeletionWorker {
    pub fn new(
        pool: SqlitePool,
        file_storage_service: Arc<dyn FileStorageService>,
    ) -> Self {
        Self {
            pool,
            file_storage_service,
            shutdown_signal: None,
            active_files_check_enabled: true,
        }
    }
    
    /// Set shutdown signal receiver
    pub fn with_shutdown_signal(mut self, receiver: tokio::sync::oneshot::Receiver<()>) -> Self {
        self.shutdown_signal = Some(receiver);
        self
    }
    
    /// Disable active files check (useful for testing or cleanup mode)
    pub fn disable_active_files_check(mut self) -> Self {
        self.active_files_check_enabled = false;
        self
    }
    
    /// Start the worker loop
    pub async fn start(mut self) -> Result<(), ServiceError> {
        log::info!("Starting file deletion worker");
        
        // Run every 5 minutes
        let mut interval = time::interval(time::Duration::from_secs(5 * 60));
        
        loop {
            tokio::select! {
                _ = interval.tick() => {
                    if let Err(e) = self.process_deletion_queue().await {
                        log::error!("Error processing file deletion queue: {:?}", e);
                    }
                }
                _ = async {
                    if let Some(mut signal) = self.shutdown_signal.take() {
                        let _ = signal.await;
                        true
                    } else {
                        // Never complete if no shutdown signal
                        std::future::pending::<bool>().await
                    }
                } => {
                    log::info!("Received shutdown signal, stopping file deletion worker");
                    break;
                }
            }
        }
        
        Ok(())
    }
    
    /// Process the pending items in the file deletion queue
    async fn process_deletion_queue(&self) -> Result<(), ServiceError> {
        log::info!("Processing file deletion queue");
        
        // Get pending deletions where grace period has expired
        let pending_deletions = self.get_pending_deletions().await?;
        
        if pending_deletions.is_empty() {
            log::info!("No pending file deletions to process");
            return Ok(());
        }
        
        log::info!("Found {} pending file deletions", pending_deletions.len());
        
        for deletion in pending_deletions {
            // Skip files that are currently in use if active check is enabled
            if self.active_files_check_enabled && self.is_file_in_use(&deletion.document_id).await? {
                log::info!("Skipping file in use: document_id={}", deletion.document_id);
                
                // Update the last attempt timestamp
                let now_str = Utc::now().to_rfc3339();
                sqlx::query!(
                    r#"
                    UPDATE file_deletion_queue
                    SET 
                        last_attempt_at = ?,
                        attempts = attempts + 1
                    WHERE id = ?
                    "#,
                    now_str,
                    deletion.id
                )
                .execute(&self.pool)
                .await.map_err(|e| ServiceError::Domain(DomainError::Database(DbError::from(e))))?;
                
                continue;
            }
            
            // Try to delete the original file
            let original_result = if let Some(path) = &deletion.file_path {
                self.file_storage_service.delete_file(path).await
            } else {
                Ok(()) // No path, so "success"
            };
            
            // Try to delete the compressed file if it exists
            let compressed_result = if let Some(path) = &deletion.compressed_file_path {
                self.file_storage_service.delete_file(path).await
            } else {
                Ok(()) // No compressed path, so "success"
            };
            
            // Check results and update database
            let mut error_message = None;
            
            if let Err(e) = &original_result {
                if !matches!(e, FileStorageError::NotFound(_)) {
                    // Only log real errors, not just missing files
                    error_message = Some(format!("Error deleting original file: {}", e));
                }
            }
            
            if let Err(e) = &compressed_result {
                if !matches!(e, FileStorageError::NotFound(_)) {
                    let compressed_err = format!("Error deleting compressed file: {}", e);
                    error_message = match error_message {
                        Some(msg) => Some(format!("{}; {}", msg, compressed_err)),
                        None => Some(compressed_err),
                    };
                }
            }
            
            // Update database - mark as completed if both successfully deleted or not found
            if original_result.is_ok() && compressed_result.is_ok() {
                let now_str = Utc::now().to_rfc3339();
                sqlx::query!(
                    r#"
                    UPDATE file_deletion_queue
                    SET 
                        completed_at = ?,
                        last_attempt_at = ?,
                        attempts = attempts + 1,
                        error_message = NULL
                    WHERE id = ?
                    "#,
                    now_str,
                    now_str,
                    deletion.id
                )
                .execute(&self.pool)
                .await.map_err(|e| ServiceError::Domain(DomainError::Database(DbError::from(e))))?;
                
                log::info!("Successfully deleted files for document: {}", deletion.document_id);
            } else {
                // Update attempt count and error message
                let now_str = Utc::now().to_rfc3339();
                sqlx::query!(
                    r#"
                    UPDATE file_deletion_queue
                    SET 
                        last_attempt_at = ?,
                        attempts = attempts + 1,
                        error_message = ?
                    WHERE id = ?
                    "#,
                    now_str,
                    error_message,
                    deletion.id
                )
                .execute(&self.pool)
                .await.map_err(|e| ServiceError::Domain(DomainError::Database(DbError::from(e))))?;
                
                log::warn!(
                    "Failed to delete files for document: {} - Error: {:?}",
                    deletion.document_id,
                    error_message
                );
            }
        }
        
        Ok(())
    }
    
    /// Get pending deletions that are ready to be processed (grace period expired)
    async fn get_pending_deletions(&self) -> Result<Vec<PendingFileDeletion>, ServiceError> {
        let rows = sqlx::query!(
            r#"
            SELECT 
                id as "id!",
                document_id as "document_id!",
                file_path as "file_path: Option<String>",
                compressed_file_path as "compressed_file_path: Option<String>",
                requested_at as "requested_at!",
                attempts
            FROM file_deletion_queue
            WHERE 
                completed_at IS NULL AND 
                datetime(requested_at) <= datetime('now', '-' || grace_period_seconds || ' seconds')
            ORDER BY
                attempts ASC, -- Try not-yet-attempted files first
                requested_at ASC -- Then oldest first
            LIMIT 100 -- Process in batches
            "#
        )
        .fetch_all(&self.pool)
        .await.map_err(|e| ServiceError::Domain(DomainError::Database(DbError::from(e))))?;
        
        let mut result = Vec::with_capacity(rows.len());
        
        for row in rows {
            result.push(PendingFileDeletion {
                id: row.id,
                document_id: row.document_id,
                file_path: row.file_path,
                compressed_file_path: row.compressed_file_path.flatten(),
                requested_at: row.requested_at,
                attempts: row.attempts.unwrap_or(0),
            });
        }
        
        Ok(result)
    }
    
    /// Check if file is currently in use
    async fn is_file_in_use(&self, document_id: &str) -> Result<bool, ServiceError> {
        // Check active_file_usage table
        let result = sqlx::query!(
            r#"
            SELECT EXISTS(
                SELECT 1 
                FROM active_file_usage 
                WHERE 
                    document_id = ? AND 
                    last_active_at > datetime('now', '-5 minutes')
            ) as in_use
            "#,
            document_id
        )
        .fetch_one(&self.pool)
        .await.map_err(|e| ServiceError::Domain(DomainError::Database(DbError::from(e))))?;
        
        Ok(result.in_use == 1)
    }
}

/// Struct representing a pending file deletion
struct PendingFileDeletion {
    id: String,
    document_id: String,
    file_path: Option<String>,
    compressed_file_path: Option<String>,
    requested_at: String,
    attempts: i64,
} use async_trait::async_trait;
use std::path::{Path, PathBuf};
use thiserror::Error;
use tokio::fs; // Use tokio::fs for async file operations
use uuid::Uuid;
use std::io;

#[derive(Debug, Error)]
pub enum FileStorageError {
    #[error("I/O error: {0}")]
    Io(#[from] std::io::Error),
    #[error("File not found: {0}")]
    NotFound(String),
    #[error("Configuration error: {0}")]
    Configuration(String),
    #[error("Permission denied: {0}")]
    PermissionDenied(String),
    #[error("Storage limit exceeded")]
    LimitExceeded,
    #[error("Invalid path component: {0}")]
    InvalidPathComponent(String),
    #[error("Unknown storage error: {0}")]
    Other(String),
}

pub type FileStorageResult<T> = Result<T, FileStorageError>;

/// Service trait for abstracting file storage operations
#[async_trait]
pub trait FileStorageService: Send + Sync {
    /// Save file data to storage, returning the relative path and size.
    /// The implementation determines the final path structure.
    async fn save_file(
        &self,
        data: Vec<u8>,
        entity_type: &str, // e.g., "strategic_goals", "documents"
        entity_or_temp_id: &str,   // Associated entity ID (Uuid as string) or temp ID
        suggested_filename: &str, // Original filename for extension/naming hint
    ) -> FileStorageResult<(String, u64)>; // Returns (relative_path, size_bytes)

    /// Delete a file from storage using its relative path.
    async fn delete_file(&self, relative_path: &str) -> FileStorageResult<()>;

    /// Get a readable stream or bytes for a file.
    /// (Using Vec<u8> for simplicity, could use streams like Tokio's AsyncRead)
    async fn get_file_data(&self, relative_path: &str) -> FileStorageResult<Vec<u8>>;
    
    /// Get the full absolute path for a given relative path (for internal use if needed)
    fn get_absolute_path(&self, relative_path: &str) -> PathBuf;
}

// --- Local File Storage Implementation ---

pub struct LocalFileStorageService {
    base_path: PathBuf,
    original_subdir: String,
    compressed_subdir: String,
}

impl LocalFileStorageService {
    /// Creates a new LocalFileStorageService.
    /// Ensures the base directory and subdirectories exist.
    pub fn new(base_path_str: &str) -> io::Result<Self> {
        let base_path = PathBuf::from(base_path_str);
        let original_subdir = "original".to_string();
        let compressed_subdir = "compressed".to_string();

        let original_path = base_path.join(&original_subdir);
        let compressed_path = base_path.join(&compressed_subdir);

        // Create directories synchronously during setup
        std::fs::create_dir_all(&original_path)?;
        std::fs::create_dir_all(&compressed_path)?;

        Ok(Self {
            base_path,
            original_subdir,
            compressed_subdir,
        })
    }

    /// Sanitizes a path component to prevent directory traversal issues.
    fn sanitize_component(component: &str) -> Result<String, FileStorageError> {
        // Check for invalid characters or patterns
        if component.is_empty() || component.contains('/') || component.contains('\\') || component == "." || component == ".." {
            Err(FileStorageError::InvalidPathComponent(component.to_string()))
        } else {
            Ok(component.to_string())
        }
    }

    /// Generates a unique filename based on suggestion and a new UUID.
    fn generate_unique_filename(suggested_filename: &str) -> String {
        let extension = Path::new(suggested_filename)
            .extension()
            .and_then(|ext| ext.to_str())
            .map(|ext| format!(".{}", ext))
            .unwrap_or_default();
        format!("{}{}", Uuid::new_v4(), extension)
    }
}

#[async_trait]
impl FileStorageService for LocalFileStorageService {
    async fn save_file(
        &self,
        data: Vec<u8>,
        entity_type: &str,
        entity_or_temp_id: &str,
        suggested_filename: &str,
    ) -> FileStorageResult<(String, u64)> {
        let sanitized_entity_type = Self::sanitize_component(entity_type)?;
        let sanitized_id = Self::sanitize_component(entity_or_temp_id)?;
        let unique_filename = Self::generate_unique_filename(suggested_filename);

        // Construct relative path: original/entity_type/entity_or_temp_id/unique_filename.ext
        let relative_path = Path::new(&self.original_subdir)
            .join(&sanitized_entity_type)
            .join(&sanitized_id)
            .join(&unique_filename);

        // Correctly get the relative path as a string slice for get_absolute_path
        let relative_path_str = relative_path.to_str().ok_or_else(|| FileStorageError::Other("Failed to convert relative path to string".to_string()))?;
        let absolute_path = self.get_absolute_path(relative_path_str);

        let parent_dir = absolute_path.parent().ok_or_else(|| FileStorageError::Other("Invalid path generated, no parent directory".to_string()))?;

        // Ensure the parent directory exists
        fs::create_dir_all(parent_dir).await?;

        let file_size = data.len() as u64;

        // Write the file asynchronously
        fs::write(&absolute_path, data).await?;

        Ok((relative_path_str.to_string(), file_size))
    }

    async fn delete_file(&self, relative_path: &str) -> FileStorageResult<()> {
        let absolute_path = self.get_absolute_path(relative_path);

        // Basic check to prevent deleting outside the base path (though sanitize helps)
        if !absolute_path.starts_with(&self.base_path) {
            return Err(FileStorageError::PermissionDenied("Attempt to delete outside base path".to_string()));
        }

        match fs::remove_file(&absolute_path).await {
            Ok(_) => Ok(()),
            Err(e) if e.kind() == io::ErrorKind::NotFound => {
                // Consider it success if the file is already gone
                Ok(())
            }
            Err(e) => Err(FileStorageError::Io(e)),
        }
        // TODO: Consider deleting empty parent directories? Maybe not necessary.
    }

    async fn get_file_data(&self, relative_path: &str) -> FileStorageResult<Vec<u8>> {
        let absolute_path = self.get_absolute_path(relative_path);

        // Basic check
        if !absolute_path.starts_with(&self.base_path) {
             return Err(FileStorageError::PermissionDenied("Attempt to read outside base path".to_string()));
        }

        match fs::read(&absolute_path).await {
            Ok(data) => Ok(data),
            Err(e) if e.kind() == io::ErrorKind::NotFound => {
                 Err(FileStorageError::NotFound(relative_path.to_string()))
            }
            Err(e) => Err(FileStorageError::Io(e)),
        }
    }

    fn get_absolute_path(&self, relative_path: &str) -> PathBuf {
        // IMPORTANT: This assumes relative_path is ALREADY somewhat sanitized
        // or comes from a trusted source (like the DB). It cleans ".." etc.
        // normalize_path from the path_clean crate could be more robust if needed.
        let mut abs_path = self.base_path.clone();
        for component in Path::new(relative_path).components() {
             match component {
                std::path::Component::Normal(comp_str) => {
                    // Convert OsStr to str for checking. Handle potential non-UTF8 gracefully.
                    if let Some(s) = comp_str.to_str() {
                        if s.is_empty() || s.contains('/') || s.contains('\\') {
                            // Skip potentially problematic components
                            // Logging this might be useful in practice
                            continue;
                        }
                        abs_path.push(comp_str);
                    } else {
                        // Handle non-UTF8 path components if necessary, 
                        // for now, we might skip them or return an error.
                        // Skipping for simplicity.
                        continue; 
                    }
                },
                _ => { /* Skip RootDir, CurDir, ParentDir safely */ }
            }
        }
        abs_path
    }
} use crate::auth::AuthContext;
use crate::types::UserRole;
use crate::domains::core::dependency_checker::DependencyChecker;
use crate::domains::core::delete_service::{BaseDeleteService, DeleteOptions, DeleteService, DeleteServiceRepository};
use crate::domains::core::file_storage_service::FileStorageService;
use crate::domains::core::repository::{DeleteResult, FindById, HardDeletable, SoftDeletable};
use crate::domains::sync::repository::{TombstoneRepository, ChangeLogRepository};
use crate::domains::sync::types::{Tombstone, ChangeLogEntry, ChangeOperationType, SyncPriority};
use crate::domains::document::repository::{
    DocumentAccessLogRepository, DocumentTypeRepository, MediaDocumentRepository,
    DocumentVersionRepository,
};
use crate::domains::document::types::{
    DocumentType, DocumentTypeResponse, MediaDocument, MediaDocumentResponse, NewDocumentType,
    UpdateDocumentType,
    NewMediaDocument,
    DocumentVersion, NewDocumentAccessLog,
    DocumentAccessLog, CompressionStatus, BlobSyncStatus, DocumentAccessType,
    DocumentSummary,
    DocumentFileInfo,
};
use crate::domains::compression::service::CompressionService;
use crate::domains::compression::types::CompressionPriority;
use crate::errors::{DbError, DomainError, DomainResult, ServiceError, ServiceResult, ValidationError};
use crate::types::{PaginatedResult, PaginationParams};
use crate::validation::Validate;
use async_trait::async_trait;
use sqlx::{SqlitePool, Transaction, Sqlite};
use std::sync::Arc;
use std::collections::HashMap;
use uuid::Uuid;
use tokio::fs;
use std::str::FromStr;
use std::path::Path;
use chrono::{Utc, DateTime};
use serde_json;
use crate::domains::core::delete_service::PendingDeletionManager;
// --- Includes Enum ---
#[derive(Debug, Clone, PartialEq, Eq)]
pub enum DocumentInclude {
    DocumentType,
    Versions,
    AccessLogs(PaginationParams),
}

// --- Document Service Trait ---
#[async_trait]
pub trait DocumentService:
    DeleteService<DocumentType> + DeleteService<MediaDocument> + Send + Sync
{
    // Document Type Operations
    async fn create_document_type(
        &self,
        auth: &AuthContext,
        new_type: NewDocumentType,
    ) -> ServiceResult<DocumentTypeResponse>;

    async fn get_document_type_by_id(
        &self,
        id: Uuid,
    ) -> ServiceResult<DocumentTypeResponse>;

    async fn list_document_types(
        &self,
        params: PaginationParams,
    ) -> ServiceResult<PaginatedResult<DocumentTypeResponse>>;

    async fn update_document_type(
        &self,
        auth: &AuthContext,
        id: Uuid,
        update_data: UpdateDocumentType,
    ) -> ServiceResult<DocumentTypeResponse>;

    /// Hard deletes a document type (Admin only)
    async fn delete_document_type(
        &self,
        auth: &AuthContext,
        id: Uuid,
    ) -> ServiceResult<DeleteResult>;

    // Media Document Operations - Create Only
    async fn upload_document(
        &self,
        auth: &AuthContext,
        file_data: Vec<u8>,
        original_filename: String,
        title: Option<String>,
        document_type_id: Uuid,
        related_entity_id: Uuid,
        related_entity_type: String,
        linked_field: Option<String>,
        sync_priority: SyncPriority,
        compression_priority: Option<CompressionPriority>,
        temp_related_id: Option<Uuid>,
    ) -> ServiceResult<MediaDocumentResponse>;

    /// Bulk upload multiple documents with a single shared title
    async fn bulk_upload_documents(
        &self,
        auth: &AuthContext,
        files: Vec<(Vec<u8>, String)>, // (data, filename) - no individual titles
        title: Option<String>, // Single shared title for all documents
        document_type_id: Uuid,
        related_entity_id: Uuid,
        related_entity_type: String,
        sync_priority: SyncPriority,
        compression_priority: Option<CompressionPriority>,
        temp_related_id: Option<Uuid>,
    ) -> ServiceResult<Vec<MediaDocumentResponse>>;

    // Media Document Operations - Read Only
    async fn get_media_document_by_id(
        &self,
        auth: &AuthContext,
        id: Uuid,
        include: Option<&[DocumentInclude]>,
    ) -> ServiceResult<MediaDocumentResponse>;

    async fn list_media_documents_by_related_entity(
        &self,
        auth: &AuthContext,
        related_table: &str,
        related_id: Uuid,
        params: PaginationParams,
        include: Option<&[DocumentInclude]>,
    ) -> ServiceResult<PaginatedResult<MediaDocumentResponse>>;

    // Media Document Operations - Download and Access
    /// Download document data (Admin/TL only), returns None for data if not local
    async fn download_document(
        &self,
        auth: &AuthContext,
        id: Uuid,
    ) -> ServiceResult<(String, Option<Vec<u8>>)>;

    /// Get file path for opening (if available locally)
    async fn open_document(
        &self,
        auth: &AuthContext,
        document_id: Uuid,
    ) -> ServiceResult<Option<String>>;

    /// Check if document is available on current device
    async fn is_document_available_on_device(
        &self,
        document_id: Uuid,
    ) -> ServiceResult<bool>;

    // Delete Operation (Admin only)
    /// Hard deletes a media document (Admin only)
    async fn delete_media_document(
        &self,
        auth: &AuthContext,
        id: Uuid,
    ) -> ServiceResult<DeleteResult>;

    // Document summarization
    /// Calculate document summary by linked fields
    async fn calculate_document_summary_by_linked_fields(
        &self,
        auth: &AuthContext,
        related_table: &str,
        related_id: Uuid,
    ) -> ServiceResult<DocumentSummary>;
    
    /// Link previously uploaded documents with a temporary ID to their final entity
    async fn link_temp_documents(
        &self,
        temp_related_id: Uuid,
        final_related_table: &str,
        final_related_id: Uuid,
    ) -> ServiceResult<u64>; // Returns number of documents linked

    // Active File Usage Tracking
    async fn register_document_in_use(
        &self,
        document_id: Uuid,
        user_id: Uuid,
        device_id: Uuid,
        use_type: &str,  // "view" or "edit"
    ) -> ServiceResult<()>;

    async fn unregister_document_in_use(
        &self,
        document_id: Uuid,
        user_id: Uuid,
        device_id: Uuid,
    ) -> ServiceResult<()>;
}

// --- Service Implementation ---
pub struct DocumentServiceImpl {
    pool: SqlitePool,
    doc_type_repo: Arc<dyn DocumentTypeRepository>,
    media_doc_repo: Arc<dyn MediaDocumentRepository>,
    doc_ver_repo: Arc<dyn DocumentVersionRepository>,
    doc_log_repo: Arc<dyn DocumentAccessLogRepository>,
    delete_service_doc_type: Arc<BaseDeleteService<DocumentType>>,
    delete_service_media_doc: Arc<BaseDeleteService<MediaDocument>>,
    file_storage_service: Arc<dyn FileStorageService>,
    compression_service: Arc<dyn CompressionService>,
}

impl DocumentServiceImpl {
    pub fn new(
        pool: SqlitePool,
        doc_type_repo: Arc<dyn DocumentTypeRepository>,
        media_doc_repo: Arc<dyn MediaDocumentRepository>,
        doc_ver_repo: Arc<dyn DocumentVersionRepository>,
        doc_log_repo: Arc<dyn DocumentAccessLogRepository>,
        tombstone_repo: Arc<dyn TombstoneRepository + Send + Sync>,
        change_log_repo: Arc<dyn ChangeLogRepository + Send + Sync>,
        dependency_checker: Arc<dyn DependencyChecker + Send + Sync>,
        file_storage_service: Arc<dyn FileStorageService>,
        compression_service: Arc<dyn CompressionService>,
        deletion_manager: Arc<PendingDeletionManager>,
    ) -> Self {
        // --- Adapters for Delete Services ---
        struct DocTypeRepoAdapter(Arc<dyn DocumentTypeRepository>);

        #[async_trait]
        impl FindById<DocumentType> for DocTypeRepoAdapter {
            async fn find_by_id(&self, id: Uuid) -> DomainResult<DocumentType> { 
                self.0.find_by_id(id).await 
            }
        }

        #[async_trait]
        impl SoftDeletable for DocTypeRepoAdapter {
            async fn soft_delete(&self, id: Uuid, auth: &AuthContext) -> DomainResult<()> { 
                self.0.soft_delete(id, auth).await 
            }

            async fn soft_delete_with_tx(&self, id: Uuid, auth: &AuthContext, tx: &mut Transaction<'_, Sqlite>) -> DomainResult<()> { 
                self.0.soft_delete_with_tx(id, auth, tx).await 
            }
        }

        #[async_trait]
        impl HardDeletable for DocTypeRepoAdapter {
            fn entity_name(&self) -> &'static str { 
                self.0.entity_name() 
            }

            async fn hard_delete(&self, id: Uuid, auth: &AuthContext) -> DomainResult<()> { 
                self.0.hard_delete(id, auth).await 
            }

            async fn hard_delete_with_tx(&self, id: Uuid, auth: &AuthContext, tx: &mut Transaction<'_, Sqlite>) -> DomainResult<()> { 
                self.0.hard_delete_with_tx(id, auth, tx).await 
            }
        }

        let adapted_doc_type_repo: Arc<dyn DeleteServiceRepository<DocumentType>> = 
            Arc::new(DocTypeRepoAdapter(doc_type_repo.clone()));

        // Adapter for MediaDocument
        struct MediaDocRepoAdapter(Arc<dyn MediaDocumentRepository>);

        #[async_trait]
        impl FindById<MediaDocument> for MediaDocRepoAdapter {
            async fn find_by_id(&self, id: Uuid) -> DomainResult<MediaDocument> { 
                MediaDocumentRepository::find_by_id(&*self.0, id).await 
            }
        }

        #[async_trait]
        impl SoftDeletable for MediaDocRepoAdapter {
            async fn soft_delete(&self, id: Uuid, auth: &AuthContext) -> DomainResult<()> { 
                self.0.soft_delete(id, auth).await 
            }

            async fn soft_delete_with_tx(&self, id: Uuid, auth: &AuthContext, tx: &mut Transaction<'_, Sqlite>) -> DomainResult<()> { 
                self.0.soft_delete_with_tx(id, auth, tx).await 
            }
        }

        #[async_trait]
        impl HardDeletable for MediaDocRepoAdapter {
            fn entity_name(&self) -> &'static str { 
                HardDeletable::entity_name(self.0.as_ref())
            }

            async fn hard_delete(&self, id: Uuid, auth: &AuthContext) -> DomainResult<()> { 
                self.0.hard_delete(id, auth).await 
            }

            async fn hard_delete_with_tx(&self, id: Uuid, auth: &AuthContext, tx: &mut Transaction<'_, Sqlite>) -> DomainResult<()> { 
                self.0.hard_delete_with_tx(id, auth, tx).await 
            }
        }

        let adapted_media_doc_repo: Arc<dyn DeleteServiceRepository<MediaDocument>> = 
            Arc::new(MediaDocRepoAdapter(media_doc_repo.clone()));

        // --- Create Delete Services ---
        let delete_service_doc_type = Arc::new(BaseDeleteService::new(
            pool.clone(),
            adapted_doc_type_repo,
            tombstone_repo.clone(),
            change_log_repo.clone(),
            dependency_checker.clone(),
            None,
            deletion_manager.clone(),
        ));

        let delete_service_media_doc = Arc::new(BaseDeleteService::new(
            pool.clone(),
            adapted_media_doc_repo,
            tombstone_repo,
            change_log_repo,
            dependency_checker,
            Some(media_doc_repo.clone()),
            deletion_manager,
        ));

        Self {
            pool,
            doc_type_repo,
            media_doc_repo,
            doc_ver_repo,
            doc_log_repo,
            delete_service_doc_type,
            delete_service_media_doc,
            file_storage_service,
            compression_service,
        }
    }

    /// Helper to enrich MediaDocumentResponse with included data and check availability
    async fn enrich_response(
        &self,
        mut response: MediaDocumentResponse,
        include: Option<&[DocumentInclude]>,
    ) -> ServiceResult<MediaDocumentResponse> {
        // Determine the primary path and size to display based on compression
        let mut _display_path = response.file_path.clone();
        let mut _display_size = response.size_bytes;

        // Skip availability check for error documents
        if response.has_error {
            response.is_available_locally = false;
        } else {
            if let Some(compressed_path) = &response.compressed_file_path {
                if response.compression_status == CompressionStatus::Completed.as_str() {
                    _display_path = compressed_path.clone();
                    if let Some(compressed_size) = response.compressed_size_bytes {
                        _display_size = compressed_size;
                    }
                }
            }

            // Check if the relevant file (original or completed compressed) is available locally
            let path_to_check = if response.compression_status == CompressionStatus::Completed.as_str() && response.compressed_file_path.is_some() {
                response.compressed_file_path.as_ref().unwrap()
            } else {
                &response.file_path
            };
            let absolute_path = self.file_storage_service.get_absolute_path(path_to_check);

            // Set availability flag based on file existence check
            match fs::metadata(&absolute_path).await {
                Ok(_) => {
                    response.is_available_locally = true;
                },
                Err(e) if e.kind() == std::io::ErrorKind::NotFound => {
                    response.is_available_locally = false;
                }
                Err(e) => {
                    // Log error but don't fail the entire enrichment, set as unavailable
                    eprintln!("Error checking file metadata for {}: {}", absolute_path.display(), e);
                    response.is_available_locally = false;
                }
            }
        }

        // Handle includes
        if let Some(includes) = include {
            for inc in includes {
                match inc {
                    DocumentInclude::DocumentType => {
                        if response.type_name.is_none() {
                            match self.doc_type_repo.find_by_id(response.type_id).await {
                                Ok(doc_type) => {
                                    response.type_name = Some(doc_type.name);
                                }
                                Err(DomainError::EntityNotFound(_, _)) => {
                                    response.type_name = Some("<Type Not Found>".to_string());
                                }
                                Err(e) => return Err(ServiceError::Domain(e)),
                            }
                        }
                    }
                    DocumentInclude::Versions => {
                        let versions = self.doc_ver_repo.find_by_document_id(response.id).await?;
                        response.versions = Some(versions);
                    }
                    DocumentInclude::AccessLogs(params) => {
                        let logs = self.doc_log_repo.find_by_document_id(response.id, params.clone()).await?;
                        response.access_logs = Some(logs);
                    }
                }
            }
        }

        Ok(response)
    }

    // --- Sync Service Interface Methods ---
    
    /// Update compression status - Called by Compression Service
    pub async fn update_compression_status(
        &self,
        id: Uuid,
        status: CompressionStatus,
        compressed_file_path: Option<&str>,
        compressed_size_bytes: Option<i64>,
    ) -> ServiceResult<()> {
        self.media_doc_repo.update_compression_status(id, status, compressed_file_path, compressed_size_bytes).await
            .map_err(|e| ServiceError::Domain(e))
    }

    /// Update blob sync status - Called by Sync Service
    pub async fn update_blob_sync_status(
        &self,
        id: Uuid,
        status: BlobSyncStatus,
        blob_key: Option<&str>,
    ) -> ServiceResult<()> {
        self.media_doc_repo.update_blob_sync_status(id, status, blob_key).await
            .map_err(|e| ServiceError::Domain(e))
    }

    /// Get document file info for sync - Called by Sync Service
    pub async fn get_document_file_info(
        &self,
        document_id: Uuid,
    ) -> ServiceResult<DocumentFileInfo> {
        let doc = MediaDocumentRepository::find_by_id(&*self.media_doc_repo, document_id).await
            .map_err(|e| ServiceError::Domain(e))?;

        if doc.file_path == "ERROR" {
            return Err(ServiceError::Domain(DomainError::Internal("Cannot get file info for error document".to_string())));
        }

        let use_compressed_for_sync = doc.compressed_file_path.is_some() &&
                                      doc.compression_status == CompressionStatus::Completed.as_str();

        let (sync_file_path, sync_size_bytes) = if use_compressed_for_sync {
            (doc.compressed_file_path.as_ref().unwrap().clone(), doc.compressed_size_bytes.unwrap_or(doc.size_bytes))
        } else {
            (doc.file_path.clone(), doc.size_bytes)
        };

        let absolute_path = self.file_storage_service.get_absolute_path(&sync_file_path);

        let file_exists_locally = match fs::metadata(&absolute_path).await {
            Ok(_) => true,
            Err(e) if e.kind() == std::io::ErrorKind::NotFound => false,
            Err(e) => {
                eprintln!("Error checking file metadata for sync info {}: {}", absolute_path.display(), e);
                false
            }
        };

        Ok(DocumentFileInfo {
            id: doc.id,
            file_path: sync_file_path,
            absolute_path: absolute_path.to_string_lossy().to_string(),
            is_compressed: use_compressed_for_sync,
            size_bytes: sync_size_bytes,
            mime_type: doc.mime_type.clone(),
            file_exists_locally,
            blob_status: doc.blob_status.clone(),
            blob_key: doc.blob_key.clone(),
            sync_priority: doc.sync_priority.clone(),
            original_file_path: doc.file_path.clone(),
            original_size_bytes: doc.size_bytes,
            compression_status: CompressionStatus::from_str(&doc.compression_status)
                .unwrap_or(CompressionStatus::Pending)
        })
    }

    /// Update file paths after download - Called by Sync Service
    pub async fn update_document_file_paths(
        &self,
        document_id: Uuid,
        file_path: Option<&str>,
        compressed_file_path: Option<&str>,
        compression_status: Option<CompressionStatus>,
        compressed_size_bytes: Option<i64>,
    ) -> ServiceResult<()> {
        self.media_doc_repo.update_paths_and_status(
            document_id,
            file_path,
            compressed_file_path,
            compressed_size_bytes,
            compression_status,
        ).await.map_err(|e| ServiceError::Domain(e))
    }
}

// --- DeleteService Implementations ---
#[async_trait]
impl DeleteService<DocumentType> for DocumentServiceImpl {
    fn repository(&self) -> &dyn FindById<DocumentType> { 
        self.delete_service_doc_type.repository() 
    }

    fn tombstone_repository(&self) -> &dyn TombstoneRepository { 
        self.delete_service_doc_type.tombstone_repository() 
    }

    fn change_log_repository(&self) -> &dyn ChangeLogRepository { 
        self.delete_service_doc_type.change_log_repository() 
    }

    fn dependency_checker(&self) -> &dyn DependencyChecker { 
        self.delete_service_doc_type.dependency_checker() 
    }

    async fn delete(
        &self, 
        id: Uuid, 
        auth: &AuthContext, 
        options: DeleteOptions
    ) -> DomainResult<DeleteResult> { 
        self.delete_service_doc_type.delete(id, auth, options).await 
    }

    async fn batch_delete(
        &self, 
        ids: &[Uuid], 
        auth: &AuthContext, 
        options: DeleteOptions
    ) -> DomainResult<crate::domains::core::delete_service::BatchDeleteResult> { 
        self.delete_service_doc_type.batch_delete(ids, auth, options).await 
    }

    async fn delete_with_dependencies(
        &self, 
        id: Uuid, 
        auth: &AuthContext
    ) -> DomainResult<DeleteResult> { 
        self.delete_service_doc_type.delete_with_dependencies(id, auth).await 
    }

    async fn get_failed_delete_details(
        &self,
        batch_result: &crate::domains::core::delete_service::BatchDeleteResult,
        auth: &AuthContext,
    ) -> DomainResult<Vec<crate::domains::core::delete_service::FailedDeleteDetail<DocumentType>>> { 
        self.delete_service_doc_type.get_failed_delete_details(batch_result, auth).await 
    }
}

#[async_trait]
impl DeleteService<MediaDocument> for DocumentServiceImpl {
    fn repository(&self) -> &dyn FindById<MediaDocument> { 
        self.delete_service_media_doc.repository() 
    }

    fn tombstone_repository(&self) -> &dyn TombstoneRepository { 
        self.delete_service_media_doc.tombstone_repository() 
    }

    fn change_log_repository(&self) -> &dyn ChangeLogRepository { 
        self.delete_service_media_doc.change_log_repository() 
    }

    fn dependency_checker(&self) -> &dyn DependencyChecker { 
        self.delete_service_media_doc.dependency_checker() 
    }

    async fn delete(
        &self, 
        id: Uuid, 
        auth: &AuthContext, 
        options: DeleteOptions
    ) -> DomainResult<DeleteResult> { 
        self.delete_service_media_doc.delete(id, auth, options).await 
    }

    async fn batch_delete(
        &self, 
        ids: &[Uuid], 
        auth: &AuthContext, 
        options: DeleteOptions
    ) -> DomainResult<crate::domains::core::delete_service::BatchDeleteResult> { 
        self.delete_service_media_doc.batch_delete(ids, auth, options).await 
    }

    async fn delete_with_dependencies(
        &self, 
        id: Uuid, 
        auth: &AuthContext
    ) -> DomainResult<DeleteResult> { 
        self.delete_service_media_doc.delete_with_dependencies(id, auth).await 
    }

    async fn get_failed_delete_details(
        &self,
        batch_result: &crate::domains::core::delete_service::BatchDeleteResult,
        auth: &AuthContext,
    ) -> DomainResult<Vec<crate::domains::core::delete_service::FailedDeleteDetail<MediaDocument>>> { 
        self.delete_service_media_doc.get_failed_delete_details(batch_result, auth).await 
    }
}

// --- DocumentService Implementation ---
#[async_trait]
impl DocumentService for DocumentServiceImpl {
    // --- Document Type Methods ---
    async fn create_document_type(
        &self,
        auth: &AuthContext,
        new_type: NewDocumentType,
    ) -> ServiceResult<DocumentTypeResponse> {
        new_type.validate()?;
        let created = self.doc_type_repo.create(&new_type, auth).await?;
        Ok(DocumentTypeResponse::from(created))
    }

    async fn get_document_type_by_id(
        &self,
        id: Uuid,
    ) -> ServiceResult<DocumentTypeResponse> {
        let doc_type = self.doc_type_repo.find_by_id(id).await?;
        Ok(DocumentTypeResponse::from(doc_type))
    }

    async fn list_document_types(
        &self,
        params: PaginationParams,
    ) -> ServiceResult<PaginatedResult<DocumentTypeResponse>> {
        let paginated_result = self.doc_type_repo.find_all(params).await?;
        let response_items = paginated_result.items.into_iter()
            .map(DocumentTypeResponse::from).collect();
        Ok(PaginatedResult::new(response_items, paginated_result.total, params))
    }

    async fn update_document_type(
        &self,
        auth: &AuthContext,
        id: Uuid,
        update_data: UpdateDocumentType,
    ) -> ServiceResult<DocumentTypeResponse> {
        update_data.validate()?;
        
        if let Some(name) = &update_data.name {
            if let Some(existing) = self.doc_type_repo.find_by_name(name).await? {
                if existing.id != id {
                    return Err(ServiceError::Domain(DomainError::Validation(
                        ValidationError::unique("name")
                    )));
                }
            }
        }
        
        let updated = self.doc_type_repo.update(id, &update_data, auth).await?;
        Ok(DocumentTypeResponse::from(updated))
    }

    async fn delete_document_type(
        &self,
        auth: &AuthContext,
        id: Uuid,
    ) -> ServiceResult<DeleteResult> {
        if auth.role != UserRole::Admin {
            return Err(ServiceError::PermissionDenied(
                "Only admins can hard delete document types".to_string()
            ));
        }

        let options = DeleteOptions { 
            allow_hard_delete: true, 
            fallback_to_soft_delete: false, 
            force: false 
        };

        let result = DeleteService::<DocumentType>::delete(self, id, auth, options).await?;
        Ok(result)
    }

    // --- Media Document Methods ---
    async fn upload_document(
        &self,
        auth: &AuthContext,
        file_data: Vec<u8>,
        original_filename: String,
        title: Option<String>,
        document_type_id: Uuid,
        related_entity_id: Uuid,
        related_entity_type: String,
        linked_field: Option<String>,
        sync_priority: SyncPriority,
        compression_priority: Option<CompressionPriority>,
        temp_related_id: Option<Uuid>,
    ) -> ServiceResult<MediaDocumentResponse> {
        let doc_type = self.doc_type_repo.find_by_id(document_type_id).await?;
        let entity_or_temp_id_str = if let Some(temp_id) = temp_related_id {
            temp_id.to_string()
        } else {
            related_entity_id.to_string()
        };
        
        let file_save_result = self.file_storage_service.save_file(
            file_data.clone(),
            &if temp_related_id.is_some() { "temp" } else { &related_entity_type },
            &entity_or_temp_id_str,
            &original_filename,
        ).await;
        
        match file_save_result {
            Ok((relative_path, size_bytes)) => {
                let new_doc_metadata = NewMediaDocument {
                    id: Uuid::new_v4(),
                    related_table: if temp_related_id.is_some() { "TEMP".to_string() } else { related_entity_type },
                    related_id: if temp_related_id.is_some() { None } else { Some(related_entity_id) },
                    temp_related_id,
                    type_id: document_type_id,
                    original_filename: original_filename.clone(),
                    title: title.or_else(|| Some(original_filename.clone())),
                    mime_type: guess_mime_type(&original_filename),
                    size_bytes: size_bytes as i64,
                    field_identifier: linked_field,
                    sync_priority: sync_priority.as_str().to_string(),
                    created_by_user_id: Some(auth.user_id),
                    file_path: relative_path.clone(),
                    description: None,
                    compression_status: CompressionStatus::Pending.as_str().to_string(),
                    blob_status: BlobSyncStatus::Pending.as_str().to_string(),
                    blob_key: None,
                    compressed_file_path: None,
                    compressed_size_bytes: None,
                };
                new_doc_metadata.validate()?;
                let created_doc = self.media_doc_repo.create(&new_doc_metadata).await?;
                let final_compression_priority = compression_priority
                    .or_else(|| CompressionPriority::from_str(&doc_type.default_priority).ok())
                    .unwrap_or(CompressionPriority::Normal);
                if let Err(e) = self.compression_service
                    .queue_document_for_compression(created_doc.id, final_compression_priority)
                    .await
                {
                    eprintln!("Failed to queue document {} for compression: {:?}", created_doc.id, e);
                }
                let mut response = MediaDocumentResponse::from_doc(&created_doc, Some(doc_type.name));
                response = self.enrich_response(response, None).await?;
                Ok(response)
            },
            Err(e) => {
                let error_message = format!("Failed to save file: {}", e);
                let new_doc_metadata = NewMediaDocument {
                    id: Uuid::new_v4(),
                    related_table: if temp_related_id.is_some() { "TEMP".to_string() } else { related_entity_type },
                    related_id: if temp_related_id.is_some() { None } else { Some(related_entity_id) },
                    temp_related_id,
                    type_id: document_type_id,
                    original_filename: original_filename.clone(),
                    title: title.or_else(|| Some(original_filename.clone())),
                    mime_type: guess_mime_type(&original_filename),
                    size_bytes: 0,
                    field_identifier: linked_field,
                    sync_priority: sync_priority.as_str().to_string(),
                    created_by_user_id: Some(auth.user_id),
                    file_path: "ERROR".to_string(),
                    description: Some(error_message.clone()),
                    compression_status: CompressionStatus::Failed.as_str().to_string(),
                    blob_status: BlobSyncStatus::Failed.as_str().to_string(),
                    blob_key: None,
                    compressed_file_path: None,
                    compressed_size_bytes: None,
                };
                match new_doc_metadata.validate() {
                    Ok(()) => {
                        match self.media_doc_repo.create(&new_doc_metadata).await {
                            Ok(created_doc) => {
                                let mut response = MediaDocumentResponse::from_doc(&created_doc, Some(doc_type.name));
                                response.is_available_locally = false;
                                response.has_error = true;
                                response.error_type = Some("upload_failure".to_string());
                                response.error_message = Some(error_message);
                                Ok(response)
                            },
                            Err(e) => Err(ServiceError::Domain(e))
                        }
                    },
                    Err(val_err) => Err(ServiceError::Domain(val_err))
                }
            }
        }
    }

    async fn bulk_upload_documents(
        &self,
        auth: &AuthContext,
        files: Vec<(Vec<u8>, String)>,
        title: Option<String>,
        document_type_id: Uuid,
        related_entity_id: Uuid,
        related_entity_type: String,
        sync_priority: SyncPriority,
        compression_priority: Option<CompressionPriority>,
        temp_related_id: Option<Uuid>,
    ) -> ServiceResult<Vec<MediaDocumentResponse>> {
        let mut results = Vec::new();
        let doc_type = self.doc_type_repo.find_by_id(document_type_id).await?;
        let final_compression_priority = compression_priority
            .or_else(|| CompressionPriority::from_str(&doc_type.default_priority).ok())
            .unwrap_or(CompressionPriority::Normal);

        for (file_data, original_filename) in files {
            match self.upload_document(
                auth,
                file_data,
                original_filename.clone(),
                title.clone(),
                document_type_id,
                related_entity_id,
                related_entity_type.clone(),
                None,
                sync_priority,
                Some(final_compression_priority),
                temp_related_id,
            ).await {
                Ok(response) => results.push(response),
                Err(e) => {
                    let error_msg_detail = format!("Upload failed: {}", e);
                    let error_response = MediaDocumentResponse {
                        id: Uuid::new_v4(),
                        type_id: document_type_id,
                        type_name: Some(doc_type.name.clone()),
                        title: title.clone(),
                        original_filename: original_filename.clone(),
                        description: Some(error_msg_detail.clone()),
                        field_identifier: None,
                        file_path: "ERROR".to_string(),
                        is_available_locally: false,
                        has_error: true,
                        error_type: Some("upload_failure".to_string()),
                        error_message: Some(error_msg_detail),
                        size_bytes: 0,
                        mime_type: "application/octet-stream".to_string(),
                        created_at: Utc::now().to_rfc3339(),
                        updated_at: Utc::now().to_rfc3339(),
                        created_by_user_id: Some(auth.user_id),
                        compression_status: CompressionStatus::Failed.as_str().to_string(),
                        blob_status: BlobSyncStatus::Failed.as_str().to_string(),
                        sync_priority: sync_priority.as_str().to_string(),
                        related_id: if temp_related_id.is_some() { None } else { Some(related_entity_id) },
                        related_table: if temp_related_id.is_some() { "TEMP".to_string() } else { related_entity_type.clone() },
                        temp_related_id,
                        compressed_file_path: None,
                        compressed_size_bytes: None,
                        versions: None,
                        access_logs: None,
                    };
                    results.push(error_response);
                }
            }
        }
        Ok(results)
    }

    async fn get_media_document_by_id(
        &self,
        auth: &AuthContext,
        id: Uuid,
        include: Option<&[DocumentInclude]>,
    ) -> ServiceResult<MediaDocumentResponse> {
        let doc = MediaDocumentRepository::find_by_id(&*self.media_doc_repo, id).await?;
        let new_log = NewDocumentAccessLog { document_id: id, user_id: auth.user_id, access_type: DocumentAccessType::View.as_str().to_string(), details: None };
        if let Err(e) = self.doc_log_repo.create(&new_log).await { eprintln!("Failed to log document access for {}: {:?}", id, e); }
        let type_name = if include.map_or(false, |incs| incs.contains(&DocumentInclude::DocumentType)) { None } else { self.doc_type_repo.find_by_id(doc.type_id).await.ok().map(|dt| dt.name) };
        let mut response = MediaDocumentResponse::from_doc(&doc, type_name);
        response = self.enrich_response(response, include).await?;
        Ok(response)
    }

    async fn list_media_documents_by_related_entity(
        &self,
        auth: &AuthContext,
        related_table: &str,
        related_id: Uuid,
        params: PaginationParams,
        include: Option<&[DocumentInclude]>,
    ) -> ServiceResult<PaginatedResult<MediaDocumentResponse>> {
        let result = self.media_doc_repo.find_by_related_entity(related_table, related_id, params.clone()).await?;
        let mut response_items = Vec::new();
        let type_map = HashMap::<Uuid, String>::new();
        for item in result.items {
            let initial_type_name = type_map.get(&item.type_id).cloned();
            let response = MediaDocumentResponse::from_doc(&item, initial_type_name);
            let enriched_response = self.enrich_response(response, include).await?;
            response_items.push(enriched_response);
        }
        Ok(PaginatedResult::new(response_items, result.total, params))
    }

    async fn download_document(
        &self,
        auth: &AuthContext,
        id: Uuid,
    ) -> ServiceResult<(String, Option<Vec<u8>>)> {
        if auth.role != UserRole::Admin && auth.role != UserRole::FieldTeamLead { return Err(ServiceError::PermissionDenied("Admin or Field Team Lead required to download documents".to_string())); }
        let doc = MediaDocumentRepository::find_by_id(&*self.media_doc_repo, id).await?;
        if doc.file_path == "ERROR" {
            let new_log = NewDocumentAccessLog { document_id: id, user_id: auth.user_id, access_type: DocumentAccessType::AttemptDownload.as_str().to_string(), details: Some("Attempted to download error document".to_string()) };
            if let Err(e) = self.doc_log_repo.create(&new_log).await { eprintln!("Failed to log error document download attempt for {}: {:?}", id, e); }
            return Err(ServiceError::Ui(format!("Document has an error status: {}", doc.description.unwrap_or_else(|| "Unknown error".to_string()))));
        }
        let file_path_to_check = if let Some(compressed_path) = &doc.compressed_file_path { if doc.compression_status == CompressionStatus::Completed.as_str() { compressed_path } else { &doc.file_path } } else { &doc.file_path };
        let absolute_path = self.file_storage_service.get_absolute_path(file_path_to_check);
        match fs::metadata(&absolute_path).await {
            Ok(_) => {
                match fs::File::open(&absolute_path).await {
                    Ok(_) => {
                        match self.file_storage_service.get_file_data(file_path_to_check).await {
                            Ok(file_data) => {
                                let new_log = NewDocumentAccessLog { document_id: id, user_id: auth.user_id, access_type: DocumentAccessType::Download.as_str().to_string(), details: None };
                                if let Err(e) = self.doc_log_repo.create(&new_log).await { eprintln!("Failed to log document download for {}: {:?}", id, e); }
                                Ok((doc.original_filename.clone(), Some(file_data)))
                            },
                            Err(e) => {
                                eprintln!("Error reading file data for {}: {}", absolute_path.display(), e);
                                Err(ServiceError::Domain(DomainError::Internal(format!("Error reading document file: {}", e))))
                            }
                        }
                    },
                    Err(e) => {
                        let details = Some(format!("File exists but is locked/inaccessible: {}", e));
                        let new_log = NewDocumentAccessLog { document_id: id, user_id: auth.user_id, access_type: DocumentAccessType::AttemptDownload.as_str().to_string(), details };
                        if let Err(log_err) = self.doc_log_repo.create(&new_log).await { eprintln!("Failed to log locked download attempt for {}: {:?}", id, log_err); }
                        if doc.compression_status == CompressionStatus::InProgress.as_str() { Err(ServiceError::Ui("Document is currently being compressed. Please try again shortly.".to_string())) } else { Err(ServiceError::Ui("Cannot access document file. It may be in use.".to_string())) }
                    }
                }
            },
            Err(e) if e.kind() == std::io::ErrorKind::NotFound => {
                let new_log = NewDocumentAccessLog { document_id: id, user_id: auth.user_id, access_type: DocumentAccessType::RequestDownload.as_str().to_string(), details: Some("File not found locally, requires download/sync".to_string()) };
                if let Err(log_err) = self.doc_log_repo.create(&new_log).await { eprintln!("Failed to log download request for {}: {:?}", id, log_err); }
                Ok((doc.original_filename.clone(), None))
            }
            Err(e) => {
                Err(ServiceError::Domain(DomainError::Internal(format!("Error checking document file: {}", e))))
            }
        }
    }

    async fn open_document(
        &self,
        auth: &AuthContext,
        document_id: Uuid,
    ) -> ServiceResult<Option<String>> {
        let doc = MediaDocumentRepository::find_by_id(&*self.media_doc_repo, document_id).await?;
        
        // Check for error documents
        if doc.file_path == "ERROR" {
            let new_log = NewDocumentAccessLog { document_id, user_id: auth.user_id, access_type: DocumentAccessType::AttemptView.as_str().to_string(), details: Some("Attempted to open error document".to_string()) };
            if let Err(e) = self.doc_log_repo.create(&new_log).await { eprintln!("Failed to log error document open attempt for {}: {:?}", document_id, e); }
            return Err(ServiceError::Ui(format!("Document has an error status: {}", doc.description.unwrap_or_else(|| "Unknown error".to_string()))));
        }

        // *** ADDED: Check compression status ***
        if doc.compression_status == CompressionStatus::InProgress.as_str() {
            // Log attempt to view while compressing
            let new_log = NewDocumentAccessLog { document_id, user_id: auth.user_id, access_type: DocumentAccessType::AttemptView.as_str().to_string(), details: Some("Attempted view during compression".to_string()) };
            if let Err(log_err) = self.doc_log_repo.create(&new_log).await { eprintln!("Failed to log view attempt during compression for {}: {:?}", document_id, log_err); }
            return Err(ServiceError::Ui("Document is currently being compressed. Please try again shortly.".to_string()));
        }

        // *** ADDED: Register usage ***
        // Attempt to parse device_id, use a placeholder if invalid
        let device_id = Uuid::parse_str(&auth.device_id).unwrap_or_else(|_| Uuid::new_v4()); 
        if let Err(e) = self.register_document_in_use(document_id, auth.user_id, device_id, "view").await {
            eprintln!("Failed to register document {} in use: {:?}", document_id, e);
            // Log error, but proceed with opening anyway
        }

        let file_path_to_check = if let Some(compressed_path) = &doc.compressed_file_path { if doc.compression_status == CompressionStatus::Completed.as_str() { compressed_path } else { &doc.file_path } } else { &doc.file_path };
        let absolute_path = self.file_storage_service.get_absolute_path(file_path_to_check);
        match fs::metadata(&absolute_path).await {
            Ok(_) => {
                match fs::File::open(&absolute_path).await {
                    Ok(_) => {
                        let new_log = NewDocumentAccessLog { document_id, user_id: auth.user_id, access_type: DocumentAccessType::View.as_str().to_string(), details: Some("Opened locally".to_string()) };
                        if let Err(e) = self.doc_log_repo.create(&new_log).await { eprintln!("Failed to log document view for {}: {:?}", document_id, e); }
                        #[cfg(target_os = "ios")] { let ios_path = format!("file://{}", absolute_path.display()); Ok(Some(ios_path)) }
                        #[cfg(not(target_os = "ios"))] { Ok(Some(absolute_path.to_string_lossy().to_string())) }
                    },
                    Err(e) => {
                        let details = Some(format!("File exists but is locked/inaccessible: {}", e));
                        let new_log = NewDocumentAccessLog { document_id, user_id: auth.user_id, access_type: DocumentAccessType::AttemptView.as_str().to_string(), details };
                        if let Err(log_err) = self.doc_log_repo.create(&new_log).await { eprintln!("Failed to log locked view attempt for {}: {:?}", document_id, log_err); }
                        // Removed compression check here as it's done earlier
                        Err(ServiceError::Ui("Cannot open document file. It may be in use.".to_string()))
                    }
                }
            },
            Err(e) if e.kind() == std::io::ErrorKind::NotFound => {
                let new_log = NewDocumentAccessLog { document_id, user_id: auth.user_id, access_type: DocumentAccessType::AttemptView.as_str().to_string(), details: Some("File not found locally".to_string()) };
                if let Err(log_err) = self.doc_log_repo.create(&new_log).await { eprintln!("Failed to log view attempt for {}: {:?}", document_id, log_err); }
                Ok(None)
            },
            Err(e) => {
                Err(ServiceError::Domain(DomainError::Internal(format!("Error checking document file: {}", e))))
            }
        }
    }

    async fn is_document_available_on_device(
        &self,
        document_id: Uuid,
    ) -> ServiceResult<bool> {
        let doc = MediaDocumentRepository::find_by_id(&*self.media_doc_repo, document_id)
            .await.map_err(|e| ServiceError::Domain(e))?;
        
        if doc.file_path == "ERROR" {
            return Ok(false);
        }

        let file_path_to_check = if let Some(compressed_path) = &doc.compressed_file_path {
            if doc.compression_status == CompressionStatus::Completed.as_str() {
                compressed_path
            } else {
                &doc.file_path
            }
        } else {
            &doc.file_path
        };

        let absolute_path = self.file_storage_service.get_absolute_path(file_path_to_check);

        match fs::metadata(&absolute_path).await {
            Ok(_) => Ok(true),
            Err(e) if e.kind() == std::io::ErrorKind::NotFound => Ok(false),
            Err(e) => Err(ServiceError::Domain(DomainError::Internal(format!("Failed to check file availability: {}", e)))),
        }
    }

    /// Implements proper individual document deletion with file cleanup
    async fn delete_media_document(
        &self,
        auth: &AuthContext,
        id: Uuid,
    ) -> ServiceResult<DeleteResult> {
        if auth.role != UserRole::Admin { return Err(ServiceError::PermissionDenied("Only admins can hard delete documents".to_string())); }
        let doc_to_delete = match MediaDocumentRepository::find_by_id(&*self.media_doc_repo, id).await {
            Ok(doc) => doc,
            Err(e @ DomainError::EntityNotFound(_, _)) => return Err(ServiceError::Domain(e)), 
            Err(e) => return Err(ServiceError::Domain(e)), 
        };
        let doc_id_str = id.to_string();
        let active_users_result = sqlx::query!(
            r#"SELECT COUNT(*) as count FROM active_file_usage WHERE document_id = ? AND last_active_at > datetime('now', '-5 minutes')"#,
            doc_id_str
        ).fetch_one(&self.pool).await;
        let active_users = match active_users_result { Ok(result) => result.count, Err(e) => { eprintln!("Failed to query active file usage for {}: {:?}. Assuming 0.", id, e); 0 } };
        if active_users > 0 { return Err(ServiceError::Ui(format!("Document is currently in use by {} user(s). Please try again later.", active_users))); }
        
        let mut tx = self.pool.begin().await.map_err(|e| ServiceError::Domain(DomainError::Database(DbError::from(e))))?;
        let mut tombstone = Tombstone::new(id, "media_documents", auth.user_id);
        let metadata = serde_json::json!({
            "file_path": doc_to_delete.file_path,
            "compressed_file_path": doc_to_delete.compressed_file_path,
            "original_filename": doc_to_delete.original_filename,
            "mime_type": doc_to_delete.mime_type,
            "size_bytes": doc_to_delete.size_bytes,
            "related_table": doc_to_delete.related_table,
            "related_id": doc_to_delete.related_id,
            "type_id": doc_to_delete.type_id,
            "deletion_type": "individual",
            "timestamp": Utc::now().to_rfc3339()
        });
        tombstone.additional_metadata = Some(metadata.to_string());
        let operation_id = tombstone.operation_id;
        if let Err(e) = self.delete_service_media_doc.tombstone_repository().create_tombstone_with_tx(&tombstone, &mut tx).await {
             let _ = tx.rollback().await;
             return Err(ServiceError::Domain(e));
        }
        
        let change_log = ChangeLogEntry {
            operation_id, entity_table: "media_documents".to_string(), entity_id: id, operation_type: ChangeOperationType::HardDelete, field_name: None, old_value: None, new_value: None,
            document_metadata: Some(metadata.to_string()), timestamp: Utc::now(), user_id: auth.user_id, device_id: auth.device_id.parse().ok(), sync_batch_id: None, processed_at: None, sync_error: None,
        };
        if let Err(e) = self.delete_service_media_doc.change_log_repository().create_change_log_with_tx(&change_log, &mut tx).await {
             let _ = tx.rollback().await;
             return Err(ServiceError::Domain(e));
        }
        
        if doc_to_delete.compression_status == CompressionStatus::InProgress.as_str() {
            if let Err(e) = self.compression_service.cancel_compression(id).await { eprintln!("Failed to cancel compression for deleted document {}: {:?}", id, e); }
        }
        
        let queue_id_str = Uuid::new_v4().to_string();
        let doc_id_q_str = id.to_string();
        let requested_at_str = Utc::now().to_rfc3339();
        let user_id_str = auth.user_id.to_string();
        let grace_period: i64 = 86400;
        if let Err(e) = sqlx::query!(
            r#"INSERT INTO file_deletion_queue (id, document_id, file_path, compressed_file_path, requested_at, requested_by, grace_period_seconds) VALUES (?, ?, ?, ?, ?, ?, ?)"#,
            queue_id_str, doc_id_q_str, doc_to_delete.file_path, doc_to_delete.compressed_file_path, requested_at_str, user_id_str, grace_period
        ).execute(&mut *tx).await {
             eprintln!("Failed to queue file for deletion DB insert {}: {:?}. Rolling back.", id, e);
             let _ = tx.rollback().await;
             return Err(ServiceError::Domain(DomainError::Database(DbError::from(e))));
        }
        
        if let Err(e) = self.media_doc_repo.hard_delete_with_tx(id, auth, &mut tx).await {
             let _ = tx.rollback().await;
             return Err(ServiceError::Domain(e));
        }
        
        let log_id_str = Uuid::new_v4().to_string();
        let log_doc_id_str = id.to_string();
        let log_user_id_str = auth.user_id.to_string();
        let log_access_type = DocumentAccessType::Delete.as_str();
        let log_timestamp_str = Utc::now().to_rfc3339();
        let log_details = "Admin hard delete".to_string();
        if let Err(e) = sqlx::query!(
            r#"INSERT INTO document_access_logs (id, document_id, user_id, access_type, access_date, details) VALUES (?, ?, ?, ?, ?, ?)"#,
            log_id_str, log_doc_id_str, log_user_id_str, log_access_type, log_timestamp_str, log_details
        ).execute(&mut *tx).await {
             eprintln!("Failed to log hard delete access for document {}: {:?}", id, e);
        }
        
        if let Err(e) = tx.commit().await {
            return Err(ServiceError::Domain(DomainError::Database(DbError::from(e))));
        }
        
            if doc_to_delete.file_path != "ERROR" {
            let mut original_deleted = false;
            let mut compressed_deleted = false;
    
            // Try to delete original file
                if let Err(e) = self.file_storage_service.delete_file(&doc_to_delete.file_path).await {
                 // Only log if it's not a NotFound error, as that's expected if already deleted
                 if !matches!(e, crate::domains::core::file_storage_service::FileStorageError::NotFound(_)) {
                    eprintln!("Warning: Could not immediately delete file {}: {:?}", doc_to_delete.file_path, e);
                 }
                 // Consider NotFound as success for queue update purposes
                 original_deleted = matches!(e, crate::domains::core::file_storage_service::FileStorageError::NotFound(_));
            } else {
                original_deleted = true;
                }

            // Try to delete compressed file if it exists
                if let Some(compressed_path) = &doc_to_delete.compressed_file_path {
                    if let Err(e) = self.file_storage_service.delete_file(compressed_path).await {
                    if !matches!(e, crate::domains::core::file_storage_service::FileStorageError::NotFound(_)) {
                        eprintln!("Warning: Could not immediately delete compressed file {}: {:?}", compressed_path, e);
                    }
                    compressed_deleted = matches!(e, crate::domains::core::file_storage_service::FileStorageError::NotFound(_));
                } else {
                    compressed_deleted = true;
                }
            } else {
                // No compressed file, consider it "deleted" for the check
                compressed_deleted = true;
            }
            
            // ** ADDED OPTIMIZATION: Update queue if both files deleted **
            if original_deleted && compressed_deleted {
                let doc_id_str = id.to_string();
                let now_str = Utc::now().to_rfc3339();
                let attempts_val: i64 = 1; // Set attempts to 1 since we tried
                
                if let Err(e) = sqlx::query!(
                    r#"
                    UPDATE file_deletion_queue 
                     SET completed_at = ?, 
                         last_attempt_at = ?, 
                         attempts = ? 
                     WHERE document_id = ? AND completed_at IS NULL 
                    "#,
                    now_str, // completed_at
                    now_str, // last_attempt_at
                    attempts_val, // attempts
                    doc_id_str // document_id
                ).execute(&self.pool).await {
                    // Log but don't fail the overall operation if this update fails
                    eprintln!("Warning: Could not update file deletion queue entry after successful immediate delete for doc {}: {:?}", doc_id_str, e);
        }
            }
        }
        Ok(DeleteResult::HardDeleted)
    }

    async fn calculate_document_summary_by_linked_fields(
        &self,
        auth: &AuthContext,
        related_table: &str,
        related_id: Uuid,
    ) -> ServiceResult<DocumentSummary> {
        let doc_params = PaginationParams { page: 1, per_page: 10000 };
        let docs_result = self.media_doc_repo.find_by_related_entity(related_table, related_id, doc_params).await?;
        let mut linked_fields = HashMap::new();
        let mut unlinked_count: i64 = 0;
        let total_count = docs_result.items.len() as i64;
        for doc in &docs_result.items {
            if let Some(field) = &doc.field_identifier { let cleaned_field = field.trim().to_lowercase(); if !cleaned_field.is_empty() { *linked_fields.entry(cleaned_field).or_insert(0i64) += 1; } else { unlinked_count += 1; } } else { unlinked_count += 1; }
        }
        Ok(DocumentSummary { total_count, unlinked_count, linked_fields })
    }
    
    /// Link previously uploaded documents with a temporary ID to their final entity
    async fn link_temp_documents(
        &self,
        temp_related_id: Uuid,
        final_related_table: &str,
        final_related_id: Uuid,
    ) -> ServiceResult<u64> {
        let mut tx = self.pool.begin().await.map_err(|e| ServiceError::Domain(DomainError::Database(DbError::from(e))))?;
        let count = match self.media_doc_repo.link_temp_documents_with_tx(temp_related_id, final_related_table, final_related_id, &mut tx).await {
            Ok(count) => count,
            Err(e) => { let _ = tx.rollback().await; return Err(ServiceError::Domain(e)); }
        };
        tx.commit().await.map_err(|e| ServiceError::Domain(DomainError::Database(DbError::from(e))))?;
        Ok(count)
    }

    // Active File Usage Tracking
    async fn register_document_in_use(
        &self,
        document_id: Uuid,
        user_id: Uuid,
        device_id: Uuid,
        use_type: &str,  // "view" or "edit"
    ) -> ServiceResult<()> {
        let id_str = Uuid::new_v4().to_string();
        let doc_id_str = document_id.to_string();
        let user_id_str = user_id.to_string();
        let device_id_str = device_id.to_string();
        let now_str = Utc::now().to_rfc3339();
        let use_type_str = use_type.to_string();
        
        sqlx::query!(
            r#"INSERT INTO active_file_usage (id, document_id, user_id, device_id, started_at, last_active_at, use_type) VALUES (?, ?, ?, ?, ?, ?, ?) ON CONFLICT(document_id, user_id, device_id) DO UPDATE SET last_active_at = excluded.last_active_at, use_type = excluded.use_type"#,
            id_str, doc_id_str, user_id_str, device_id_str, now_str, now_str, use_type_str
        ).execute(&self.pool).await.map_err(|e| ServiceError::Domain(DomainError::Database(DbError::from(e))))?;
        Ok(())
    }

    async fn unregister_document_in_use(
        &self,
        document_id: Uuid,
        user_id: Uuid,
        device_id: Uuid,
    ) -> ServiceResult<()> {
        let doc_id_str = document_id.to_string();
        let user_id_str = user_id.to_string();
        let device_id_str = device_id.to_string();
        
        sqlx::query!(
            r#"DELETE FROM active_file_usage WHERE document_id = ? AND user_id = ? AND device_id = ?"#,
            doc_id_str, user_id_str, device_id_str
        ).execute(&self.pool).await.map_err(|e| ServiceError::Domain(DomainError::Database(DbError::from(e))))?;
        Ok(())
    }
}

/// Utility function to guess mime type from filename
fn guess_mime_type(filename: &str) -> String {
    let ext = filename.split('.').last().unwrap_or("").to_lowercase();
    match ext.as_str() {
        "jpg" | "jpeg" => "image/jpeg", "png" => "image/png", "gif" => "image/gif", "pdf" => "application/pdf", "doc" => "application/msword", "docx" => "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "xls" => "application/vnd.ms-excel", "xlsx" => "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet", "ppt" => "application/vnd.ms-powerpoint", "pptx" => "application/vnd.openxmlformats-officedocument.presentationml.presentation", "txt" => "text/plain", "html" | "htm" => "text/html", "csv" => "text/csv", "mp3" => "audio/mpeg", "mp4" => "video/mp4", "mov" => "video/quicktime", "zip" => "application/zip", _ => "application/octet-stream",
    }.to_string()
}

/// Extension methods for the document service to handle deferred deletions and worker start
impl DocumentServiceImpl {
    /// Register document as being actively viewed or edited
    pub async fn register_document_in_use(
        &self,
        document_id: Uuid,
        user_id: Uuid,
        device_id: Uuid,
        use_type: &str, // "view" or "edit"
    ) -> ServiceResult<()> {
        let id_str = Uuid::new_v4().to_string();
        let doc_id_str = document_id.to_string();
        let user_id_str = user_id.to_string();
        let device_id_str = device_id.to_string();
        let now_str = Utc::now().to_rfc3339();
        let use_type_str = use_type.to_string();
        
        sqlx::query!(
            r#"INSERT INTO active_file_usage (id, document_id, user_id, device_id, started_at, last_active_at, use_type) VALUES (?, ?, ?, ?, ?, ?, ?) ON CONFLICT(document_id, user_id, device_id) DO UPDATE SET last_active_at = excluded.last_active_at, use_type = excluded.use_type"#,
            id_str, doc_id_str, user_id_str, device_id_str, now_str, now_str, use_type_str
        ).execute(&self.pool).await.map_err(|e| ServiceError::Domain(DomainError::Database(DbError::from(e))))?;
        
        Ok(())
    }
    
    /// Mark document as no longer in use
    pub async fn unregister_document_in_use(
        &self,
        document_id: Uuid,
        user_id: Uuid,
        device_id: Uuid,
    ) -> ServiceResult<()> {
        let doc_id_str = document_id.to_string();
        let user_id_str = user_id.to_string();
        let device_id_str = device_id.to_string();
        
        sqlx::query!(
            r#"DELETE FROM active_file_usage WHERE document_id = ? AND user_id = ? AND device_id = ?"#,
            doc_id_str, user_id_str, device_id_str
        ).execute(&self.pool).await.map_err(|e| ServiceError::Domain(DomainError::Database(DbError::from(e))))?;
        Ok(())
    }
    
    /// Start the file deletion worker
    pub fn start_file_deletion_worker(
        pool: SqlitePool,
        file_storage_service: Arc<dyn FileStorageService>,
    ) -> tokio::sync::oneshot::Sender<()> {
        let (shutdown_tx, shutdown_rx) = tokio::sync::oneshot::channel();
        
        // Need to import FileDeletionWorker here
        use crate::domains::document::file_deletion_worker::FileDeletionWorker;

        let worker = FileDeletionWorker::new(pool, file_storage_service)
            .with_shutdown_signal(shutdown_rx);
        
        tokio::spawn(async move {
            if let Err(e) = worker.start().await {
                eprintln!("File deletion worker stopped with error: {:?}", e);
            }
        });
        
        shutdown_tx
    }
}use crate::auth::AuthContext;
use crate::domains::core::repository::{FindById, SoftDeletable, HardDeletable};
use crate::domains::core::delete_service::DeleteServiceRepository;
use crate::domains::document::types::{
    DocumentType, DocumentTypeRow, NewDocumentType, UpdateDocumentType,
    MediaDocument, MediaDocumentRow, NewMediaDocument, CompressionStatus, BlobSyncStatus,
    // UpdateMediaDocument, // REMOVED
    DocumentVersion, DocumentVersionRow,
    DocumentAccessLog, DocumentAccessLogRow, NewDocumentAccessLog
};
use crate::errors::{DbError, DomainError, DomainResult, ValidationError};
use crate::domains::sync::types::SyncPriority;
use async_trait::async_trait;
use chrono::Utc;
use sqlx::{query, query_as, query_scalar, Pool, Row, Sqlite, Transaction};
use uuid::Uuid;
use std::collections::HashMap; // For update_paths_and_status
use std::str::FromStr; // Import FromStr if not already imported at the top of the file
use crate::types::{PaginatedResult, PaginationParams};
// --- Add imports for ChangeLog ---
use crate::domains::sync::repository::ChangeLogRepository;
use crate::domains::sync::types::{ChangeLogEntry, ChangeOperationType, MergeOutcome};
use std::sync::Arc;
use serde_json;
use crate::domains::sync::repository::MergeableEntityRepository;
use crate::domains::document::types::MediaDocumentFullState;
use crate::domains::sync::types::SyncPriority as SyncPriorityFromSyncDomain;

/// Temporary table identifier for linking documents before the main entity exists
pub const TEMP_RELATED_TABLE: &str = "TEMP";

// --- Document Type Repository ---

#[async_trait]
pub trait DocumentTypeRepository: DeleteServiceRepository<DocumentType> + Send + Sync {
    async fn create(
        &self,
        new_type: &NewDocumentType,
        auth: &AuthContext,
    ) -> DomainResult<DocumentType>;

    async fn update(
        &self,
        id: Uuid,
        update_data: &UpdateDocumentType,
        auth: &AuthContext,
    ) -> DomainResult<DocumentType>;

    async fn find_all(&self, params: PaginationParams) -> DomainResult<PaginatedResult<DocumentType>>;

    async fn find_by_name(&self, name: &str) -> DomainResult<Option<DocumentType>>;
}

pub struct SqliteDocumentTypeRepository {
    pool: Pool<Sqlite>,
    change_log_repo: Arc<dyn ChangeLogRepository + Send + Sync>,
}

impl SqliteDocumentTypeRepository {
    pub fn new(pool: Pool<Sqlite>, change_log_repo: Arc<dyn ChangeLogRepository + Send + Sync>) -> Self {
        Self { pool, change_log_repo }
    }
    fn entity_name() -> &'static str {
        "document_types" // Table name
    }
    fn map_row(row: DocumentTypeRow) -> DomainResult<DocumentType> {
        row.into_entity() // Use the conversion method
    }
    // Helper for internal use within transactions
    async fn find_by_id_with_tx<'t>(
        &self,
        id: Uuid,
        tx: &mut Transaction<'t, Sqlite>,
    ) -> DomainResult<DocumentType> {
        query_as::<_, DocumentTypeRow>("SELECT * FROM document_types WHERE id = ? AND deleted_at IS NULL")
            .bind(id.to_string())
            .fetch_optional(&mut **tx)
            .await
            .map_err(DbError::from)?
            .ok_or_else(|| DomainError::EntityNotFound(Self::entity_name().to_string(), id))
            .and_then(Self::map_row)
    }
    // Helper to log changes consistently
    async fn log_change_entry<'t>(
        &self,
        entry: ChangeLogEntry,
        tx: &mut Transaction<'t, Sqlite>,
    ) -> DomainResult<()> {
        self.change_log_repo.create_change_log_with_tx(&entry, tx).await
    }
}

#[async_trait]
impl FindById<DocumentType> for SqliteDocumentTypeRepository {
    async fn find_by_id(&self, id: Uuid) -> DomainResult<DocumentType> {
        query_as::<_, DocumentTypeRow>("SELECT * FROM document_types WHERE id = ? AND deleted_at IS NULL")
            .bind(id.to_string())
            .fetch_optional(&self.pool)
            .await
            .map_err(DbError::from)?
            .ok_or_else(|| DomainError::EntityNotFound(self.entity_name().to_string(), id))
            .and_then(Self::map_row)
    }
}

#[async_trait]
impl SoftDeletable for SqliteDocumentTypeRepository {
    async fn soft_delete_with_tx(
        &self,
        id: Uuid,
        auth: &AuthContext,
        tx: &mut Transaction<'_, Sqlite>,
    ) -> DomainResult<()> {
        let now = Utc::now().to_rfc3339();
        let now_dt = Utc::now(); // For logging
        let user_uuid = auth.user_id;
        let device_uuid: Option<Uuid> = auth.device_id.parse::<Uuid>().ok();

        let result = query(
            "UPDATE document_types SET deleted_at = ?, deleted_by_user_id = ?, updated_at = ? WHERE id = ? AND deleted_at IS NULL"
        )
        .bind(&now)
        .bind(auth.user_id.to_string())
        .bind(&now)
        .bind(id.to_string())
        .execute(&mut **tx)
        .await
        .map_err(DbError::from)?;

        if result.rows_affected() == 0 {
            // Could be already deleted or not found
            Err(DomainError::EntityNotFound(Self::entity_name().to_string(), id))
        } else {
            Ok(())
        }
    }
    async fn soft_delete(&self, id: Uuid, auth: &AuthContext) -> DomainResult<()> {
        let mut tx = self.pool.begin().await.map_err(DbError::from)?;
        let result = self.soft_delete_with_tx(id, auth, &mut tx).await;
        match result {
            Ok(_) => { tx.commit().await.map_err(DbError::from)?; Ok(()) },
            Err(e) => { let _ = tx.rollback().await; Err(e) },
        }
    }
}

#[async_trait]
impl HardDeletable for SqliteDocumentTypeRepository {
    fn entity_name(&self) -> &'static str {
        "document_types"
    }
    async fn hard_delete_with_tx(
        &self,
        id: Uuid,
        _auth: &AuthContext, // Auth context might be used for logging/checks later
        tx: &mut Transaction<'_, Sqlite>,
    ) -> DomainResult<()> {
        let now_dt = Utc::now(); // For logging
        let user_uuid = _auth.user_id;
        let device_uuid: Option<Uuid> = _auth.device_id.parse::<Uuid>().ok();

        let result = query("DELETE FROM document_types WHERE id = ?")
            .bind(id.to_string())
            .execute(&mut **tx)
            .await
            .map_err(DbError::from)?;

        if result.rows_affected() == 0 {
            Err(DomainError::EntityNotFound(<Self as HardDeletable>::entity_name(self).to_string(), id))
        } else {
            Ok(())
        }
    }
    async fn hard_delete(&self, id: Uuid, auth: &AuthContext) -> DomainResult<()> {
        let mut tx = self.pool.begin().await.map_err(DbError::from)?;
        // Consider adding file system cleanup logic here or in the service calling this
        let result = self.hard_delete_with_tx(id, auth, &mut tx).await;
        match result {
            Ok(_) => { tx.commit().await.map_err(DbError::from)?; Ok(()) },
            Err(e) => { let _ = tx.rollback().await; Err(e) },
        }
    }
}

#[async_trait]
impl DocumentTypeRepository for SqliteDocumentTypeRepository {
    async fn create(
        &self,
        new_type: &NewDocumentType,
        auth: &AuthContext,
    ) -> DomainResult<DocumentType> {
        let id = Uuid::new_v4();
        let mut tx = self.pool.begin().await.map_err(DbError::from)?;

        let result = async {
            let now = Utc::now().to_rfc3339();
            let now_dt = Utc::now(); // Use DateTime<Utc> for logging
            let user_id = auth.user_id.to_string();
            let user_uuid = auth.user_id;
            let device_uuid: Option<Uuid> = auth.device_id.parse::<Uuid>().ok();

            query(
                r#"INSERT INTO document_types (
                    id, name, description, icon, color, default_priority, // Now TEXT
                    created_at, updated_at, created_by_user_id, updated_by_user_id,
                    name_updated_at, name_updated_by, description_updated_at, description_updated_by,
                    icon_updated_at, icon_updated_by, color_updated_at, color_updated_by
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)"# 
            )
            .bind(id.to_string())
            .bind(&new_type.name)
            .bind(&new_type.description)
            .bind(&new_type.icon)
            .bind(&new_type.color)
            .bind(new_type.default_priority.as_str()) // Bind default_priority as TEXT
            .bind(&now).bind(&now)
            .bind(&user_id).bind(&user_id)
            // LWW fields initialization
            .bind(&now).bind(&user_id) // name
            .bind(new_type.description.as_ref().map(|_| &now))
            .bind(new_type.description.as_ref().map(|_| &user_id)) // description
            .bind(new_type.icon.as_ref().map(|_| &now))
            .bind(new_type.icon.as_ref().map(|_| &user_id)) // icon
            .bind(new_type.color.as_ref().map(|_| &now))
            .bind(new_type.color.as_ref().map(|_| &user_id)) // color
            .execute(&mut *tx)
            .await
            .map_err(DbError::from)?;

            // Log Create Operation within the transaction
            let entry = ChangeLogEntry {
                operation_id: Uuid::new_v4(),
                entity_table: Self::entity_name().to_string(),
                entity_id: id,
                operation_type: ChangeOperationType::Create,
                field_name: None,
                old_value: None,
                new_value: None, // Optionally serialize new_type
                timestamp: now_dt,
                user_id: user_uuid,
                device_id: device_uuid,
                document_metadata: None,
                sync_batch_id: None,
                processed_at: None,
                sync_error: None,
            };
            self.log_change_entry(entry, &mut tx).await?;

            // Fetch within the transaction before commit might be slightly faster
            // but requires careful handling if commit fails.
            // Fetching after commit is safer.
            Ok(id) // Return ID to fetch after commit
        }.await;

        match result {
            Ok(created_id) => {
                tx.commit().await.map_err(DbError::from)?;
                // Fetch the newly created record outside the transaction
                self.find_by_id(created_id).await
            },
            Err(e) => {
                let _ = tx.rollback().await; // Ensure rollback on error
                Err(e)
            }
        }
    }

    async fn update(
        &self,
        id: Uuid,
        update_data: &UpdateDocumentType,
        auth: &AuthContext,
    ) -> DomainResult<DocumentType> {
        let mut tx = self.pool.begin().await.map_err(DbError::from)?;
        let result = async {
            // Fetch existing to ensure it exists and for potential LWW checks (though not implemented here)
            let old_entity = self.find_by_id_with_tx(id, &mut tx).await?;
            let now = Utc::now().to_rfc3339();
            let now_dt = Utc::now(); // For logging
            let user_id = auth.user_id.to_string();
            let user_uuid = auth.user_id;
            let device_uuid: Option<Uuid> = auth.device_id.parse::<Uuid>().ok();

            // Use dynamic query building for updates
            let mut sets: Vec<String> = Vec::new();
            let mut binds: Vec<String> = Vec::new(); // Using String for simplicity, sqlx handles types

             macro_rules! add_lww_update {
                 ($field:ident, $value:expr) => {
                     sets.push(format!("{0} = ?, {0}_updated_at = ?, {0}_updated_by = ?", stringify!($field)));
                     binds.push($value.to_string()); // Value
                     binds.push(now.clone());       // Updated At
                     binds.push(user_id.clone());     // Updated By
                 };
                 ($field:ident, $value:expr, $type:ty) => { // For Option<String>
                     if let Some(val) = $value {
                        add_lww_update!($field, val);
                     }
                 };
             }

             if let Some(name) = &update_data.name {
                 add_lww_update!(name, name);
             }
             // Use explicit check for Option<String> to handle None vs Some("") if needed
             add_lww_update!(description, &update_data.description, Option<String>);
             add_lww_update!(icon, &update_data.icon, Option<String>);
             add_lww_update!(color, &update_data.color, Option<String>);

             // Non-LWW field update (if needed)
              if let Some(prio) = &update_data.default_priority {
                 sets.push("default_priority = ?".to_string());
                 binds.push(prio.as_str().to_string()); // Bind as TEXT
             }


            if sets.is_empty() {
                // No fields to update, just return existing
                return self.find_by_id_with_tx(id, &mut tx).await;
            }

            // Always update updated_at and updated_by_user_id
            sets.push("updated_at = ?".to_string());
            binds.push(now.clone());
            sets.push("updated_by_user_id = ?".to_string());
            binds.push(user_id.clone());

            let query_str = format!("UPDATE document_types SET {} WHERE id = ?", sets.join(", "));

            // Build and execute the query
            let mut q = query(&query_str);
            for bind_val in binds {
                q = q.bind(bind_val);
            }
            q = q.bind(id.to_string());

            q.execute(&mut **tx).await.map_err(DbError::from)?;

            // Fetch and return the updated record
            let new_entity = self.find_by_id_with_tx(id, &mut tx).await?;

            // Log field-level updates
            macro_rules! log_if_changed {
                ($field:ident) => {
                    if old_entity.$field != new_entity.$field {
                        let entry = ChangeLogEntry {
                            operation_id: Uuid::new_v4(),
                            entity_table: Self::entity_name().to_string(),
                            entity_id: id,
                            operation_type: ChangeOperationType::Update,
                            field_name: Some(stringify!($field).to_string()),
                            old_value: Some(serde_json::to_string(&old_entity.$field).unwrap_or_default()),
                            new_value: Some(serde_json::to_string(&new_entity.$field).unwrap_or_default()),
                            timestamp: now_dt,
                            user_id: user_uuid,
                            device_id: device_uuid,
                            document_metadata: None,
                            sync_batch_id: None,
                            processed_at: None,
                            sync_error: None,
                        };
                        self.log_change_entry(entry, &mut tx).await?;
                    }
                };
            }

            log_if_changed!(name);
            log_if_changed!(description);
            log_if_changed!(icon);
            log_if_changed!(color);
            log_if_changed!(default_priority);

            Ok(new_entity)

        }.await;

        match result {
            Ok(doc_type) => { tx.commit().await.map_err(DbError::from)?; Ok(doc_type) },
            Err(e) => { let _ = tx.rollback().await; Err(e) },
        }
    }

    async fn find_all(&self, params: PaginationParams) -> DomainResult<PaginatedResult<DocumentType>> {
        let offset = (params.page - 1) * params.per_page;
        let total: i64 = query_scalar("SELECT COUNT(*) FROM document_types WHERE deleted_at IS NULL")
            .fetch_one(&self.pool).await.map_err(DbError::from)?;

        let rows = query_as::<_, DocumentTypeRow>(
                "SELECT * FROM document_types WHERE deleted_at IS NULL ORDER BY name ASC LIMIT ? OFFSET ?"
            )
            .bind(params.per_page as i64).bind(offset as i64)
            .fetch_all(&self.pool).await.map_err(DbError::from)?;

        let items = rows.into_iter().map(Self::map_row).collect::<DomainResult<Vec<_>>>()?;
        Ok(PaginatedResult::new(items, total as u64, params))
    }

     async fn find_by_name(&self, name: &str) -> DomainResult<Option<DocumentType>> {
         query_as::<_, DocumentTypeRow>("SELECT * FROM document_types WHERE name = ? AND deleted_at IS NULL")
            .bind(name)
            .fetch_optional(&self.pool)
            .await
            .map_err(DbError::from)?
            .map(Self::map_row) // Use map_row for conversion
            .transpose() // Convert Option<Result<T, E>> to Result<Option<T>, E>
     }
}

// --- Media Document Repository ---

#[async_trait]
pub trait MediaDocumentRepository:
    DeleteServiceRepository<MediaDocument> +
    MergeableEntityRepository<MediaDocument> +
    Send + Sync 
{
    async fn create(
        &self,
        new_doc: &NewMediaDocument,
        // file_path provided by service after saving file
    ) -> DomainResult<MediaDocument>;

    // UPDATE methods REMOVED - Documents are immutable via public API

    async fn find_by_related_entity(
        &self,
        related_table: &str,
        related_id: Uuid,
        params: PaginationParams,
    ) -> DomainResult<PaginatedResult<MediaDocument>>;

    async fn find_by_id(&self, id: Uuid) -> DomainResult<MediaDocument>;

    async fn find_by_id_with_tx<'t>(&self, id: Uuid, tx: &mut Transaction<'t, Sqlite>) -> DomainResult<MediaDocument>;

    /// Update compression status and optionally the compressed file path and size.
    /// Called internally by compression service.
    async fn update_compression_status(
        &self,
        id: Uuid,
        status: CompressionStatus,
        compressed_file_path: Option<&str>,
        compressed_size_bytes: Option<i64>, // ADDED size
    ) -> DomainResult<()>;

    /// Update blob sync status and key. Called internally by sync service.
    async fn update_blob_sync_status(
        &self,
        id: Uuid,
        status: BlobSyncStatus,
        blob_key: Option<&str>,
    ) -> DomainResult<()>;

    /// Update sync priority for one or more documents. Called internally? Or by specific API?
    async fn update_sync_priority(
        &self,
        ids: &[Uuid],
        priority: SyncPriority,
        auth: &AuthContext, // To track who initiated the change
    ) -> DomainResult<u64>;

    /// Update file paths and potentially status after sync download. Called internally by sync service.
    async fn update_paths_and_status(
        &self,
        document_id: Uuid,
        file_path: Option<&str>,
        compressed_file_path: Option<&str>,
        compressed_size_bytes: Option<i64>,
        compression_status: Option<CompressionStatus>,
    ) -> DomainResult<()>;


    /// Links documents created with a temporary ID to the actual entity ID after creation.
    async fn link_temp_documents(
        &self,
        temp_related_id: Uuid,
        final_related_table: &str,
        final_related_id: Uuid,
        tx: &mut Transaction<'_, Sqlite>, // Requires a transaction
    ) -> DomainResult<u64>; // Returns the number of documents linked
    
    /// Links documents with a temporary ID to their final entity
    /// This method handles the actual implementation and is called by link_temp_documents
    async fn link_temp_documents_with_tx(
        &self,
        temp_related_id: Uuid,
        final_related_table: &str,
        final_related_id: Uuid,
        tx: &mut Transaction<'_, Sqlite>,
    ) -> DomainResult<u64>;
}

pub struct SqliteMediaDocumentRepository {
    pool: Pool<Sqlite>,
    change_log_repo: Arc<dyn ChangeLogRepository + Send + Sync>,
}

impl SqliteMediaDocumentRepository {
    pub fn new(pool: Pool<Sqlite>, change_log_repo: Arc<dyn ChangeLogRepository + Send + Sync>) -> Self {
        Self { pool, change_log_repo }
    }
    fn entity_name() -> &'static str {
        "media_documents" // Table name
    }
     fn map_row(row: MediaDocumentRow) -> DomainResult<MediaDocument> {
        row.into_entity() // Use the conversion method
    }
    // Helper to log changes consistently
    async fn log_change_entry<'t>(
        &self,
        entry: ChangeLogEntry,
        tx: &mut Transaction<'t, Sqlite>,
    ) -> DomainResult<()> {
        self.change_log_repo.create_change_log_with_tx(&entry, tx).await
    }

    async fn find_by_id_option_with_tx<'t>(&self, id: Uuid, tx: &mut Transaction<'t, Sqlite>) -> DomainResult<Option<MediaDocument>> {
        let row_opt = query_as::<_, MediaDocumentRow>(
            "SELECT * FROM media_documents WHERE id = ?"
        )
        .bind(id.to_string())
        .fetch_optional(&mut **tx)
        .await
        .map_err(DbError::from)?;

        match row_opt {
            Some(row) => Ok(Some(Self::map_row(row)?)),
            None => Ok(None),
        }
    }

    async fn insert_from_full_state<'t>(&self, tx: &mut Transaction<'t, Sqlite>, state: &MediaDocumentFullState) -> DomainResult<()> {
        // Values from MediaDocumentFullState (LWW-style)
        let id_str = state.id.to_string();
        let original_filename_str = state.original_filename.as_deref().unwrap_or("unknown.bin");
        let mime_type_str = state.mime_type.as_deref().unwrap_or("application/octet-stream");
        let file_path_str = state.file_path.as_deref().unwrap_or("pending/path"); // This path should ideally be set meaningfully
        let size_bytes_val = state.size_bytes.unwrap_or(0);
        let blob_status_str = state.blob_status.as_deref().unwrap_or(BlobSyncStatus::Pending.as_str());
        let related_table_str = state.related_table.as_deref().unwrap_or("unknown_table");
        let related_id_str = state.related_id.map(|id| id.to_string());
        let description_str = state.description.as_deref();
        let created_at_str = state.created_at.to_rfc3339();
        let created_by_user_id_str = state.created_by_user_id.to_string(); // In FullState, this is Uuid, not Option<Uuid>
        let updated_at_str = state.updated_at.to_rfc3339();
        let updated_by_user_id_str = state.updated_by_user_id.to_string(); // In FullState, this is Uuid, not Option<Uuid>
        let deleted_at_str = state.deleted_at.map(|dt| dt.to_rfc3339());
        let deleted_by_user_id_str = state.deleted_by_user_id.map(|id| id.to_string());

        // Defaults for fields not in MediaDocumentFullState or needing schema defaults
        let temp_related_id_str: Option<String> = None;
        
        // CRITICAL: type_id cannot be reliably derived from MediaDocumentFullState.document_type (String) here.
        // Binding a placeholder. User MUST address this.
        let type_id_str = Uuid::nil().to_string(); 

        let compressed_file_path_str: Option<String> = None;
        let compressed_size_bytes_val: Option<i64> = None;
        let field_identifier_str: Option<String> = None;
        let title_str: Option<String> = None;
        let has_error_val: i32 = 0; // Default 0 (false)
        let error_message_str: Option<String> = None;
        let error_type_str: Option<String> = None;
        let compression_status_str = CompressionStatus::Pending.as_str(); // Default 'pending'
        let blob_key_str: Option<String> = None;
        // Assuming SyncPriorityFromSyncDomain is available via `use crate::domains::sync::types::SyncPriority as SyncPriorityFromSyncDomain;`
        let sync_priority_str = SyncPriorityFromSyncDomain::Normal.as_str(); // Default 'normal'
        let last_sync_attempt_at_str: Option<String> = None;
        let sync_attempt_count_val: i32 = 0; // Default 0

        // Ensure all NOT NULL columns in media_documents are covered
        sqlx::query!(
            r#"
            INSERT INTO media_documents (
                id, related_table, related_id, temp_related_id, type_id,
                original_filename, file_path, compressed_file_path, compressed_size_bytes,
                field_identifier, title, description, mime_type, size_bytes,
                has_error, error_message, error_type, compression_status,
                blob_status, blob_key, sync_priority, last_sync_attempt_at,
                sync_attempt_count, created_at, updated_at, created_by_user_id,
                updated_by_user_id, deleted_at, deleted_by_user_id
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            "#,
            id_str, related_table_str, related_id_str, temp_related_id_str, type_id_str,
            original_filename_str, file_path_str, compressed_file_path_str, compressed_size_bytes_val,
            field_identifier_str, title_str, description_str, mime_type_str, size_bytes_val,
            has_error_val, error_message_str, error_type_str, compression_status_str,
            blob_status_str, blob_key_str, sync_priority_str, last_sync_attempt_at_str,
            sync_attempt_count_val, created_at_str, updated_at_str, created_by_user_id_str, // Use created_by_user_id_str from state
            updated_by_user_id_str, // Use updated_by_user_id_str from state
            deleted_at_str, deleted_by_user_id_str
        )
        .execute(&mut **tx)
        .await
        .map_err(|e| {
            eprintln!("Insert from full state failed. State: {:?}, Error: {:?}", state, e);
            DbError::from(e)
        })?;
        Ok(())
    }
}

// --- Basic trait implementations remain the same ---

#[async_trait]
impl FindById<MediaDocument> for SqliteMediaDocumentRepository {
    async fn find_by_id(&self, id: Uuid) -> DomainResult<MediaDocument> {
        query_as::<_, MediaDocumentRow>(
            "SELECT * FROM media_documents WHERE id = ? AND deleted_at IS NULL",
        )
        .bind(id.to_string())
        .fetch_optional(&self.pool)
        .await
        .map_err(DbError::from)?
        .ok_or_else(|| DomainError::EntityNotFound(Self::entity_name().to_string(), id))
        .and_then(Self::map_row)
    }
}

#[async_trait]
impl SoftDeletable for SqliteMediaDocumentRepository {
     async fn soft_delete_with_tx(
        &self,
        id: Uuid,
        auth: &AuthContext,
        tx: &mut Transaction<'_, Sqlite>,
    ) -> DomainResult<()> {
        let now = Utc::now().to_rfc3339();
        let now_dt = Utc::now(); // For logging
        let user_uuid = auth.user_id;
        let device_uuid: Option<Uuid> = auth.device_id.parse::<Uuid>().ok();

        let result = query(
            "UPDATE media_documents SET deleted_at = ?, deleted_by_user_id = ?, updated_at = ? WHERE id = ? AND deleted_at IS NULL"
        )
        .bind(&now)
        .bind(auth.user_id.to_string())
        .bind(&now)
        .bind(id.to_string())
        .execute(&mut **tx)
        .await
        .map_err(DbError::from)?;

        if result.rows_affected() == 0 {
            Err(DomainError::EntityNotFound(Self::entity_name().to_string(), id))
        } else {
            Ok(())
        }
    }
    async fn soft_delete(&self, id: Uuid, auth: &AuthContext) -> DomainResult<()> {
        let mut tx = self.pool.begin().await.map_err(DbError::from)?;
        let result = self.soft_delete_with_tx(id, auth, &mut tx).await;
        match result {
            Ok(_) => { tx.commit().await.map_err(DbError::from)?; Ok(()) },
            Err(e) => { let _ = tx.rollback().await; Err(e) },
        }
    }
}

#[async_trait]
impl HardDeletable for SqliteMediaDocumentRepository {
     fn entity_name(&self) -> &'static str {
        "media_documents"
    }
    async fn hard_delete_with_tx(
        &self,
        id: Uuid,
        _auth: &AuthContext,
        tx: &mut Transaction<'_, Sqlite>,
    ) -> DomainResult<()> {
         // Assumes ON DELETE CASCADE is set up for related logs/versions in DB schema
         let result = query("DELETE FROM media_documents WHERE id = ?")
            .bind(id.to_string())
            .execute(&mut **tx)
            .await
            .map_err(DbError::from)?;

        if result.rows_affected() == 0 {
            Err(DomainError::EntityNotFound(<Self as HardDeletable>::entity_name(self).to_string(), id))
        } else {
            Ok(())
        }
    }
    async fn hard_delete(&self, id: Uuid, auth: &AuthContext) -> DomainResult<()> {
        let mut tx = self.pool.begin().await.map_err(DbError::from)?;
        // Consider adding file system cleanup logic here or in the service calling this
        let result = self.hard_delete_with_tx(id, auth, &mut tx).await;
        match result {
            Ok(_) => { tx.commit().await.map_err(DbError::from)?; Ok(()) },
            Err(e) => { let _ = tx.rollback().await; Err(e) },
        }
    }
}

// --- MediaDocumentRepository implementation ---

#[async_trait]
impl MediaDocumentRepository for SqliteMediaDocumentRepository {
    async fn create(
        &self,
        new_doc: &NewMediaDocument,
        // file_path is NOT part of NewMediaDocument DTO. Assumed to be handled by service.
        // The file_path column in DB will be set by the service calling create or later update.
    ) -> DomainResult<MediaDocument> {
        let mut tx = self.pool.begin().await.map_err(DbError::from)?;
        let result = async {
            let now = Utc::now().to_rfc3339();
            let now_dt = Utc::now(); // For logging
            let user_uuid = new_doc.created_by_user_id;
            // NOTE: NewMediaDocument does not have device_id. Logging will use None.
            // If device_id logging is needed here, add it to NewMediaDocument DTO.
            let device_uuid: Option<Uuid> = None;

            // Use the user_uuid captured outside the async block for consistency
            let user_id_str_bind = user_uuid.map(|id| id.to_string());

            // Determine related_table and related_id based on temp_related_id
            let (actual_related_table, actual_related_id_str) = if new_doc.temp_related_id.is_some() {
                (TEMP_RELATED_TABLE.to_string(), None) // Store temp ID separately
            } else {
                // If temp_id is None, related_id MUST be Some (validated in DTO)
                (new_doc.related_table.clone(), new_doc.related_id.map(|id| id.to_string()))
            };
            let temp_related_id_str = new_doc.temp_related_id.map(|id| id.to_string());

            // REMOVED file_path and description from INSERT list and bind list
            // Assumes file_path will be set later, description column might not exist or is optional
            query(
                r#"INSERT INTO media_documents (
                    id, related_table, related_id, type_id,
                    original_filename, compressed_file_path, compressed_size_bytes,
                    field_identifier, title, mime_type, size_bytes,
                    compression_status, blob_key, blob_status, sync_priority,
                    temp_related_id,
                    created_at, updated_at, created_by_user_id, updated_by_user_id,
                    deleted_at, deleted_by_user_id,
                    file_path -- explicitly setting file_path to NULL initially
                ) VALUES (
                    ?, ?, ?, ?, -- id, related_table, related_id, type_id
                    ?, NULL, NULL, -- original_filename, compressed_file_path, compressed_size_bytes
                    ?, ?, ?, ?, -- field_identifier, title, mime_type, size_bytes
                    ?, NULL, ?, ?, -- compression_status, blob_key, blob_status, sync_priority
                    ?, -- temp_related_id
                    ?, ?, ?, ?, -- created_at, updated_at, created_by_user_id, updated_by_user_id
                    NULL, NULL, -- deleted_at, deleted_by_user_id
                    NULL -- file_path
                )"#
            )
            .bind(new_doc.id.to_string())
            .bind(actual_related_table) // Store actual or TEMP_RELATED_TABLE
            .bind(actual_related_id_str) // Store actual ID or NULL if temp
            .bind(new_doc.type_id.to_string())
            .bind(&new_doc.original_filename)
             // compressed fields initialized as NULL
            .bind(&new_doc.field_identifier)
            .bind(&new_doc.title)
            .bind(&new_doc.mime_type)
            .bind(new_doc.size_bytes)
            .bind(CompressionStatus::Pending.as_str()) // Default status
            .bind(BlobSyncStatus::Pending.as_str()) // Default status
            .bind(
                SyncPriority::from_str(&new_doc.sync_priority)
                    .map_err(|_| DomainError::Validation(ValidationError::custom(&format!("Invalid sync priority string: {}", new_doc.sync_priority))))?
                    .as_str() // Bind the string representation
            )
            .bind(temp_related_id_str) // Store temp ID if provided
            .bind(&now).bind(&now)
            .bind(user_id_str_bind.as_deref()).bind(user_id_str_bind.as_deref()) // Use Option<String> binding for both
            .execute(&mut tx)
            .await
            .map_err(|e| {
                 if let sqlx::Error::Database(db_err) = &e {
                     if db_err.is_unique_violation() {
                         return DomainError::Database(DbError::Conflict(format!(
                             "MediaDocument with ID {} already exists.", new_doc.id
                         )));
                     }
                      // Check for foreign key violation on type_id
                     if db_err.message().contains("FOREIGN KEY constraint failed") {
                         // FIX: Use ValidationError::Custom instead of non-existent foreign_key variant
                         return DomainError::Validation(ValidationError::Custom(format!(
                             "Invalid document type ID ({}): Does not exist.", new_doc.type_id
                         )));
                     }
                 }
                 DomainError::Database(DbError::from(e))
             })?;

            // Log Create Operation
            let entry = ChangeLogEntry {
                operation_id: Uuid::new_v4(),
                entity_table: Self::entity_name().to_string(),
                entity_id: new_doc.id,
                operation_type: ChangeOperationType::Create,
                field_name: None,
                old_value: None,
                new_value: None, // Optionally serialize new_doc
                timestamp: now_dt,
                user_id: user_uuid.unwrap_or_else(Uuid::nil), // Provide default if None
                device_id: device_uuid,
                document_metadata: None,
                sync_batch_id: None,
                processed_at: None,
                sync_error: None,
            };
            self.log_change_entry(entry, &mut tx).await?;

            self.find_by_id_with_tx(new_doc.id, &mut tx).await
        }.await;

        match result {
            Ok(doc) => { tx.commit().await.map_err(DbError::from)?; Ok(doc) }
            Err(e) => { let _ = tx.rollback().await; Err(e) }
        }
    }

    async fn find_by_related_entity(
        &self,
        related_table: &str,
        related_id: Uuid,
        params: PaginationParams,
    ) -> DomainResult<PaginatedResult<MediaDocument>> {
        let offset = (params.page - 1) * params.per_page;
        let related_id_str = related_id.to_string();

        let count_query = query_scalar::<_, i64>(
            "SELECT COUNT(*) FROM media_documents WHERE related_table = ? AND related_id = ? AND deleted_at IS NULL"
        ).bind(related_table).bind(&related_id_str);

        let total: i64 = count_query.fetch_one(&self.pool).await.map_err(DbError::from)?;

        // Order by creation date, newest first
        let select_query = query_as::<_, MediaDocumentRow>(
            "SELECT * FROM media_documents WHERE related_table = ? AND related_id = ? AND deleted_at IS NULL ORDER BY created_at DESC LIMIT ? OFFSET ?"
        ).bind(related_table).bind(related_id_str).bind(params.per_page as i64).bind(offset as i64);

        let rows = select_query.fetch_all(&self.pool).await.map_err(DbError::from)?;
        let items = rows.into_iter().map(Self::map_row).collect::<DomainResult<Vec<_>>>()?;
        Ok(PaginatedResult::new(items, total as u64, params))
    }

    async fn update_compression_status(
        &self,
        id: Uuid,
        status: CompressionStatus,
        compressed_file_path: Option<&str>,
        compressed_size_bytes: Option<i64>, // ADDED size
    ) -> DomainResult<()> {
        let mut tx = self.pool.begin().await.map_err(DbError::from)?;
        let old_entity = self.find_by_id_with_tx(id, &mut tx).await.ok(); // Ignore error if not found for logging
        let now = Utc::now();
        let system_user_id = Uuid::nil(); // System operation
        let device_uuid: Option<Uuid> = None;

        let result = query(
            "UPDATE media_documents SET compression_status = ?, compressed_file_path = ?, compressed_size_bytes = ?, updated_at = ? WHERE id = ?"
        )
        .bind(status.as_str())
        .bind(compressed_file_path)
        .bind(compressed_size_bytes) // Bind the size
        .bind(now.to_rfc3339())
        .bind(id.to_string())
        .execute(&mut **tx) // Use transaction
        .await
        .map_err(DbError::from)?;

        if result.rows_affected() == 0 {
            let _ = tx.rollback().await;
            Err(DomainError::EntityNotFound(Self::entity_name().to_string(), id))
        } else {
            // Log changes if old entity was found
            if let Some(old) = old_entity {
                // Fetch new state within the transaction
                let new_entity = self.find_by_id_with_tx(id, &mut tx).await?;

                macro_rules! log_if_changed {
                    ($field_name:ident, $field_sql:literal) => {
                        if old.$field_name != new_entity.$field_name {
                            let entry = ChangeLogEntry {
                                operation_id: Uuid::new_v4(),
                                entity_table: Self::entity_name().to_string(),
                                entity_id: id,
                                operation_type: ChangeOperationType::Update,
                                field_name: Some($field_sql.to_string()),
                                old_value: serde_json::to_string(&old.$field_name).ok(),
                                new_value: serde_json::to_string(&new_entity.$field_name).ok(),
                                timestamp: now,
                                user_id: system_user_id,
                                device_id: device_uuid.clone(),
                                document_metadata: None,
                                sync_batch_id: None,
                                processed_at: None,
                                sync_error: None,
                            };
                            self.log_change_entry(entry, &mut tx).await?;
                        }
                    };
                }
                log_if_changed!(compression_status, "compression_status");
                log_if_changed!(compressed_file_path, "compressed_file_path");
                log_if_changed!(compressed_size_bytes, "compressed_size_bytes");
            }
            tx.commit().await.map_err(DbError::from)?;
            Ok(())
        }
    }

    async fn update_blob_sync_status(
        &self,
        id: Uuid,
        status: BlobSyncStatus,
        blob_key: Option<&str>,
    ) -> DomainResult<()> {
        let mut tx = self.pool.begin().await.map_err(DbError::from)?;
        let old_entity = self.find_by_id_with_tx(id, &mut tx).await.ok();
        let now = Utc::now();
        let system_user_id = Uuid::nil(); // System operation
        let device_uuid: Option<Uuid> = None;

        let result = query(
            "UPDATE media_documents SET blob_status = ?, blob_key = ?, updated_at = ? WHERE id = ?"
        )
        .bind(status.as_str())
        .bind(blob_key)
        .bind(now.to_rfc3339())
        .bind(id.to_string())
        .execute(&mut **tx)
        .await
        .map_err(DbError::from)?;

        if result.rows_affected() == 0 {
            let _ = tx.rollback().await;
            Err(DomainError::EntityNotFound(Self::entity_name().to_string(), id))
        } else {
            // Log changes if old entity was found
            if let Some(old) = old_entity {
                let new_entity = self.find_by_id_with_tx(id, &mut tx).await?;

                macro_rules! log_if_changed {
                    ($field_name:ident, $field_sql:literal) => {
                        if old.$field_name != new_entity.$field_name {
                            let entry = ChangeLogEntry {
                                operation_id: Uuid::new_v4(),
                                entity_table: Self::entity_name().to_string(),
                                entity_id: id,
                                operation_type: ChangeOperationType::Update,
                                field_name: Some($field_sql.to_string()),
                                old_value: serde_json::to_string(&old.$field_name).ok(),
                                new_value: serde_json::to_string(&new_entity.$field_name).ok(),
                                timestamp: now,
                                user_id: system_user_id,
                                device_id: device_uuid.clone(),
                                document_metadata: None,
                                sync_batch_id: None,
                                processed_at: None,
                                sync_error: None,
                            };
                            self.log_change_entry(entry, &mut tx).await?;
                        }
                    };
                }
                log_if_changed!(blob_status, "blob_status");
                log_if_changed!(blob_key, "blob_key");
            }
            tx.commit().await.map_err(DbError::from)?;
            Ok(())
        }
    }

    async fn update_sync_priority(
        &self,
        ids: &[Uuid],
        priority: SyncPriority,
        auth: &AuthContext, // Keep auth context for tracking who updated
    ) -> DomainResult<u64> {
        if ids.is_empty() {
            return Ok(0);
        }

        let mut tx = self.pool.begin().await.map_err(DbError::from)?;

        // --- Fetch Old Priorities ---
        let id_strings: Vec<String> = ids.iter().map(Uuid::to_string).collect();
        let select_query = format!(
            "SELECT id, sync_priority FROM media_documents WHERE id IN ({})",
            vec!["?"; ids.len()].join(", ")
        );
        let mut select_builder = query_as::<_, (String, String)>(&select_query); // Expect TEXT
        for id_str in &id_strings {
            select_builder = select_builder.bind(id_str);
        }
        let old_priorities: std::collections::HashMap<Uuid, SyncPriority> = select_builder
            .fetch_all(&mut *tx) // Use the transaction
            .await.map_err(DbError::from)?
            .into_iter()
            .filter_map(|(id_str, prio_text_opt)| { // Changed to prio_text_opt: Option<String> or String
                // Assuming sync_priority is NOT NULL in DB and TEXT
                match Uuid::parse_str(&id_str) {
                     Ok(id) => Some((id, SyncPriority::from_str(&prio_text_opt).unwrap_or(SyncPriority::Normal))),
                     Err(_) => None,
                }
            }).collect();

        let now = Utc::now().to_rfc3339();
        let now_dt = Utc::now(); // For logging
        let user_id_str = auth.user_id.to_string();
        let user_uuid = auth.user_id;
        let device_uuid: Option<Uuid> = auth.device_id.parse::<Uuid>().ok();
        let priority_str = priority.as_str(); // Store as string

        let placeholders = ids.iter().map(|_| "?").collect::<Vec<_>>().join(",");
        let query_str = format!(
            "UPDATE media_documents
             SET sync_priority = ?, updated_at = ?, updated_by_user_id = ?
             WHERE id IN ({}) AND deleted_at IS NULL",
            placeholders
        );

        let mut query_builder = sqlx::query(&query_str)
            .bind(priority_str) // Bind string
            .bind(now)
            .bind(user_id_str);

        for id in ids {
            query_builder = query_builder.bind(id.to_string());
        }

        let result = query_builder.execute(&mut *tx).await.map_err(DbError::from)?;
        let rows_affected = result.rows_affected();

        // --- Log Changes ---
        for id in ids {
            if let Some(old_priority) = old_priorities.get(id) {
                if *old_priority != priority { // Compare SyncPriority enums
                    let entry = ChangeLogEntry {
                        operation_id: Uuid::new_v4(),
                        entity_table: Self::entity_name().to_string(),
                        entity_id: *id,
                        operation_type: ChangeOperationType::Update,
                        field_name: Some("sync_priority".to_string()),
                        old_value: serde_json::to_string(old_priority.as_str()).ok(), // Log old priority as string
                        new_value: serde_json::to_string(priority_str).ok(), // Log new priority as string
                        timestamp: now_dt,
                        user_id: user_uuid,
                        device_id: device_uuid.clone(),
                        document_metadata: None,
                        sync_batch_id: None,
                        processed_at: None,
                        sync_error: None,
                    };
                    self.log_change_entry(entry, &mut tx).await?;
                }
            }
        }

        tx.commit().await.map_err(DbError::from)?;
        Ok(rows_affected)
    }

    /// Internal method to find by ID within a transaction
    async fn find_by_id_with_tx<'t>(
        &self,
        id: Uuid,
        tx: &mut Transaction<'t, Sqlite>,
    ) -> DomainResult<MediaDocument> {
        query_as::<_, MediaDocumentRow>("SELECT * FROM media_documents WHERE id = ? AND deleted_at IS NULL")
            .bind(id.to_string())
            .fetch_optional(&mut **tx)
            .await
            .map_err(DbError::from)?
            .ok_or_else(|| DomainError::EntityNotFound(Self::entity_name().to_string(), id))
            .and_then(Self::map_row)
    }

    // Re-implement find_by_id to satisfy the trait bound, delegating to the public FindById impl
    async fn find_by_id(&self, id: Uuid) -> DomainResult<MediaDocument> {
        <Self as FindById<MediaDocument>>::find_by_id(self, id).await
    }

     /// Links documents created with a temporary ID to the actual entity ID.
     /// Implementation that delegates to link_temp_documents_with_tx
    async fn link_temp_documents(
        &self,
        temp_related_id: Uuid,
        final_related_table: &str,
        final_related_id: Uuid,
        tx: &mut Transaction<'_, Sqlite>,
    ) -> DomainResult<u64> {
        self.link_temp_documents_with_tx(
            temp_related_id,
            final_related_table,
            final_related_id,
            tx
        ).await
    }
    
    /// Implementation of the actual linking logic
    async fn link_temp_documents_with_tx(
        &self,
        temp_related_id: Uuid,
        final_related_table: &str,
        final_related_id: Uuid,
        tx: &mut Transaction<'_, Sqlite>,
    ) -> DomainResult<u64> {
        let now_dt = Utc::now(); // For logging
        // Assume this is called by a system user or within a context without specific user/device
        let system_user_id = Uuid::nil(); // Placeholder system user ID
        let device_uuid: Option<Uuid> = None;

        // --- Fetch IDs and old paths/state BEFORE updating ---
        let temp_id_str_fetch = temp_related_id.to_string();
        struct OldDocState {
            id: Uuid,
            file_path: Option<String>,
            compressed_file_path: Option<String>,
        }
        let old_docs = query_as::<_, (String, Option<String>, Option<String>)>(r#"
            SELECT id, file_path, compressed_file_path
            FROM media_documents
            WHERE temp_related_id = ? AND related_table = ?
            "#)
            .bind(&temp_id_str_fetch)
            .bind(TEMP_RELATED_TABLE)
            .fetch_all(&mut **tx)
            .await
            .map_err(DbError::from)?
            .into_iter()
            .filter_map(|(id_str, fp, cfp)| {
                Uuid::parse_str(&id_str).ok().map(|id| OldDocState {
                    id,
                    file_path: fp,
                    compressed_file_path: cfp,
                })
            })
            .collect::<Vec<_>>();

        if old_docs.is_empty() {
            return Ok(0); // No documents to link or log
        }

        // First query to count how many documents will be updated
        let temp_id_str_count = temp_related_id.to_string(); // Store in variable
        let count = sqlx::query!(
            r#"
            SELECT COUNT(*) as count
            FROM media_documents
            WHERE temp_related_id = ? AND related_table = ?
            "#,
            temp_id_str_count, // Use variable
            TEMP_RELATED_TABLE
        )
        .fetch_one(&mut **tx)
        .await
        .map_err(|e| {
            DomainError::Database(DbError::from(e)) // Simplified error mapping
        })?
        .count as u64;

        if count == 0 {
            return Ok(0); // No documents to update
        }

        // Update the documents to point to the final entity
        let final_id_str_update = final_related_id.to_string(); // Store in variable
        let now_str_update = Utc::now().to_rfc3339();        // Store in variable
        let temp_id_str_update = temp_related_id.to_string();   // Store in variable
        let rows_affected = sqlx::query!(
            r#"
            UPDATE media_documents
            SET related_id = ?, 
                related_table = ?, 
                temp_related_id = NULL,
                updated_at = ? -- Also update timestamp
            WHERE temp_related_id = ? AND related_table = ?
            "#,
            final_id_str_update,  // Use variable
            final_related_table,
            now_str_update,       // Use variable
            temp_id_str_update,   // Use variable
            TEMP_RELATED_TABLE
        )
        .execute(&mut **tx)
        .await
        .map_err(|e| {
            DomainError::Database(DbError::from(e)) // Simplified error mapping
        })?
        .rows_affected() as u64;

        // --- Log Field Changes ---
        // Fetch new paths after update
        let ids_to_fetch: Vec<String> = old_docs.iter().map(|d| d.id.to_string()).collect();
        let query_fetch_new_paths = format!(
            "SELECT id, file_path, compressed_file_path FROM media_documents WHERE id IN ({})",
            vec!["?"; ids_to_fetch.len()].join(", ")
        );
        let mut new_paths_builder = query_as::<_, (String, Option<String>, Option<String>)>(&query_fetch_new_paths);
        for id_str in &ids_to_fetch {
            new_paths_builder = new_paths_builder.bind(id_str);
        }
        let new_paths_map: HashMap<Uuid, (Option<String>, Option<String>)> = new_paths_builder
            .fetch_all(&mut **tx)
            .await
            .map_err(DbError::from)?
            .into_iter()
            .filter_map(|(id_str, fp, cfp)| Uuid::parse_str(&id_str).ok().map(|id| (id, (fp, cfp))))
            .collect();

        for old_doc in old_docs {
            let new_paths = new_paths_map.get(&old_doc.id);
            let new_file_path = new_paths.and_then(|(fp, _)| fp.clone());
            let new_compressed_path = new_paths.and_then(|(_, cfp)| cfp.clone());

            // Log related_id change
            self.log_change_entry(ChangeLogEntry {
                operation_id: Uuid::new_v4(),
                entity_table: Self::entity_name().to_string(),
                entity_id: old_doc.id,
                operation_type: ChangeOperationType::Update,
                field_name: Some("related_id".to_string()),
                old_value: serde_json::to_string(&Option::<Uuid>::None).ok(), // Was implicitly null when temp_related_id was set
                new_value: serde_json::to_string(&Some(final_related_id)).ok(),
                timestamp: now_dt,
                user_id: system_user_id,
                device_id: device_uuid.clone(),
                document_metadata: None,
                sync_batch_id: None,
                processed_at: None,
                sync_error: None,
            }, tx).await?;

            // Log related_table change
            self.log_change_entry(ChangeLogEntry {
                operation_id: Uuid::new_v4(),
                entity_table: Self::entity_name().to_string(),
                entity_id: old_doc.id,
                operation_type: ChangeOperationType::Update,
                field_name: Some("related_table".to_string()),
                old_value: Some(TEMP_RELATED_TABLE.to_string()),
                new_value: Some(final_related_table.to_string()),
                timestamp: now_dt,
                user_id: system_user_id,
                device_id: device_uuid.clone(),
                document_metadata: None,
                sync_batch_id: None,
                processed_at: None,
                sync_error: None,
            }, tx).await?;

            // Log temp_related_id change (becoming NULL)
            self.log_change_entry(ChangeLogEntry {
                operation_id: Uuid::new_v4(),
                entity_table: Self::entity_name().to_string(),
                entity_id: old_doc.id,
                operation_type: ChangeOperationType::Update,
                field_name: Some("temp_related_id".to_string()),
                old_value: serde_json::to_string(&Some(temp_related_id)).ok(),
                new_value: serde_json::to_string(&Option::<Uuid>::None).ok(),
                timestamp: now_dt,
                user_id: system_user_id,
                device_id: device_uuid.clone(),
                document_metadata: None,
                sync_batch_id: None,
                processed_at: None,
                sync_error: None,
            }, tx).await?;

            // Log file_path change if it changed
            if old_doc.file_path != new_file_path {
                self.log_change_entry(ChangeLogEntry {
                    operation_id: Uuid::new_v4(),
                    entity_table: Self::entity_name().to_string(),
                    entity_id: old_doc.id,
                    operation_type: ChangeOperationType::Update,
                    field_name: Some("file_path".to_string()),
                    old_value: serde_json::to_string(&old_doc.file_path).ok(),
                    new_value: serde_json::to_string(&new_file_path).ok(),
                    timestamp: now_dt,
                    user_id: system_user_id,
                    device_id: device_uuid.clone(),
                    document_metadata: None,
                    sync_batch_id: None,
                    processed_at: None,
                    sync_error: None,
                }, tx).await?;
            }

            // Log compressed_file_path change if it changed
            if old_doc.compressed_file_path != new_compressed_path {
                self.log_change_entry(ChangeLogEntry {
                    operation_id: Uuid::new_v4(),
                    entity_table: Self::entity_name().to_string(),
                    entity_id: old_doc.id,
                    operation_type: ChangeOperationType::Update,
                    field_name: Some("compressed_file_path".to_string()),
                    old_value: serde_json::to_string(&old_doc.compressed_file_path).ok(),
                    new_value: serde_json::to_string(&new_compressed_path).ok(),
                    timestamp: now_dt,
                    user_id: system_user_id,
                    device_id: device_uuid.clone(),
                    document_metadata: None,
                    sync_batch_id: None,
                    processed_at: None,
                    sync_error: None,
                }, tx).await?;
            }
        }

        Ok(rows_affected) // Return rows affected by the main linking update
    }

     /// Updates paths and status, typically after a sync download.
     async fn update_paths_and_status(
         &self,
         document_id: Uuid,
         file_path: Option<&str>, // New original path? Unlikely use case?
         compressed_file_path: Option<&str>,
         compressed_size_bytes: Option<i64>,
         compression_status: Option<CompressionStatus>,
     ) -> DomainResult<()> {
          let mut tx = self.pool.begin().await.map_err(DbError::from)?;

          // --- Fetch Old State ---
          let old_entity = match self.find_by_id_with_tx(document_id, &mut tx).await {
             Ok(entity) => entity,
             Err(e) => {
                 let _ = tx.rollback().await;
                 return Err(e);
             }
         };

         let mut sets: Vec<String> = Vec::new();
         let mut binds: Vec<String> = Vec::new(); // Using String for simplicity
         let mut binds_i64: HashMap<String, i64> = HashMap::new(); // For integer binds

         let now = Utc::now(); // For logging and timestamp
         // Assuming system operation - no specific user/device ID
         let system_user_id = Uuid::nil();
         let device_uuid: Option<Uuid> = None;

         // Use macro to simplify adding updates
         macro_rules! add_update {
            ($field:ident, $value:expr, $bind_vec:ident) => {
               if let Some(val) = $value {
                   sets.push(format!("{} = ?", stringify!($field)));
                   $bind_vec.push(val.to_string());
               }
            };
             ($field:ident, $value:expr, $bind_map:ident, $type:ty) => {
                if let Some(val) = $value {
                    sets.push(format!("{} = ?", stringify!($field)));
                    $bind_map.insert(stringify!($field).to_string(), val as $type);
                }
            };
         }

        // Add fields to update
        add_update!(file_path, file_path, binds);
        add_update!(compressed_file_path, compressed_file_path, binds);
        add_update!(compressed_size_bytes, compressed_size_bytes, binds_i64, i64);
        if let Some(status) = compression_status {
            sets.push("compression_status = ?".to_string());
            binds.push(status.as_str().to_string());
        }


        if sets.is_empty() {
            return Ok(()); // Nothing to update
        }

        // Always update updated_at
        sets.push("updated_at = ?".to_string());
        binds.push(now.to_rfc3339()); // Use captured 'now'
        // Optionally update updated_by_user_id if tracking sync agent ID
        // sets.push("updated_by_user_id = ?".to_string());
        // binds.push(SYSTEM_USER_ID.to_string());

        let query_str = format!("UPDATE media_documents SET {} WHERE id = ?", sets.join(", "));

        // Build and execute the query
        let mut q = query(&query_str);
        for bind_val in binds { // Bind strings first
            q = q.bind(bind_val);
        }
        // Bind integers (order matters if placeholders are mixed)
        // Assuming integer placeholders come after string ones based on SET order
        if let Some(val) = binds_i64.get("compressed_size_bytes") {
            q = q.bind(val);
        }
        // Add binds for other i64 fields if needed

        q = q.bind(document_id.to_string()); // Bind the WHERE clause ID

        let result = q.execute(&mut **tx).await.map_err(DbError::from)?;

        if result.rows_affected() == 0 {
            let _ = tx.rollback().await;
            Err(DomainError::EntityNotFound(Self::entity_name().to_string(), document_id))
        } else {
            // --- Log Field Changes ---
            let new_entity = self.find_by_id_with_tx(document_id, &mut tx).await?;

            macro_rules! log_if_changed {
                ($field_name:ident, $field_sql:literal) => {
                    if old_entity.$field_name != new_entity.$field_name {
                        let entry = ChangeLogEntry {
                            operation_id: Uuid::new_v4(),
                            entity_table: Self::entity_name().to_string(),
                            entity_id: document_id,
                            operation_type: ChangeOperationType::Update,
                            field_name: Some($field_sql.to_string()),
                            old_value: serde_json::to_string(&old_entity.$field_name).ok(),
                            new_value: serde_json::to_string(&new_entity.$field_name).ok(),
                            timestamp: now,
                            user_id: system_user_id,
                            device_id: device_uuid.clone(),
                            document_metadata: None,
                            sync_batch_id: None,
                            processed_at: None,
                            sync_error: None,
                        };
                        self.log_change_entry(entry, &mut tx).await?;
                    }
                };
            }

            log_if_changed!(file_path, "file_path");
            log_if_changed!(compressed_file_path, "compressed_file_path");
            log_if_changed!(compressed_size_bytes, "compressed_size_bytes");
            log_if_changed!(compression_status, "compression_status");

            tx.commit().await.map_err(DbError::from)?;
            Ok(())
        }
    }
}

#[async_trait]
impl MergeableEntityRepository<MediaDocument> for SqliteMediaDocumentRepository {
    fn entity_name(&self) -> &'static str { "media_documents" }

    async fn merge_remote_change<'t>(
        &self,
        tx: &mut Transaction<'t, Sqlite>,
        remote_change: &ChangeLogEntry,
    ) -> DomainResult<MergeOutcome> {
        if remote_change.entity_table != <Self as MergeableEntityRepository<MediaDocument>>::entity_name(self) {
            return Err(DomainError::Internal(format!(
                "MediaDocumentRepository received change for wrong table: {}",
                remote_change.entity_table
            )));
        }
        let document_id = remote_change.entity_id;
        use crate::domains::sync::types::ChangeOperationType as Op;
        match remote_change.operation_type {
            Op::Create => {
                let state_json = remote_change.new_value.as_ref()
                    .ok_or_else(|| DomainError::Validation(ValidationError::custom("Missing new_value for document create")))?;
                let payload: MediaDocumentFullState = serde_json::from_str(state_json)
                    .map_err(|e| DomainError::Validation(ValidationError::format("new_value_document_create", &format!("Invalid JSON: {}", e))))?;
                if self.find_by_id_option_with_tx(document_id, tx).await?.is_some() {
                    return Ok(MergeOutcome::NoOp("Document already exists".to_string()));
                }
                self.insert_from_full_state(tx, &payload).await?;
                Ok(MergeOutcome::Created(document_id))
            }
            Op::Update => {
                // Simplified LWW: overwrite whole row if exists else NoOp
                let state_json = remote_change.new_value.as_ref()
                    .ok_or_else(|| DomainError::Validation(ValidationError::custom("Missing new_value for document update")))?;
                let payload: MediaDocumentFullState = serde_json::from_str(state_json)
                    .map_err(|e| DomainError::Validation(ValidationError::format("new_value_document_update", &format!("Invalid JSON: {}", e))))?;
                if self.find_by_id_option_with_tx(document_id, tx).await?.is_none() {
                    // treat as create
                    self.insert_from_full_state(tx, &payload).await?;
                    return Ok(MergeOutcome::Created(document_id));
                }
                // Overwrite some columns (simplified)
                let original_filename_bind = payload.original_filename.as_deref();
                let mime_type_bind = payload.mime_type.as_deref();
                let size_bytes_bind = payload.size_bytes.unwrap_or(0);
                let updated_at_bind = payload.updated_at.to_rfc3339();
                let document_id_bind = document_id.to_string();

                query!(r#"UPDATE media_documents SET
                    original_filename = ?, mime_type = ?, size_bytes = ?, updated_at = ?
                    WHERE id = ?"#,
                    original_filename_bind,
                    mime_type_bind,
                    size_bytes_bind,
                    updated_at_bind,
                    document_id_bind)
                    .execute(&mut **tx).await.map_err(DbError::from)?;
                Ok(MergeOutcome::Updated(document_id))
            }
            Op::HardDelete => {
                if self.find_by_id_option_with_tx(document_id, tx).await?.is_none() {
                    return Ok(MergeOutcome::NoOp("Already deleted".to_string()));
                }
                let temp_auth = AuthContext::internal_system_context();
                self.hard_delete_with_tx(document_id, &temp_auth, tx).await?;
                Ok(MergeOutcome::HardDeleted(document_id))
            }
            Op::Delete => {
                Ok(MergeOutcome::NoOp("Soft delete ignored".to_string()))
            }
        }
    }
}

// --- Document Version Repository --- (Assumed schema matches types)

#[async_trait]
pub trait DocumentVersionRepository: Send + Sync {
    async fn create(
        &self,
        doc_id: Uuid,
        version_number: i64,
        file_path: &str,
        file_size: i64,
        mime_type: &str,
        blob_key: Option<&str>,
        auth: &AuthContext, // Assuming versions are created by user actions
    ) -> DomainResult<DocumentVersion>;

    async fn find_by_document_id(&self, doc_id: Uuid) -> DomainResult<Vec<DocumentVersion>>;
}

pub struct SqliteDocumentVersionRepository {
    pool: Pool<Sqlite>,
    change_log_repo: Arc<dyn ChangeLogRepository + Send + Sync>,
}

impl SqliteDocumentVersionRepository {
    pub fn new(pool: Pool<Sqlite>, change_log_repo: Arc<dyn ChangeLogRepository + Send + Sync>) -> Self {
        Self { pool, change_log_repo }
    }
    fn map_row(row: DocumentVersionRow) -> DomainResult<DocumentVersion> {
        row.into_entity()
    }
}

#[async_trait]
impl DocumentVersionRepository for SqliteDocumentVersionRepository {
    async fn create(
        &self,
        doc_id: Uuid,
        version_number: i64,
        file_path: &str,
        file_size: i64,
        mime_type: &str,
        blob_key: Option<&str>,
        auth: &AuthContext,
    ) -> DomainResult<DocumentVersion> {
        let id = Uuid::new_v4();
        let now = Utc::now().to_rfc3339();
        let now_dt = Utc::now(); // For logging
        let user_id_str = auth.user_id.to_string();
        let user_uuid = auth.user_id;
        let device_uuid: Option<Uuid> = auth.device_id.parse::<Uuid>().ok();

        let mut tx = self.pool.begin().await.map_err(DbError::from)?;

        let result = async {
            query(
                r#"INSERT INTO document_versions (
                    id, document_id, version_number, file_path, file_size, mime_type, blob_storage_key,
                    created_at, created_by_user_id
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)"#
            )
            .bind(id.to_string())
            .bind(doc_id.to_string())
            .bind(version_number)
            .bind(file_path)
            .bind(file_size)
            .bind(mime_type)
            .bind(blob_key)
            .bind(&now)
            .bind(user_id_str) // Use user ID from AuthContext
            .execute(&mut **tx)
            .await
            .map_err(DbError::from)?;

            // Log Create Operation within the transaction
            let entry = ChangeLogEntry {
                operation_id: Uuid::new_v4(),
                entity_table: "document_versions".to_string(),
                entity_id: id,
                operation_type: ChangeOperationType::Create,
                field_name: None,
                old_value: None,
                new_value: None,
                timestamp: now_dt,
                user_id: user_uuid,
                device_id: device_uuid,
                document_metadata: None,
                sync_batch_id: None,
                processed_at: None,
                sync_error: None,
            };
            self.change_log_repo.create_change_log_with_tx(&entry, &mut tx).await?;

            Ok(id) // Return ID to fetch after commit
        }.await;

        match result {
            Ok(created_id) => {
                tx.commit().await.map_err(DbError::from)?;
                // Fetch the newly created record outside the transaction
                let row = query_as::<_, DocumentVersionRow>("SELECT * FROM document_versions WHERE id = ?")
                   .bind(created_id.to_string()).fetch_one(&self.pool).await.map_err(DbError::from)?;
                Self::map_row(row)
            },
            Err(e) => {
                let _ = tx.rollback().await;
                Err(e)
            }
        }
    }

    async fn find_by_document_id(&self, doc_id: Uuid) -> DomainResult<Vec<DocumentVersion>> {
        let rows = query_as::<_, DocumentVersionRow>(
            "SELECT * FROM document_versions WHERE document_id = ? ORDER BY version_number DESC"
        )
        .bind(doc_id.to_string())
        .fetch_all(&self.pool)
        .await
        .map_err(DbError::from)?;
        rows.into_iter().map(Self::map_row).collect()
    }
}

// --- Document Access Log Repository --- (Assumed schema matches types)

#[async_trait]
pub trait DocumentAccessLogRepository: Send + Sync {
     async fn create(&self, new_log: &NewDocumentAccessLog) -> DomainResult<DocumentAccessLog>;
     async fn find_by_document_id(&self, doc_id: Uuid, params: PaginationParams) -> DomainResult<PaginatedResult<DocumentAccessLog>>;
}

pub struct SqliteDocumentAccessLogRepository {
    pool: Pool<Sqlite>
}

impl SqliteDocumentAccessLogRepository {
    pub fn new(pool: Pool<Sqlite>) -> Self {
        Self { pool }
    }
    fn map_row(row: DocumentAccessLogRow) -> DomainResult<DocumentAccessLog> {
        row.into_entity()
    }
}

#[async_trait]
impl DocumentAccessLogRepository for SqliteDocumentAccessLogRepository {
    async fn create(&self, new_log: &NewDocumentAccessLog) -> DomainResult<DocumentAccessLog> {
        let id = Uuid::new_v4();
        let now = Utc::now().to_rfc3339();

        query(
            "INSERT INTO document_access_logs (id, document_id, user_id, access_type, access_date, details) VALUES (?, ?, ?, ?, ?, ?)"
        )
        .bind(id.to_string())
        .bind(new_log.document_id.to_string())
        .bind(new_log.user_id.to_string()) // Use provided user_id (could be system user)
        .bind(&new_log.access_type) // Assumes access_type is already validated string
        .bind(&now)
        .bind(&new_log.details)
        .execute(&self.pool)
        .await
        .map_err(DbError::from)?;

        // Fetch the created record
        let row = query_as::<_, DocumentAccessLogRow>("SELECT * FROM document_access_logs WHERE id = ?")
            .bind(id.to_string()).fetch_one(&self.pool).await.map_err(DbError::from)?;
        Self::map_row(row)
    }

     async fn find_by_document_id(&self, doc_id: Uuid, params: PaginationParams) -> DomainResult<PaginatedResult<DocumentAccessLog>> {
        let offset = (params.page - 1) * params.per_page;
        let doc_id_str = doc_id.to_string();

        let count_query = query_scalar::<_, i64>(
            "SELECT COUNT(*) FROM document_access_logs WHERE document_id = ?"
        ).bind(&doc_id_str);
        let total: i64 = count_query.fetch_one(&self.pool).await.map_err(DbError::from)?;

        let select_query = query_as::<_, DocumentAccessLogRow>(
            "SELECT * FROM document_access_logs WHERE document_id = ? ORDER BY access_date DESC LIMIT ? OFFSET ?"
        ).bind(doc_id_str).bind(params.per_page as i64).bind(offset as i64);

        let rows = select_query.fetch_all(&self.pool).await.map_err(DbError::from)?;
        let items = rows.into_iter().map(Self::map_row).collect::<DomainResult<Vec<_>>>()?;
        Ok(PaginatedResult::new(items, total as u64, params))
    }
}